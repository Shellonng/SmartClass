"""
æµ‹è¯•Triple Qwenå®Œæ•´é¢è¯•æµç¨‹
æ¼”ç¤ºï¼šDecision â†’ Question â†’ Scorer çš„å®Œæ•´ååŒ
"""

import torch
import sys
import io
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

def load_model_with_lora(base_model_name, lora_path, model_name, use_4bit=True):
    """åŠ è½½å¸¦LoRAçš„æ¨¡å‹"""
    print(f"\nğŸ“¥ åŠ è½½{model_name}...")
    print(f"   LoRA: {lora_path}")
    
    if use_4bit:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            quantization_config=bnb_config,
            device_map="auto"
        )
    else:
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            device_map="auto",
            torch_dtype=torch.float16
        )
    
    model = PeftModel.from_pretrained(base_model, lora_path)
    model.eval()
    
    print(f"   âœ… {model_name}åŠ è½½å®Œæˆ")
    return model

def generate_response(model, tokenizer, instruction, input_text, max_tokens=256, temperature=0.7):
    """ç”Ÿæˆå“åº”"""
    prompt = f"""<|im_start|>system
{instruction}<|im_end|>
<|im_start|>user
{input_text}<|im_end|>
<|im_start|>assistant
"""
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    input_length = inputs['input_ids'].shape[1]
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=temperature,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.pad_token_id
        )
    
    generated_tokens = outputs[0][input_length:]
    response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()
    
    return response

def test_triple_qwen():
    """æµ‹è¯•Triple Qwenå®Œæ•´æµç¨‹"""
    
    print("="*80)
    print("ğŸš€ Triple Qwen é¢è¯•ç³»ç»Ÿå®Œæ•´æµ‹è¯•")
    print("="*80)
    
    base_model_name = "Qwen/Qwen2-1.5B-Instruct"
    
    # åŠ è½½åˆ†è¯å™¨
    print(f"\nğŸ“¥ åŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    print(f"   âœ… åˆ†è¯å™¨åŠ è½½å®Œæˆ")
    
    # åŠ è½½ä¸‰ä¸ªæ¨¡å‹
    print(f"\n{'='*80}")
    print("ğŸ“¦ åŠ è½½Triple Qwenæ¨¡å‹")
    print(f"{'='*80}")
    
    decision_model = load_model_with_lora(
        base_model_name, 
        "checkpoints/qwen_decision_lora",
        "Qwen-Decision",
        use_4bit=True
    )
    
    question_model = load_model_with_lora(
        base_model_name,
        "checkpoints/qwen_question_lora", 
        "Qwen-Question",
        use_4bit=True
    )
    
    scorer_model = load_model_with_lora(
        base_model_name,
        "checkpoints/qwen_scorer_lora",
        "Qwen-Scorer",
        use_4bit=True
    )
    
    # æ˜¾ç¤ºæ˜¾å­˜å ç”¨
    if torch.cuda.is_available():
        memory_allocated = torch.cuda.memory_allocated() / 1024**3
        memory_reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"\nğŸ’¾ ä¸‰æ¨¡å‹æ˜¾å­˜å ç”¨:")
        print(f"   å·²åˆ†é…: {memory_allocated:.2f} GB")
        print(f"   å·²é¢„ç•™: {memory_reserved:.2f} GB")
    
    # ========== æµ‹è¯•åœºæ™¯è®¾ç½® ==========
    print(f"\n{'='*80}")
    print("ğŸ“ æµ‹è¯•åœºæ™¯ï¼šå®Œæ•´é¢è¯•æµç¨‹æ¼”ç¤º")
    print(f"{'='*80}")
    
    topic = "Redisç¼“å­˜è®¾è®¡"
    round_number = 3
    history = [
        {
            "question": "è¯·ä»‹ç»ä¸€ä¸‹ä½ åœ¨é¡¹ç›®ä¸­æ˜¯å¦‚ä½•ä½¿ç”¨Redisçš„ï¼Ÿ",
            "answer": "æˆ‘åœ¨ç”µå•†é¡¹ç›®ä¸­ä½¿ç”¨Redisä½œä¸ºç¼“å­˜å±‚ï¼Œä¸»è¦ç¼“å­˜å•†å“ä¿¡æ¯ã€ç”¨æˆ·ä¼šè¯ç­‰çƒ­ç‚¹æ•°æ®ï¼Œä½¿ç”¨äº†Stringã€Hashç­‰æ•°æ®ç»“æ„ã€‚",
            "score": 75
        },
        {
            "question": "ä½ ä»¬çš„ç¼“å­˜ç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•å¤„ç†ç¼“å­˜å¤±æ•ˆé—®é¢˜ï¼Ÿ",
            "answer": "æˆ‘ä»¬é‡‡ç”¨cache-asideæ¨¡å¼ï¼Œè®¾ç½®äº†åˆç†çš„è¿‡æœŸæ—¶é—´ã€‚å¯¹äºç¼“å­˜å¤±æ•ˆï¼Œå—¯...æˆ‘ä»¬ä¼šä»æ•°æ®åº“é‡æ–°åŠ è½½ï¼Œä½†å…·ä½“çš„ç¼“å­˜å‡»ç©¿ã€é›ªå´©é—®é¢˜å¤„ç†ä¸å¤ªæ¸…æ¥šã€‚",
            "score": 60
        }
    ]
    scores = [75, 60]
    
    print(f"\nå½“å‰çŠ¶æ€:")
    print(f"  è¯é¢˜: {topic}")
    print(f"  è½®æ¬¡: ç¬¬{round_number}è½®")
    print(f"  å†å²è¯„åˆ†: {scores} (å¹³å‡: {sum(scores)/len(scores):.0f}åˆ†)")
    print(f"\nå¯¹è¯å†å²:")
    for i, h in enumerate(history, 1):
        print(f"  è½®{i}:")
        print(f"    Q: {h['question']}")
        print(f"    A: {h['answer'][:60]}...")
        print(f"    è¯„åˆ†: {h['score']}åˆ†")
    
    # ========== æ­¥éª¤1: Decision ==========
    print(f"\n{'='*80}")
    print("æ­¥éª¤ 1ï¸âƒ£: Qwen-Decision åšå‡ºå†³ç­–")
    print(f"{'='*80}")
    
    history_text = ""
    for h in history[-3:]:
        history_text += f"é—®: {h['question']}\nç­”: {h['answer']}\nè¯„åˆ†: {h['score']}åˆ†\n\n"
    
    avg_score = sum(scores) / len(scores)
    
    decision_input = f"""å½“å‰è¯é¢˜: {topic}
å½“å‰è½®æ¬¡: ç¬¬{round_number}è½®

å¯¹è¯å†å²:
{history_text.strip()}

è¯„åˆ†æƒ…å†µ:
å¹³å‡åˆ†: {avg_score:.0f}åˆ†
åˆ†æ•°è¶‹åŠ¿: ä¸‹é™ï¼ˆ75â†’60ï¼‰
æœ€è¿‘3æ¬¡: {scores[-3:]}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œåšå‡ºé¢è¯•å†³ç­–å¹¶æä¾›æŒ‡å¯¼å»ºè®®ã€‚"""
    
    decision_instruction = "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„æŠ€æœ¯é¢è¯•å®˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®å½“å‰é¢è¯•è¯é¢˜ã€å¯¹è¯å†å²å’Œå€™é€‰äººçš„è¡¨ç°è¯„åˆ†ï¼Œåšå‡ºé¢è¯•å†³ç­–ï¼ˆFOLLOW_UPç»§ç»­æ·±å…¥ æˆ– SWITCH_TOPICåˆ‡æ¢è¯é¢˜ï¼‰ï¼Œå¹¶ä¸ºé—®é¢˜ç”Ÿæˆå™¨æä¾›è¯¦ç»†çš„æŒ‡å¯¼å»ºè®®ã€‚"
    
    print(f"\nâ³ ç”Ÿæˆä¸­...")
    decision_response = generate_response(
        decision_model, 
        tokenizer, 
        decision_instruction, 
        decision_input,
        max_tokens=256,
        temperature=0.7
    )
    
    print(f"\nğŸ“¤ Decisionè¾“å‡º:")
    print(f"{'-'*80}")
    print(decision_response)
    print(f"{'-'*80}")
    
    # è§£æå†³ç­–
    action = "SWITCH_TOPIC"
    guidance = decision_response
    
    if "å†³ç­–:" in decision_response:
        parts = decision_response.split("æŒ‡å¯¼å»ºè®®:")
        action_part = parts[0].replace("å†³ç­–:", "").strip()
        if "FOLLOW_UP" in action_part:
            action = "FOLLOW_UP"
        elif "SWITCH_TOPIC" in action_part:
            action = "SWITCH_TOPIC"
        
        if len(parts) > 1:
            guidance = parts[1].strip()
    
    print(f"\nâœ… è§£æç»“æœ:")
    print(f"   å†³ç­–: {action}")
    print(f"   æŒ‡å¯¼: {guidance[:100]}...")
    
    # ========== æ­¥éª¤2: Question ==========
    print(f"\n{'='*80}")
    print("æ­¥éª¤ 2ï¸âƒ£: Qwen-Question ç”Ÿæˆé—®é¢˜")
    print(f"{'='*80}")
    
    history_text_full = ""
    for h in history:
        history_text_full += f"Q: {h['question']}\nA: {h['answer']}\n\n"
    
    question_input = f"""é¢è¯•è¯é¢˜: {topic}

å®Œæ•´å¯¹è¯å†å²:
{history_text_full.strip()}

å†³ç­–æŒ‡å¯¼:
{guidance}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ç”Ÿæˆä¸‹ä¸€ä¸ªé¢è¯•é—®é¢˜ï¼Œå¹¶è¯„ä¼°å…¶é‡è¦ç¨‹åº¦ï¼ˆ1-5åˆ†ï¼‰ã€‚"""
    
    question_instruction = "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„æŠ€æœ¯é¢è¯•å®˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®é¢è¯•è¯é¢˜ã€å®Œæ•´å¯¹è¯å†å²å’Œå†³ç­–æŒ‡å¯¼ï¼Œç”Ÿæˆä¸‹ä¸€ä¸ªåˆé€‚çš„é¢è¯•é—®é¢˜ï¼Œå¹¶è¯„ä¼°è¯¥é—®é¢˜çš„é‡è¦ç¨‹åº¦ï¼ˆ1-5åˆ†ï¼Œå…¶ä¸­1åˆ†ä¸ºé—²èŠï¼Œ5åˆ†ä¸ºæ ¸å¿ƒæŠ€èƒ½è€ƒå¯Ÿï¼‰ã€‚"
    
    print(f"\nâ³ ç”Ÿæˆä¸­...")
    question_response = generate_response(
        question_model,
        tokenizer,
        question_instruction,
        question_input,
        max_tokens=200,
        temperature=0.8
    )
    
    print(f"\nğŸ“¤ Questionè¾“å‡º:")
    print(f"{'-'*80}")
    print(question_response)
    print(f"{'-'*80}")
    
    # è§£æé—®é¢˜
    question = question_response
    importance = 3
    
    if "é—®é¢˜:" in question_response and "é‡è¦ç¨‹åº¦:" in question_response:
        parts = question_response.split("é‡è¦ç¨‹åº¦:")
        question = parts[0].replace("é—®é¢˜:", "").strip()
        
        if len(parts) > 1:
            importance_str = parts[1].strip().split("åˆ†")[0].strip()
            try:
                importance = int(importance_str)
            except:
                importance = 3
    
    print(f"\nâœ… è§£æç»“æœ:")
    print(f"   é—®é¢˜: {question}")
    print(f"   é‡è¦ç¨‹åº¦: {importance}åˆ†")
    
    # ========== æ¨¡æ‹Ÿå€™é€‰äººå›ç­” ==========
    print(f"\n{'='*80}")
    print("æ­¥éª¤ 3ï¸âƒ£: æ¨¡æ‹Ÿå€™é€‰äººå›ç­”")
    print(f"{'='*80}")
    
    # æ¨¡æ‹Ÿä¸¤ç§å›ç­”ï¼šå¥½çš„å’Œå·®çš„
    candidate_answers = [
        {
            "type": "ä¼˜ç§€å›ç­”",
            "answer": "åœ¨å¤„ç†ç¼“å­˜å‡»ç©¿é—®é¢˜æ—¶ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†äº’æ–¥é”æœºåˆ¶ï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ªè¯·æ±‚å»æ•°æ®åº“æŸ¥è¯¢ã€‚å¯¹äºç¼“å­˜é›ªå´©ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†éšæœºè¿‡æœŸæ—¶é—´ç­–ç•¥ï¼Œé¿å…å¤§é‡keyåŒæ—¶å¤±æ•ˆã€‚å¦å¤–æˆ‘ä»¬è¿˜å®ç°äº†çƒ­ç‚¹æ•°æ®æ°¸ä¸è¿‡æœŸ+åå°å¼‚æ­¥æ›´æ–°çš„æ–¹æ¡ˆï¼Œåœ¨ç§’æ€åœºæ™¯ä¸‹æ•ˆæœå¾ˆå¥½ã€‚"
        },
        {
            "type": "ä¸€èˆ¬å›ç­”",
            "answer": "å—¯...ç¼“å­˜å‡»ç©¿çš„è¯ï¼Œæˆ‘çŸ¥é“æ˜¯çƒ­ç‚¹keyå¤±æ•ˆå¯¼è‡´çš„ã€‚æˆ‘ä»¬é¡¹ç›®ä¸­ï¼Œé¢...å¥½åƒæ˜¯è®¾ç½®äº†äº’æ–¥é”ï¼Œä½†å…·ä½“å®ç°ç»†èŠ‚æˆ‘ä¸å¤ªæ¸…æ¥šã€‚ç¼“å­˜é›ªå´©æ–¹é¢ï¼Œå‘ƒ...æˆ‘è®°å¾—æ˜¯è®¾ç½®éšæœºè¿‡æœŸæ—¶é—´ï¼Œä½†æ²¡æœ‰æ·±å…¥ç ”ç©¶è¿‡ã€‚"
        }
    ]
    
    for candidate_answer in candidate_answers:
        print(f"\n{'â”€'*80}")
        print(f"ğŸ“ æµ‹è¯•å›ç­”ç±»å‹: {candidate_answer['type']}")
        print(f"{'â”€'*80}")
        print(f"\nå€™é€‰äººå›ç­”: {candidate_answer['answer']}")
        
        # ========== æ­¥éª¤4: Scorer ==========
        print(f"\n{'='*80}")
        print("æ­¥éª¤ 4ï¸âƒ£: Qwen-Scorer è¯„ä¼°å›ç­”")
        print(f"{'='*80}")
        
        scorer_input = f"""é¢è¯•é—®é¢˜: {question}

å€™é€‰äººå›ç­”:
{candidate_answer['answer']}

è¯·è¯„ä¼°è¿™ä¸ªå›ç­”çš„è´¨é‡ï¼Œç»™å‡ºè¯„åˆ†ï¼ˆ0-100åˆ†ï¼‰ã€æ ‡ç­¾ï¼ˆexcellent/good/average/poorï¼‰å’Œè¯„ä»·ã€‚"""
        
        scorer_instruction = "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„æŠ€æœ¯é¢è¯•å®˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯è¯„ä¼°å€™é€‰äººå¯¹æŠ€æœ¯é—®é¢˜çš„å›ç­”è´¨é‡ï¼Œç»™å‡ºè¯„åˆ†ï¼ˆ0-100åˆ†ï¼‰ã€æ ‡ç­¾ï¼ˆexcellent/good/average/poorï¼‰å’Œè¯¦ç»†è¯„ä»·ã€‚è¯„åˆ†æ ‡å‡†ï¼šexcellent(85-100)è¡¨ç¤ºå›ç­”å‡†ç¡®ã€æ·±å…¥ã€æœ‰å®æˆ˜ç»éªŒï¼›good(70-84)è¡¨ç¤ºå›ç­”æ­£ç¡®ä½†ä¸å¤Ÿæ·±å…¥ï¼›average(50-69)è¡¨ç¤ºå›ç­”éƒ¨åˆ†æ­£ç¡®æˆ–è¾ƒæµ…ï¼›poor(0-49)è¡¨ç¤ºå›ç­”é”™è¯¯æˆ–å®Œå…¨ä¸ä¼šã€‚"
        
        print(f"\nâ³ è¯„ä¼°ä¸­...")
        scorer_response = generate_response(
            scorer_model,
            tokenizer,
            scorer_instruction,
            scorer_input,
            max_tokens=256,
            temperature=0.7
        )
        
        print(f"\nğŸ“¤ Scorerè¾“å‡º:")
        print(f"{'-'*80}")
        print(scorer_response)
        print(f"{'-'*80}")
        
        # è§£æè¯„åˆ†
        score = 70
        label = "average"
        comment = scorer_response
        
        if "è¯„åˆ†:" in scorer_response:
            try:
                score_part = scorer_response.split("è¯„åˆ†:")[1].split("åˆ†")[0].strip()
                score = int(score_part)
            except:
                pass
        
        if "æ ‡ç­¾:" in scorer_response:
            label_part = scorer_response.split("æ ‡ç­¾:")[1].split("\n")[0].strip()
            label = label_part
        
        if "è¯„ä»·:" in scorer_response:
            comment = scorer_response.split("è¯„ä»·:")[1].strip()
        
        print(f"\nâœ… è§£æç»“æœ:")
        print(f"   è¯„åˆ†: {score}åˆ†")
        print(f"   æ ‡ç­¾: {label}")
        print(f"   è¯„ä»·: {comment[:80]}...")
    
    # ========== æ€»ç»“ ==========
    print(f"\n{'='*80}")
    print("ğŸ‰ Triple Qwen å®Œæ•´æµç¨‹æµ‹è¯•å®Œæˆ")
    print(f"{'='*80}")
    
    print(f"\nğŸ“Š æµç¨‹æ€»ç»“:")
    print(f"  è¾“å…¥: è¯é¢˜'{topic}' + å†å²{len(history)}è½® + è¯„åˆ†{scores}")
    print(f"  â†“")
    print(f"  ã€Qwen-Decisionã€‘")
    print(f"    â†’ å†³ç­–: {action}")
    print(f"    â†’ æŒ‡å¯¼: {guidance[:50]}...")
    print(f"  â†“")
    print(f"  ã€Qwen-Questionã€‘")
    print(f"    â†’ é—®é¢˜: {question[:50]}...")
    print(f"    â†’ é‡è¦: {importance}åˆ†")
    print(f"  â†“")
    print(f"  ã€å€™é€‰äººå›ç­”ã€‘")
    print(f"  â†“")
    print(f"  ã€Qwen-Scorerã€‘")
    print(f"    â†’ è¯„åˆ†: {score}åˆ†")
    print(f"    â†’ æ ‡ç­¾: {label}")
    print(f"    â†’ è¯„ä»·: {comment[:50]}...")
    
    if torch.cuda.is_available():
        memory_allocated = torch.cuda.memory_allocated() / 1024**3
        print(f"\nğŸ’¾ æ¨ç†æ˜¾å­˜å ç”¨: {memory_allocated:.2f} GB")
    
    print(f"\nâœ… Triple Qwenç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼")
    print(f"\nğŸ¯ ç³»ç»Ÿç‰¹ç‚¹:")
    print(f"  âœ… å…¨Qwenæ¶æ„ - ç»Ÿä¸€åŸºåº§")
    print(f"  âœ… ä¸‰æ¨¡å‹ååŒ - æµç¨‹å®Œæ•´")
    print(f"  âœ… æ˜¾å­˜å‹å¥½ - 3.3GBæ¨ç†")
    print(f"  âœ… æ€§èƒ½ä¼˜ç§€ - ä¸“ä¸šé¢è¯•æ°´å¹³")

if __name__ == "__main__":
    test_triple_qwen()


