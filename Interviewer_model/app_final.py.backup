"""
AIé¢è¯•æ•™ç»ƒç³»ç»Ÿ - å®Œæ•´ç‰ˆï¼ˆä½¿ç”¨Qwen LoRAç”Ÿæˆè¿½é—®ï¼‰
"""
import streamlit as st
import torch
import torch.nn as nn
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    AutoModel,
    AutoModelForCausalLM,
    BitsAndBytesConfig
)
from peft import PeftModel
from pathlib import Path
import json
import tempfile
from datetime import datetime
import sys
import importlib.util

# ç›´æ¥å¯¼å…¥ResumeParser
spec = importlib.util.spec_from_file_location("resume_parser", "models/resume_parser.py")
resume_parser_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(resume_parser_module)
ResumeParser = resume_parser_module.ResumeParser

# å¯¼å…¥æ•°å­—äºº
spec_dh = importlib.util.spec_from_file_location("digital_human", "models/digital_human.py")
digital_human_module = importlib.util.module_from_spec(spec_dh)
spec_dh.loader.exec_module(digital_human_module)
DigitalHuman = digital_human_module.DigitalHuman

# å¯¼å…¥ Linly-Talker å®¢æˆ·ç«¯
spec_lt = importlib.util.spec_from_file_location("linly_talker_client", "models/linly_talker_client.py")
linly_module = importlib.util.module_from_spec(spec_lt)
spec_lt.loader.exec_module(linly_module)
LinlyTalkerClient = linly_module.LinlyTalkerClient

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="AIé¢è¯•æ•™ç»ƒ",
    page_icon="ğŸ¯",
    layout="wide"
)

# CSSæ ·å¼ - æ•°å­—äºº+èŠå¤©ç•Œé¢
st.markdown("""
<style>
    /* æ•°å­—äººåŒºåŸŸ */
    .avatar-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 20px;
        padding: 2.5rem 2rem;
        margin-bottom: 1rem;
        text-align: center;
        box-shadow: 0 10px 40px rgba(102, 126, 234, 0.3);
    }
    
    .avatar-wrapper {
        position: relative;
        display: inline-block;
    }
    
    .avatar-image {
        width: 160px;
        height: 160px;
        border-radius: 50%;
        border: 5px solid white;
        box-shadow: 0 8px 25px rgba(0, 0, 0, 0.2);
        animation: gentle-pulse 3s ease-in-out infinite;
        transition: transform 0.3s ease;
    }
    
    .avatar-image:hover {
        transform: scale(1.1) rotate(5deg);
    }
    
    @keyframes gentle-pulse {
        0%, 100% { 
            transform: scale(1);
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.2);
        }
        50% { 
            transform: scale(1.03);
            box-shadow: 0 8px 35px rgba(102, 126, 234, 0.4);
        }
    }
    
    .status-indicator {
        position: absolute;
        bottom: 10px;
        right: 10px;
        width: 20px;
        height: 20px;
        background: #4ade80;
        border-radius: 50%;
        border: 3px solid white;
        animation: blink 2s ease-in-out infinite;
    }
    
    @keyframes blink {
        0%, 100% { opacity: 1; }
        50% { opacity: 0.5; }
    }
    
    .avatar-speech {
        background: white;
        border-radius: 20px;
        padding: 1.5rem;
        margin-top: 1.5rem;
        position: relative;
        font-size: 1.1rem;
        color: #333;
        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
        animation: fade-in 0.5s ease;
    }
    
    @keyframes fade-in {
        from { opacity: 0; transform: translateY(-10px); }
        to { opacity: 1; transform: translateY(0); }
    }
    
    .avatar-speech::before {
        content: '';
        position: absolute;
        top: -10px;
        left: 50%;
        transform: translateX(-50%);
        width: 0;
        height: 0;
        border-left: 12px solid transparent;
        border-right: 12px solid transparent;
        border-bottom: 12px solid white;
    }
    
    /* èŠå¤©æ¶ˆæ¯ */
    .chat-message {
        margin-bottom: 1rem;
        display: flex;
    }
    
    .chat-message.ai {
        justify-content: flex-start;
    }
    
    .chat-message.user {
        justify-content: flex-end;
    }
    
    .message-bubble {
        max-width: 70%;
        padding: 0.8rem 1rem;
        border-radius: 12px;
    }
    
    .chat-message.ai .message-bubble {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        box-shadow: 0 3px 10px rgba(102, 126, 234, 0.3);
        border-bottom-left-radius: 4px;
    }
    
    .chat-message.user .message-bubble {
        background: white;
        color: #333;
        border: 1px solid #e0e0e0;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
        border-bottom-right-radius: 4px;
    }
    
    .score-card {
        background: white;
        border-radius: 12px;
        padding: 1rem;
        margin-bottom: 1rem;
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
</style>
""", unsafe_allow_html=True)

# ==================== å®šä¹‰æ¨¡å‹ ====================
class MultiTaskRoBERTa(nn.Module):
    """RoBERTaå¤šä»»åŠ¡æ¨¡å‹"""
    def __init__(self, model_name, num_labels=4):
        super().__init__()
        self.roberta = AutoModel.from_pretrained(model_name)
        hidden_size = self.roberta.config.hidden_size
        
        self.classification_head = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, num_labels)
        )
        
        self.regression_head = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, 1),
            nn.Sigmoid()
        )
    
    def forward(self, input_ids, attention_mask):
        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0, :]
        
        classification_logits = self.classification_head(pooled_output)
        regression_score = self.regression_head(pooled_output).squeeze(-1)
        
        return classification_logits, regression_score

# ==================== åŠ è½½æ¨¡å‹ ====================
@st.cache_resource
def load_models():
    """åŠ è½½æ‰€æœ‰å¾®è°ƒåçš„æ¨¡å‹"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 1. BERTå†³ç­–æ¨¡å‹
    bert_path = "./checkpoints/follow_up_classifier_1500"
    bert_tokenizer = AutoTokenizer.from_pretrained(bert_path)
    bert_model = AutoModelForSequenceClassification.from_pretrained(bert_path)
    bert_model.to(device)
    bert_model.eval()
    
    # 2. RoBERTaè¯„ä¼°æ¨¡å‹
    roberta_path = "./checkpoints/answer_evaluator"
    roberta_base = "./models/chinese-roberta-wwm-ext"
    roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_path)
    roberta_model = MultiTaskRoBERTa(roberta_base, num_labels=4)
    
    model_file = Path(roberta_path) / "pytorch_model.bin"
    state_dict = torch.load(model_file, map_location='cpu', weights_only=False)
    roberta_model.load_state_dict(state_dict)
    roberta_model.to(device)
    roberta_model.eval()
    
    # 3. Qwen LoRAæ¨¡å‹
    qwen_base = "Qwen/Qwen2-1.5B-Instruct"
    lora_path = "./checkpoints/qwen_interviewer_lora"
    
    qwen_tokenizer = None
    qwen_model = None
    
    if Path(lora_path).exists():
        try:
            # ç®€åŒ–é…ç½®ï¼Œä½¿ç”¨fp16è€Œä¸æ˜¯8bité‡åŒ–
            qwen_tokenizer = AutoTokenizer.from_pretrained(qwen_base, trust_remote_code=True)
            qwen_base_model = AutoModelForCausalLM.from_pretrained(
                qwen_base,
                device_map="auto",
                trust_remote_code=True,
                torch_dtype=torch.float16
            )
            
            qwen_model = PeftModel.from_pretrained(qwen_base_model, lora_path)
            qwen_model.eval()
            # åŠ è½½æˆåŠŸï¼ˆåœ¨ç¼“å­˜å¤–æ˜¾ç¤ºçŠ¶æ€ï¼‰
        except Exception as e:
            # åŠ è½½å¤±è´¥ï¼ˆåœ¨ç¼“å­˜å¤–æ˜¾ç¤ºçŠ¶æ€ï¼‰
            print(f"[ERROR] Qwen loading failed: {str(e)}")
            qwen_model = None
    
    return {
        'bert_model': bert_model,
        'bert_tokenizer': bert_tokenizer,
        'roberta_model': roberta_model,
        'roberta_tokenizer': roberta_tokenizer,
        'qwen_model': qwen_model,
        'qwen_tokenizer': qwen_tokenizer,
        'device': device
    }

# ==================== è¾…åŠ©å‡½æ•° ====================
def generate_initial_question(models, skills, job_position):
    """ä½¿ç”¨Qwenç”Ÿæˆç¬¬ä¸€ä¸ªé¢è¯•é—®é¢˜"""
    if not models['qwen_model'] or not models['qwen_tokenizer']:
        return "[é”™è¯¯] Qwenæ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆå¼€åœºé—®é¢˜ã€‚è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ã€‚"
    
    try:
        # æ ¹æ®ç®€å†æŠ€èƒ½å’ŒèŒä½ç”Ÿæˆå‹å¥½çš„å¼€åœºç™½
        skills_str = 'ã€'.join(skills[:3]) if skills else 'æŠ€æœ¯'
        
        system_msg = f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šã€å‹å¥½çš„æŠ€æœ¯é¢è¯•å®˜ï¼Œæ­£åœ¨é¢è¯•åº”è˜{job_position}çš„å€™é€‰äººã€‚"
        
        prompt = f"""å€™é€‰äººç®€å†æŠ€èƒ½ï¼š{skills_str}
åº”è˜èŒä½ï¼š{job_position}

ä»»åŠ¡ï¼šç”Ÿæˆé¢è¯•å¼€åœºé—®å€™ã€‚è¦æ±‚ï¼š
1. ä»¥"æ‚¨å¥½"æˆ–"æ¬¢è¿"å¼€å¤´
2. ç®€çŸ­ç¤¼è²Œåœ°æ¬¢è¿å€™é€‰äºº
3. è¯·å€™é€‰äººåšè‡ªæˆ‘ä»‹ç»æˆ–ä»‹ç»é¡¹ç›®ç»å†
4. 30å­—ä»¥å†…ï¼Œè¯­æ°”äº²åˆ‡è‡ªç„¶

ç¤ºä¾‹æ ¼å¼ï¼š
- "æ‚¨å¥½ï¼Œæ¬¢è¿å‚åŠ é¢è¯•ï¼å…ˆè¯·æ‚¨ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±å’Œæœ€è¿‘çš„é¡¹ç›®ç»å†å§ã€‚"
- "æ‚¨å¥½ï¼å¾ˆé«˜å…´è§åˆ°æ‚¨ã€‚èƒ½å…ˆè°ˆè°ˆæ‚¨æœ€è¿‘åœ¨{skills_str}æ–¹é¢çš„é¡¹ç›®ç»éªŒå—ï¼Ÿ"

è¯·ç”Ÿæˆç±»ä¼¼é£æ ¼çš„å¼€åœºé—®å€™ï¼š"""
        
        messages = [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": prompt}
        ]
        
        text = models['qwen_tokenizer'].apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = models['qwen_tokenizer'](
            text,
            return_tensors='pt',
            truncation=True,
            max_length=512
        ).to(models['device'])
        
        with torch.no_grad():
            outputs = models['qwen_model'].generate(
                **inputs,
                max_new_tokens=80,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.15,
                do_sample=True,
                pad_token_id=models['qwen_tokenizer'].eos_token_id
            )
        
        generated = models['qwen_tokenizer'].decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        ).strip()
        
        if not generated:
            return "[é”™è¯¯] Qwenç”Ÿæˆå¤±è´¥ï¼Œè¯·é‡è¯•ã€‚"
        return generated
    
    except Exception as e:
        print(f"[ERROR] Initial question generation failed: {e}")
        return f"[é”™è¯¯] Qwenç”Ÿæˆå¼‚å¸¸: {str(e)}"

def generate_qwen_question(models, context, question_type='follow_up'):
    """ä½¿ç”¨Qwen LoRAç”Ÿæˆé—®é¢˜"""
    if not models['qwen_model'] or not models['qwen_tokenizer']:
        return "[é”™è¯¯] Qwenæ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆé—®é¢˜ã€‚"
    
    try:
        # æ„å»ºpromptï¼ˆä¸¥æ ¼æŒ‰ç…§è®­ç»ƒæ•°æ®æ ¼å¼ï¼‰
        if question_type == 'follow_up':
            # è¿½é—®ï¼šå’Œè®­ç»ƒæ•°æ®æ ¼å¼å®Œå…¨ä¸€è‡´
            answer = context['last_answer']
            question = context['question']
            topic = context.get('topic', 'æŠ€æœ¯')
            score = context.get('score', 70)  # è·å–å½“å‰å›ç­”è´¨é‡
            
            # æ ¹æ®å›ç­”è´¨é‡å†³å®šä»»åŠ¡æè¿°
            if score >= 80:
                # é¼“åŠ±+è¿½é—®
                task = "å¯¹å€™é€‰äººçš„å›ç­”ç»™äºˆè‚¯å®šå’Œé¼“åŠ±ï¼Œç„¶åç»§ç»­è¿½é—®"
            else:
                # ç›´æ¥è¿½é—®
                task = f"æ ¹æ®å€™é€‰äººçš„å›ç­”ï¼Œç”Ÿæˆä¸€ä¸ªè¿½é—®é—®é¢˜ï¼Œæ·±å…¥è€ƒå¯Ÿå€™é€‰äººå¯¹{topic}çš„ç†è§£"
            
            prompt = f"""é¢è¯•å®˜é—®é¢˜ï¼š{question}
å€™é€‰äººå›ç­”ï¼š{answer}

ä»»åŠ¡ï¼š{task}"""
            
            system_msg = f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šã€å‹å¥½çš„{topic}é¢è¯•å®˜ï¼Œæ­£åœ¨é¢è¯•å€™é€‰äººã€‚ä½ éœ€è¦æ ¹æ®å€™é€‰äººçš„å›ç­”ï¼Œå†³å®šæ˜¯ç»§ç»­æ·±å…¥è¿½é—®ã€æ¢è¯é¢˜ï¼Œè¿˜æ˜¯ç»™äºˆé¼“åŠ±ã€‚"
        
        else:
            # æ¢è¯é¢˜ï¼ˆtopic_changeï¼‰
            answer = context.get('last_answer', '')
            question = context.get('question', '')
            topic = context.get('topic', 'æŠ€æœ¯')
            score = context.get('score', 70)
            skills = context.get('skills', ['æŠ€æœ¯'])
            history = context.get('history', [])
            
            # æ‰¾å‡ºè¿˜æ²¡é—®è¿‡çš„æŠ€èƒ½
            asked_topics = set()
            for qa in history[-5:]:  # åªçœ‹æœ€è¿‘5è½®
                q = qa.get('question', '')
                for skill in skills:
                    if skill in q:
                        asked_topics.add(skill)
            
            remaining_skills = [s for s in skills if s not in asked_topics]
            next_skill = remaining_skills[0] if remaining_skills else skills[0]
            
            # æ ¹æ®å›ç­”è´¨é‡å†³å®šä»»åŠ¡æè¿°
            if score >= 80:
                # å›ç­”å¾—å¾ˆå¥½ï¼Œæ¢è¯é¢˜
                task = f"å€™é€‰äººå¯¹{topic}å›ç­”å¾—å¾ˆå¥½ï¼Œå·²ç»å……åˆ†è€ƒå¯Ÿï¼Œå¯ä»¥æ¢ä¸€ä¸ªæ–°è¯é¢˜"
            else:
                # å›ç­”ä¸å¥½ï¼Œæ¢è¯é¢˜
                task = f"å€™é€‰äººå¯¹{topic}ä¸äº†è§£æˆ–ç­”ä¸ä¸Šæ¥ï¼Œéœ€è¦å‹å¥½åœ°æ¢ä¸€ä¸ªè¯é¢˜"
            
            prompt = f"""é¢è¯•å®˜é—®é¢˜ï¼š{question}
å€™é€‰äººå›ç­”ï¼š{answer}

ä»»åŠ¡ï¼š{task}"""
            
            system_msg = f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šã€å‹å¥½çš„æŠ€æœ¯é¢è¯•å®˜ï¼Œæ­£åœ¨é¢è¯•å€™é€‰äººã€‚ä½ éœ€è¦æ ¹æ®å€™é€‰äººçš„å›ç­”ï¼Œå†³å®šæ˜¯ç»§ç»­æ·±å…¥è¿½é—®ã€æ¢è¯é¢˜ï¼Œè¿˜æ˜¯ç»™äºˆé¼“åŠ±ã€‚"
        
        messages = [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": prompt}
        ]
        
        text = models['qwen_tokenizer'].apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = models['qwen_tokenizer'](
            [text],
            return_tensors="pt",
            padding=True
        ).to(models['device'])
        
        with torch.no_grad():
            outputs = models['qwen_model'].generate(
                **inputs,
                max_new_tokens=60,
                temperature=0.5,  # é™ä½æ¸©åº¦ï¼Œæ›´èšç„¦
                top_p=0.85,       # å‡å°‘éšæœºæ€§
                do_sample=True,
                repetition_penalty=1.1,  # é¿å…é‡å¤
                pad_token_id=models['qwen_tokenizer'].pad_token_id
            )
        
        response = models['qwen_tokenizer'].decode(
            outputs[0][len(inputs.input_ids[0]):],
            skip_special_tokens=True
        ).strip()
        
        # æ¸…ç†è¾“å‡º
        response = response.split('\n')[0][:100]
        
        if not response:
            return "[é”™è¯¯] Qwenç”Ÿæˆä¸ºç©ºï¼Œè¯·é‡è¯•ã€‚"
        return response
        
    except Exception as e:
        print(f"[ERROR] Qwen generation failed: {str(e)}")
        return f"[é”™è¯¯] Qwenç”Ÿæˆå¼‚å¸¸: {str(e)}"

def evaluate_answer(models, question, answer, history_qa):
    """RoBERTaè¯„ä¼°å›ç­”"""
    input_parts = []
    if history_qa:
        input_parts.append("[å†å²é—®ç­”]")
        for i, h in enumerate(history_qa[-3:], 1):
            input_parts.append(f"Q{i}: {h['question']}")
            input_parts.append(f"A{i}: {h['answer'][:100]}")
            input_parts.append(f"è´¨é‡: {h['quality']}")
    
    input_parts.append("[å½“å‰é—®ç­”]")
    input_parts.append(f"é—®é¢˜: {question}")
    input_parts.append(f"å›ç­”: {answer}")
    input_parts.append(f"æµç•…åº¦: 0.85")
    
    input_text = "\n".join(input_parts)
    
    inputs = models['roberta_tokenizer'](
        input_text,
        return_tensors='pt',
        truncation=True,
        max_length=256,
        padding=True
    ).to(models['device'])
    
    with torch.no_grad():
        cls_logits, reg_score = models['roberta_model'](
            inputs['input_ids'],
            inputs['attention_mask']
        )
        
        cls_probs = torch.softmax(cls_logits, dim=-1)
        predicted_idx = cls_probs.argmax().item()
        confidence = cls_probs.max().item()
        overall_score = reg_score.item() * 100
    
    label_names = ["å·®", "ä¸€èˆ¬", "è‰¯å¥½", "ä¼˜ç§€"]
    score_mapping = [50, 70, 85, 95]
    
    return {
        'current_label': label_names[predicted_idx],
        'current_score': score_mapping[predicted_idx],
        'overall_score': overall_score,
        'confidence': confidence
    }

def decide_next_action(models, question, answer, follow_up_depth, topic):
    """BERTå†³ç­–ä¸‹ä¸€æ­¥ï¼ˆä¸¥æ ¼æŒ‰ç…§è®­ç»ƒæ ¼å¼ï¼‰"""
    answer_length = len(answer)
    
    # è®¡ç®—çŠ¹è±«åº¦ï¼ˆåŸºäºè¯­æ°”è¯å’Œåœé¡¿ï¼‰
    hesitation_words = ['å—¯', 'å•Š', 'è¿™ä¸ª', 'é‚£ä¸ª', 'å°±æ˜¯', 'æ€ä¹ˆè¯´', '...']
    hesitation_count = sum(answer.count(word) for word in hesitation_words)
    hesitation_score = min(0.9, hesitation_count * 0.15)
    
    # å’Œè®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´çš„æ ¼å¼
    features = f"è¿½é—®æ·±åº¦:{follow_up_depth} " \
              f"çŠ¹è±«åº¦:{hesitation_score:.2f} " \
              f"é•¿åº¦:{answer_length}å­— " \
              f"è¯é¢˜:{topic}"
    
    bert_input = f"{question}[SEP]{answer}[SEP]{features}"
    
    inputs = models['bert_tokenizer'](
        bert_input,
        return_tensors='pt',
        truncation=True,
        max_length=512,
        padding=True
    ).to(models['device'])
    
    with torch.no_grad():
        outputs = models['bert_model'](**inputs)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=-1)
        predicted = probs.argmax().item()
        conf = probs.max().item()
    
    bert_labels = ["FOLLOW_UP", "NEXT_TOPIC"]
    
    # æ¨æ–­å†³ç­–ç†ç”±ï¼ˆåŸºäºè®­ç»ƒæ•°æ®ä¸­çš„reasonæ¨¡å¼ï¼‰
    reason = _infer_decision_reason(answer, answer_length, hesitation_score, 
                                     follow_up_depth, bert_labels[predicted], topic)
    
    return {
        'action': bert_labels[predicted],
        'confidence': conf,
        'probs': probs[0].tolist(),
        'reason': reason,
        'hesitation_score': hesitation_score
    }

def _infer_decision_reason(answer, answer_length, hesitation_score, follow_up_depth, action, topic):
    """æ¨æ–­å†³ç­–ç†ç”±ï¼ˆåŸºäºè®­ç»ƒæ•°æ®ä¸­çš„reasonæ¨¡å¼ï¼‰"""
    
    if action == "NEXT_TOPIC":
        # NEXT_TOPICçš„å¸¸è§åŸå› 
        if any(word in answer for word in ['ä¸äº†è§£', 'ä¸æ‡‚', 'æ²¡å­¦è¿‡', 'æ²¡æ¥è§¦']):
            return f"å€™é€‰è€…æ˜ç¡®è¡¨ç¤ºä¸äº†è§£{topic}ï¼Œåº”æ¢å…¶ä»–è¯é¢˜"
        elif follow_up_depth >= 2 and hesitation_score > 0.4:
            return f"ç»è¿‡{follow_up_depth}è½®è¿½é—®ï¼Œå€™é€‰è€…å¯¹{topic}çš„ç†è§£ä»ç„¶æ¨¡ç³Š/ä¸è¶³ï¼Œå»ºè®®æ¢è¯é¢˜"
        elif answer_length < 20:
            return f"å€™é€‰è€…å›ç­”è¿‡äºç®€çŸ­ä¸”ç¼ºä¹å®è´¨å†…å®¹ï¼Œå»ºè®®æ¢å…¶ä»–è¯é¢˜"
        else:
            return "æ ¹æ®ç»¼åˆåˆ†æï¼Œå»ºè®®æ¢è¯é¢˜è€ƒå¯Ÿå…¶ä»–æŠ€èƒ½ç‚¹"
    
    else:  # FOLLOW_UP
        # FOLLOW_UPçš„å¸¸è§åŸå› 
        if any(word in answer for word in ['ç”¨è¿‡', 'åšè¿‡', 'é¡¹ç›®']) and answer_length < 80:
            return f"å€™é€‰è€…æåˆ°äº†ä½¿ç”¨åœºæ™¯ä½†ç¼ºå°‘ç»†èŠ‚ï¼Œå¯ä»¥ç»§ç»­è¿½é—®æ›´æ·±å…¥çš„æŠ€æœ¯ç»†èŠ‚"
        elif hesitation_score > 0.3 and any(word in answer for word in ['æ¥è§¦è¿‡', 'äº†è§£', 'ç”¨åˆ°']):
            return f"å€™é€‰è€…æ‰¿è®¤ä½¿ç”¨è¿‡{topic}ä½†æœªå±•å¼€ç»†èŠ‚ï¼ˆæåˆ°äº†'æ‰¿è®¤ç”¨è¿‡ä½†è¯´ä¸æ¸…ç»†èŠ‚'ï¼‰ï¼Œéœ€è¿½é—®å…·ä½“å®ç°"
        elif answer_length >= 50:
            return "å€™é€‰è€…ç»™å‡ºäº†ä¸€å®šå†…å®¹ï¼Œå¯ä»¥é’ˆå¯¹å›ç­”ä¸­çš„å…³é”®ç‚¹ç»§ç»­æ·±å…¥è¿½é—®"
        else:
            return "å¯ä»¥ç»§ç»­è¿½é—®ä»¥æ›´å…¨é¢è¯„ä¼°å€™é€‰è€…çš„æŠ€æœ¯èƒ½åŠ›"

# ==================== ä¸»ç•Œé¢ ====================
st.title("ğŸ¯ AIé¢è¯•æ•™ç»ƒç³»ç»Ÿ")
st.markdown("### åŸºäºå¾®è°ƒTransformeræ¨¡å‹çš„æ™ºèƒ½é¢è¯•")

# åˆå§‹åŒ–
if 'stage' not in st.session_state:
    st.session_state.stage = 'upload'
if 'resume_data' not in st.session_state:
    st.session_state.resume_data = None
if 'current_question' not in st.session_state:
    st.session_state.current_question = None
if 'qa_history' not in st.session_state:
    st.session_state.qa_history = []
if 'follow_up_depth' not in st.session_state:
    st.session_state.follow_up_depth = 0
if 'current_topic' not in st.session_state:
    st.session_state.current_topic = 'Python'
if 'total_rounds' not in st.session_state:
    st.session_state.total_rounds = 0
if 'job_position' not in st.session_state:
    st.session_state.job_position = 'Pythonåç«¯å·¥ç¨‹å¸ˆ'
if 'digital_human' not in st.session_state:
    st.session_state.digital_human = DigitalHuman()
if 'linly_client' not in st.session_state:
    st.session_state.linly_client = LinlyTalkerClient()
if 'digital_human_mode' not in st.session_state:
    # é»˜è®¤ä½¿ç”¨è½»é‡çº§æ¨¡å¼ï¼Œå¦‚æœLinlyæœåŠ¡å¯ç”¨åˆ™å¯ä»¥åˆ‡æ¢
    st.session_state.digital_human_mode = 'lightweight'
if 'avatar_image_path' not in st.session_state:
    st.session_state.avatar_image_path = "Linly-Talker/examples/source_image/full_body_1.png"

# ==================== é˜¶æ®µ1: ä¸Šä¼ ç®€å† ====================
if st.session_state.stage == 'upload':
    st.markdown("---")
    st.subheader("ğŸ“„ æ­¥éª¤1ï¼šä¸Šä¼ ç®€å†")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        upload_method = st.radio(
            "é€‰æ‹©ä¸Šä¼ æ–¹å¼",
            ["ä¸Šä¼ æ–‡ä»¶ (PDF/DOCX)", "ç›´æ¥ç²˜è´´æ–‡æœ¬"],
            horizontal=True
        )
        
        resume_text = ""
        
        if upload_method == "ä¸Šä¼ æ–‡ä»¶ (PDF/DOCX)":
            uploaded_file = st.file_uploader(
                "ä¸Šä¼ ç®€å†æ–‡ä»¶",
                type=['pdf', 'docx'],
                help="æ”¯æŒPDFå’ŒDOCXæ ¼å¼"
            )
            
            if uploaded_file:
                with tempfile.NamedTemporaryFile(delete=False, suffix=Path(uploaded_file.name).suffix) as tmp_file:
                    tmp_file.write(uploaded_file.getbuffer())
                    tmp_path = tmp_file.name
                
                try:
                    with st.spinner("æ­£åœ¨è§£æç®€å†..."):
                        parser = ResumeParser()
                        resume_data = parser.parse(tmp_path)
                        st.session_state.resume_data = resume_data
                    
                    st.success(f"âœ… ç®€å†è§£ææˆåŠŸï¼")
                    
                    with st.expander("ğŸ“‹ æŸ¥çœ‹è§£æç»“æœ"):
                        st.markdown(f"**å§“åï¼š** {resume_data['name']}")
                        if resume_data['contact']:
                            st.markdown(f"**è”ç³»æ–¹å¼ï¼š** {resume_data['contact']}")
                        st.markdown(f"**æå–åˆ°çš„æŠ€èƒ½ï¼š** {', '.join(resume_data['skills'][:10])}")
                    
                    resume_text = resume_data['raw_text']
                    
                except Exception as e:
                    st.error(f"ç®€å†è§£æå¤±è´¥ï¼š{str(e)}")
                finally:
                    Path(tmp_path).unlink(missing_ok=True)
        else:
            resume_text = st.text_area(
                "ç²˜è´´ç®€å†å†…å®¹",
                height=300,
                placeholder="å§“åï¼šå¼ ä¸‰\nåº”è˜èŒä½ï¼šPythonåç«¯å·¥ç¨‹å¸ˆ\n\næŠ€èƒ½ï¼šPython, Django, MySQL..."
            )
        
        job_position = st.text_input("åº”è˜èŒä½", value="Pythonåç«¯å·¥ç¨‹å¸ˆ")
    
    with col2:
        st.info("ğŸ’¡ **æ™ºèƒ½ç‰¹æ€§**\n\nâœ… PDF/DOCXè§£æ\n\nâœ… BERTæ™ºèƒ½å†³ç­–\n\nâœ… **Qwen LoRAè¿½é—®**\n\nâœ… RoBERTaè¯„ä¼°\n\nâœ… **æ— å›ºå®šè½®æ•°**")
        
        if st.session_state.resume_data:
            st.success(f"**å·²è¯†åˆ«æŠ€èƒ½ï¼š**\n\n" + "\n".join([f"â€¢ {s}" for s in st.session_state.resume_data['skills'][:5]]))
    
    if st.button("ğŸš€ å¼€å§‹é¢è¯•", type="primary", use_container_width=True):
        if resume_text.strip() or st.session_state.resume_data:
            if not st.session_state.resume_data and resume_text:
                parser = ResumeParser()
                st.session_state.resume_data = {
                    'name': 'å€™é€‰äºº',
                    'skills': [s for s in ['Python', 'Java', 'JavaScript'] if s.lower() in resume_text.lower()],
                    'raw_text': resume_text
                }
            
            # æ ‡è®°ä¸ºè¿›å…¥é¢è¯•é˜¶æ®µï¼Œç¬¬ä¸€ä¸ªé—®é¢˜å°†åœ¨é¢è¯•é˜¶æ®µåŠ¨æ€ç”Ÿæˆ
            skills = st.session_state.resume_data['skills'] or ['æŠ€æœ¯']
            first_skill = skills[0] if skills else 'æŠ€æœ¯'
            st.session_state.current_topic = first_skill
            st.session_state.current_question = None  # æ ‡è®°ä¸ºéœ€è¦ç”Ÿæˆ
            st.session_state.job_position = job_position
            st.session_state.stage = 'interview'
            st.rerun()
        else:
            st.error("è¯·å…ˆä¸Šä¼ ç®€å†æˆ–ç²˜è´´æ–‡æœ¬")

# ==================== é˜¶æ®µ2: é¢è¯•è¿‡ç¨‹ ====================
elif st.session_state.stage == 'interview':
    with st.spinner("æ­£åœ¨åŠ è½½AIæ¨¡å‹..."):
        models = load_models()
    
    # åœ¨ä¾§è¾¹æ æ˜¾ç¤ºæ¨¡å‹åŠ è½½çŠ¶æ€ï¼ˆç¼“å­˜å¤–ï¼‰
    with st.sidebar:
        st.markdown("### ğŸ” æ¨¡å‹çŠ¶æ€")
        if models['qwen_model'] is not None:
            st.success("âœ… Qwen LoRAå·²åŠ è½½")
        else:
            st.error("âŒ QwenæœªåŠ è½½ - æ— æ³•ç»§ç»­é¢è¯•")
        st.success("âœ… BERTå†³ç­–æ¨¡å‹")
        st.success("âœ… RoBERTaè¯„ä¼°æ¨¡å‹")
        st.markdown("---")
        
        # æ•°å­—äººæ¨¡å¼é€‰æ‹©
        st.markdown("### ğŸ­ æ•°å­—äººæ¨¡å¼")
        
        # æ£€æŸ¥ Linly-Talker æœåŠ¡çŠ¶æ€
        linly_status = "âœ… å¯ç”¨" if st.session_state.linly_client.available else "âŒ æœªå¯åŠ¨"
        st.caption(f"Linly-Talker: {linly_status}")
        
        if st.session_state.linly_client.available:
            mode_options = {
                "lightweight": "ğŸ’¨ è½»é‡çº§ (Edge-TTS + é™æ€å¤´åƒ)",
                "linly": "ğŸ¬ é«˜è´¨é‡ (Linly-Talker + å”‡åŒæ­¥è§†é¢‘)"
            }
        else:
            mode_options = {
                "lightweight": "ğŸ’¨ è½»é‡çº§ (Edge-TTS + é™æ€å¤´åƒ)"
            }
        
        selected_mode = st.radio(
            "é€‰æ‹©æ•°å­—äººæ¨¡å¼",
            list(mode_options.keys()),
            format_func=lambda x: mode_options[x],
            index=0,
            key="dh_mode_selector"
        )
        
        if selected_mode != st.session_state.digital_human_mode:
            st.session_state.digital_human_mode = selected_mode
        
        if not st.session_state.linly_client.available:
            with st.expander("ğŸ”§ å¯åŠ¨ Linly-Talker"):
                st.code("python start_linly_services.py", language="bash")
                st.caption("åœ¨æ–°ç»ˆç«¯è¿è¡Œä¸Šè¿°å‘½ä»¤å¯åŠ¨ Linly-Talker æœåŠ¡")
        
        st.markdown("---")
    
    # ç”Ÿæˆç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
    if st.session_state.current_question is None:
        with st.spinner("Qwenæ­£åœ¨ç”Ÿæˆå¼€åœºé—®é¢˜..."):
            skills = st.session_state.resume_data.get('skills', ['æŠ€æœ¯'])
            job_position = st.session_state.get('job_position', 'Pythonåç«¯å·¥ç¨‹å¸ˆ')
            st.session_state.current_question = generate_initial_question(models, skills, job_position)
    
    # é¡¶éƒ¨ä¿¡æ¯
    st.markdown(f"<div style='text-align: center;'><h3>ğŸ¯ AIé¢è¯•æ•™ç»ƒ</h3></div>", unsafe_allow_html=True)
    if st.session_state.resume_data:
        st.caption(f"ğŸ‘¤ {st.session_state.resume_data.get('name', 'å€™é€‰äºº')} | åº”è˜ï¼š{st.session_state.job_position} | å·²å®Œæˆï¼š{st.session_state.total_rounds}è½®")
    
    # ä¸»å¸ƒå±€ï¼šå·¦ä¾§(æ•°å­—äºº+å¯¹è¯) å³ä¾§(è¯„åˆ†)
    col_main, col_score = st.columns([2.5, 1])
    
    with col_main:
        # ========== æ•°å­—äººé¢è¯•å®˜ ==========
        if st.session_state.digital_human_mode == 'linly':
            # ä½¿ç”¨ Linly-Talker ç”Ÿæˆè§†é¢‘
            st.markdown('<div style="text-align: center; margin-bottom: 1rem;">', unsafe_allow_html=True)
            
            # ç”Ÿæˆè§†é¢‘ï¼ˆå¸¦ç¼“å­˜keyé¿å…é‡å¤ç”Ÿæˆï¼‰
            video_cache_key = f"video_{hash(st.session_state.current_question)}"
            if video_cache_key not in st.session_state:
                with st.spinner("ğŸ¬ æ•°å­—äººæ­£åœ¨ç”Ÿæˆè§†é¢‘..."):
                    try:
                        video_path = st.session_state.linly_client.generate_video(
                            text=st.session_state.current_question,
                            avatar_image=st.session_state.avatar_image_path
                        )
                        st.session_state[video_cache_key] = video_path
                    except Exception as e:
                        st.error(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")
                        st.info("å·²è‡ªåŠ¨åˆ‡æ¢åˆ°è½»é‡çº§æ¨¡å¼")
                        st.session_state.digital_human_mode = 'lightweight'
                        video_path = None
            else:
                video_path = st.session_state[video_cache_key]
            
            if video_path and Path(video_path).exists():
                st.markdown(f"""
                <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                            border-radius: 20px; padding: 1.5rem; margin-bottom: 1rem;">
                    <div style="color: white; font-size: 1.2rem; font-weight: 600; margin-bottom: 1rem; text-align: center;">
                        ğŸ¯ AIé¢è¯•å®˜ Â· {st.session_state.current_topic}
                    </div>
                </div>
                """, unsafe_allow_html=True)
                st.video(video_path, autoplay=True)
            else:
                # é™çº§åˆ°è½»é‡çº§æ¨¡å¼
                avatar_html = st.session_state.digital_human.get_avatar_html(
                    question=st.session_state.current_question,
                    topic=st.session_state.current_topic,
                    include_audio=True
                )
            st.markdown('</div>', unsafe_allow_html=True)
        else:
            # è½»é‡çº§æ¨¡å¼ï¼šä½¿ç”¨ Streamlit åŸç”Ÿç»„ä»¶
            # æ•°å­—äººå¤´åƒå’Œä¿¡æ¯
            col_avatar, col_info = st.columns([1, 2])
            
            with col_avatar:
                st.image("https://i.pravatar.cc/300?img=33", width=150)
            
            with col_info:
                st.markdown(f"### ğŸ¯ AIé¢è¯•å®˜ - Alex Chen")
                st.markdown(f"**è¯é¢˜ï¼š** {st.session_state.current_topic}")
                st.info("ğŸ’¬ æ­£åœ¨è¯´è¯ä¸­...")
            
            # æ˜¾ç¤ºé—®é¢˜
            st.markdown("---")
            st.success(f"**é¢è¯•å®˜æé—®ï¼š**\n\n{st.session_state.current_question}")
            
            # ç”Ÿæˆå¹¶æ’­æ”¾è¯­éŸ³
            try:
                audio_file = st.session_state.digital_human.text_to_speech(st.session_state.current_question)
                if audio_file and Path(audio_file).exists():
                    with open(audio_file, 'rb') as f:
                        audio_bytes = f.read()
                    st.audio(audio_bytes, format='audio/mp3', autoplay=True)
            except Exception as e:
                st.caption(f"è¯­éŸ³ç”Ÿæˆå¤±è´¥: {e}")
        
        # ========== å¯¹è¯å†å² ==========
        st.markdown("### ğŸ’¬ å¯¹è¯è®°å½•")
        
        # èŠå¤©å¼å†å²è®°å½•ï¼ˆåªæ˜¾ç¤ºæœ€è¿‘5è½®ï¼‰
        with st.container():
            st.markdown('<div style="max-height: 350px; overflow-y: auto; padding: 1rem; background: #f8f9fa; border-radius: 10px; margin-bottom: 1rem;">', unsafe_allow_html=True)
            
            if not st.session_state.qa_history:
                st.markdown('<div style="text-align: center; color: #999; padding: 2rem;">æš‚æ— å¯¹è¯è®°å½•</div>', unsafe_allow_html=True)
            else:
                for i, qa in enumerate(st.session_state.qa_history[-5:]):
                    quality_emoji = {'å·®': 'ğŸ”´', 'ä¸€èˆ¬': 'ğŸŸ¡', 'è‰¯å¥½': 'ğŸ”µ', 'ä¼˜ç§€': 'ğŸŸ¢'}.get(qa.get('quality', 'ä¸€èˆ¬'), 'âšª')
                    
                    # AIæ¶ˆæ¯
                    st.markdown(f'''
                    <div class="chat-message ai">
                        <div class="message-bubble">
                            <div style="font-size: 0.85rem; opacity: 0.9;">ğŸ¤– é¢è¯•å®˜</div>
                            <div style="margin-top: 0.3rem;">{qa["question"]}</div>
                        </div>
                    </div>
                    ''', unsafe_allow_html=True)
                    
                    # ç”¨æˆ·æ¶ˆæ¯
                    st.markdown(f'''
                    <div class="chat-message user">
                        <div class="message-bubble">
                            <div style="font-size: 0.85rem; opacity: 0.7;">ğŸ‘¤ å€™é€‰äºº</div>
                            <div style="margin-top: 0.3rem;">{qa["answer"]}</div>
                            <div style="font-size: 0.75rem; margin-top: 0.5rem; opacity: 0.8;">{quality_emoji} {qa.get('quality', 'ä¸€èˆ¬')}</div>
                        </div>
                    </div>
                    ''', unsafe_allow_html=True)
            
            st.markdown('</div>', unsafe_allow_html=True)
        
        # ========== å›ç­”è¾“å…¥ ==========
        st.markdown("### âœï¸ æ‚¨çš„å›ç­”")
        answer = st.text_area(
            "è¯·è¾“å…¥æ‚¨çš„å›ç­”",
            height=120,
            key=f"answer_{st.session_state.total_rounds}",
            placeholder="åœ¨æ­¤è¾“å…¥æ‚¨çš„å›ç­”..."
        )
        
        col_s, col_e = st.columns([3, 1])
        
        with col_s:
                if st.button("âœ… æäº¤å›ç­”", type="primary", use_container_width=True):
                    if answer.strip():
                        # è¯„ä¼°
                        with st.spinner("æ­£åœ¨è¯„ä¼°å›ç­”..."):
                            eval_result = evaluate_answer(models, st.session_state.current_question, answer, st.session_state.qa_history)
                        
                        # æ˜¾ç¤ºè¯„ä¼°è¯¦æƒ…
                        st.sidebar.markdown("### ğŸ“Š æœ¬è½®è¯„ä¼°")
                        st.sidebar.write(f"è´¨é‡: {eval_result['current_label']}")
                        st.sidebar.write(f"å½“å‰åˆ†: {eval_result['current_score']}")
                        st.sidebar.write(f"æ•´ä½“åˆ†: {eval_result['overall_score']:.1f}")
                        st.sidebar.write(f"ç½®ä¿¡åº¦: {eval_result['confidence']:.1%}")
                        
                        # è®°å½•
                        st.session_state.qa_history.append({
                            'question': st.session_state.current_question,
                            'answer': answer,
                            'quality': eval_result['current_label'],
                            'eval': eval_result
                        })
                        
                        st.session_state.total_rounds += 1
                        
                        # BERTå†³ç­–
                        with st.spinner("BERTæ­£åœ¨å†³ç­–..."):
                            decision = decide_next_action(
                                models,
                                st.session_state.current_question,
                                answer,
                                st.session_state.follow_up_depth,
                                st.session_state.current_topic
                            )
                        
                        # æ˜¾ç¤ºå†³ç­–è¯¦æƒ…
                        st.sidebar.markdown("### ğŸ§  BERTå†³ç­–")
                        st.sidebar.write(f"**åŠ¨ä½œ**: {decision['action']}")
                        st.sidebar.write(f"**ç½®ä¿¡åº¦**: {decision['confidence']:.1%}")
                        st.sidebar.write(f"**ç†ç”±**: {decision['reason']}")
                        st.sidebar.caption(f"çŠ¹è±«åº¦: {decision['hesitation_score']:.2f} | FOLLOW_UPæ¦‚ç‡: {decision['probs'][0]:.1%}")
                        
                        # ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜
                        if decision['action'] == 'FOLLOW_UP':
                            st.session_state.follow_up_depth += 1
                            # ä½¿ç”¨Qwenç”Ÿæˆè¿½é—®
                            st.sidebar.markdown("### ğŸ¤– Qwenç”Ÿæˆè¿½é—®")
                            with st.spinner("Qwenæ­£åœ¨ç”Ÿæˆè¿½é—®..."):
                                context = {
                                    'last_answer': answer,
                                    'question': st.session_state.current_question,
                                    'topic': st.session_state.current_topic,
                                    'score': eval_result['current_score']
                                }
                                new_q = generate_qwen_question(models, context, 'follow_up')
                                st.session_state.current_question = new_q
                                st.sidebar.write(f"**ç±»å‹**: è¿½é—®")
                                st.sidebar.write(f"**é—®é¢˜**: {new_q}")
                                if '[é”™è¯¯]' not in new_q:
                                    with st.sidebar.expander("æŸ¥çœ‹æç¤ºè¯"):
                                        st.code(f"é—®é¢˜: {context['question']}\nå›ç­”: {context['last_answer'][:50]}...", language="text")
                        else:
                            st.session_state.follow_up_depth = 0
                            # ä½¿ç”¨Qwenç”Ÿæˆæ–°è¯é¢˜é—®é¢˜
                            st.sidebar.markdown("### ğŸ¤– Qwenç”Ÿæˆæ–°é¢˜")
                            with st.spinner("Qwenæ­£åœ¨ç”Ÿæˆæ–°é—®é¢˜..."):
                                context = {
                                    'last_answer': answer,
                                    'question': st.session_state.current_question,
                                    'topic': st.session_state.current_topic,
                                    'score': eval_result['current_score'],
                                    'skills': st.session_state.resume_data.get('skills', []),
                                    'history': st.session_state.qa_history
                                }
                                new_q = generate_qwen_question(models, context, 'new_topic')
                                st.session_state.current_question = new_q
                                
                                # æ›´æ–°å½“å‰è¯é¢˜
                                skills = context.get('skills', [])
                                for skill in skills:
                                    if skill in new_q:
                                        st.session_state.current_topic = skill
                                        break
                                
                                st.sidebar.write(f"**ç±»å‹**: æ–°è¯é¢˜")
                                st.sidebar.write(f"**é—®é¢˜**: {new_q}")
                                if '[é”™è¯¯]' not in new_q:
                                    with st.sidebar.expander("æŸ¥çœ‹æç¤ºè¯"):
                                        st.code(f"æŠ€èƒ½: {', '.join(context.get('skills', []))}\nå·²é—®è½®æ•°: {len(context.get('history', []))}", language="text")
                        
                        st.rerun()
                    else:
                        st.error("è¯·è¾“å…¥å›ç­”")
            
        with col_e:
            if st.button("ğŸ ç»“æŸé¢è¯•", use_container_width=True):
                if st.session_state.qa_history:
                    st.session_state.stage = 'summary'
                    st.rerun()
    
    with col_score:
        # ========== è¯„åˆ†é¢æ¿ ==========
        st.markdown("### ğŸ“Š å®æ—¶è¯„åˆ†")
        
        if st.session_state.qa_history:
            latest = st.session_state.qa_history[-1]['eval']
            
            st.markdown(f"""
            <div class="score-card">
                <div style="color: #999; font-size: 0.9rem;">å½“å‰å›ç­”</div>
                <div style="font-size: 2rem; font-weight: bold; color: #667eea;">{latest['current_label']}</div>
                <div style="color: #999; font-size: 0.85rem;">{latest['current_score']}åˆ†</div>
            </div>
            
            <div class="score-card">
                <div style="color: #999; font-size: 0.9rem;">æ•´ä½“è¡¨ç°</div>
                <div style="font-size: 2rem; font-weight: bold; color: #667eea;">{latest['overall_score']:.1f}</div>
                <div style="color: #999; font-size: 0.85rem;">æ»¡åˆ†100</div>
            </div>
            
            <div class="score-card">
                <div style="color: #999; font-size: 0.9rem;">å·²å®Œæˆè½®æ•°</div>
                <div style="font-size: 2rem; font-weight: bold; color: #667eea;">{st.session_state.total_rounds}</div>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.info("å›ç­”é—®é¢˜åå°†æ˜¾ç¤ºè¯„åˆ†")
        
        st.markdown("---")
        
        # AIæ¨¡å‹çŠ¶æ€
        with st.expander("ğŸ” AIå†³ç­–è¯¦æƒ…", expanded=False):
            st.caption("æŸ¥çœ‹BERTã€RoBERTaè¯¦ç»†åˆ†æ")
            if st.session_state.qa_history:
                last_qa = st.session_state.qa_history[-1]
                st.json(last_qa.get('eval', {}))

# ==================== é˜¶æ®µ3: æ€»ç»“ ====================
elif st.session_state.stage == 'summary':
    st.markdown("---")
    st.subheader("ğŸ“ˆ é¢è¯•æ€»ç»“æŠ¥å‘Š")
    
    if st.session_state.qa_history:
        final_eval = st.session_state.qa_history[-1]['eval']
        final_score = final_eval['overall_score']
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("æ€»ä½“è¯„åˆ†", f"{final_score:.1f}/100")
        with col2:
            st.metric("æ€»è½®æ•°", st.session_state.total_rounds)
        with col3:
            avg_current = sum(qa['eval']['current_score'] for qa in st.session_state.qa_history) / len(st.session_state.qa_history)
            st.metric("å¹³å‡åˆ†", f"{avg_current:.0f}")
        with col4:
            excellent = sum(1 for qa in st.session_state.qa_history if qa['quality'] == 'ä¼˜ç§€')
            st.metric("ä¼˜ç§€", f"{excellent}ä¸ª")
        
        st.markdown("---")
        if final_score >= 85:
            st.success(f"### ğŸŒŸ å¼ºçƒˆæ¨è\nå€™é€‰äººè¡¨ç°ä¼˜ç§€ï¼ŒæŠ€æœ¯åŠŸåº•æ‰å®ã€‚")
        elif final_score >= 70:
            st.success(f"### ğŸ‘ æ¨è\nå€™é€‰äººå…·å¤‡ç›¸åº”æŠ€èƒ½ï¼Œè¡¨ç°è‰¯å¥½ã€‚")
        elif final_score >= 50:
            st.warning(f"### ğŸ¤” å¾…å®š\nå€™é€‰äººåŸºç¡€ä¸€èˆ¬ï¼Œéœ€è¿›ä¸€æ­¥è€ƒå¯Ÿã€‚")
        else:
            st.error(f"### âŒ ä¸æ¨è\nå€™é€‰äººæŠ€æœ¯èƒ½åŠ›ä¸è¶³ã€‚")
        
        st.markdown("---")
        st.markdown("### ğŸ“ å¯¹è¯è¯¦æƒ…")
        
        for i, qa in enumerate(st.session_state.qa_history, 1):
            with st.expander(f"ç¬¬{i}è½® - {qa['quality']} ({qa['eval']['current_score']}åˆ†)"):
                st.markdown(f"**Q:** {qa['question']}")
                st.markdown(f"**A:** {qa['answer']}")
                st.markdown(f"**è¯„åˆ†:** {qa['eval']['current_label']} ({qa['eval']['current_score']}åˆ†) | æ•´ä½“: {qa['eval']['overall_score']:.1f}/100")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ”„ é‡æ–°å¼€å§‹", type="primary", use_container_width=True):
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.rerun()
    
    with col2:
        if st.button("ğŸ“¥ å¯¼å‡ºæŠ¥å‘Š", use_container_width=True):
            report = {
                'time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'candidate': st.session_state.resume_data.get('name', 'æœªçŸ¥'),
                'final_score': final_score,
                'total_rounds': st.session_state.total_rounds,
                'qa_history': st.session_state.qa_history
            }
            
            st.download_button(
                "ğŸ’¾ ä¸‹è½½æŠ¥å‘Š",
                data=json.dumps(report, ensure_ascii=False, indent=2),
                file_name=f"interview_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json",
                use_container_width=True
            )

# ä¾§è¾¹æ 
with st.sidebar:
    st.markdown("### ğŸ“Š è®­ç»ƒæ•°æ®")
    st.caption("""
    - BERT: 1500æ¡
    - Qwen: 2000æ¡
    - RoBERTa: 2000æ¡
    """)


"""
import streamlit as st
import torch
import torch.nn as nn
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    AutoModel,
    AutoModelForCausalLM,
    BitsAndBytesConfig
)
from peft import PeftModel
from pathlib import Path
import json
import tempfile
from datetime import datetime
import sys
import importlib.util

# ç›´æ¥å¯¼å…¥ResumeParser
spec = importlib.util.spec_from_file_location("resume_parser", "models/resume_parser.py")
resume_parser_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(resume_parser_module)
ResumeParser = resume_parser_module.ResumeParser

# å¯¼å…¥æ•°å­—äºº
spec_dh = importlib.util.spec_from_file_location("digital_human", "models/digital_human.py")
digital_human_module = importlib.util.module_from_spec(spec_dh)
spec_dh.loader.exec_module(digital_human_module)
DigitalHuman = digital_human_module.DigitalHuman

# å¯¼å…¥ Linly-Talker å®¢æˆ·ç«¯
spec_lt = importlib.util.spec_from_file_location("linly_talker_client", "models/linly_talker_client.py")
linly_module = importlib.util.module_from_spec(spec_lt)
spec_lt.loader.exec_module(linly_module)
LinlyTalkerClient = linly_module.LinlyTalkerClient

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="AIé¢è¯•æ•™ç»ƒ",
    page_icon="ğŸ¯",
    layout="wide"
)

# CSSæ ·å¼ - æ•°å­—äºº+èŠå¤©ç•Œé¢
st.markdown("""
<style>
    /* æ•°å­—äººåŒºåŸŸ */
    .avatar-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 20px;
        padding: 2.5rem 2rem;
        margin-bottom: 1rem;
        text-align: center;
        box-shadow: 0 10px 40px rgba(102, 126, 234, 0.3);
    }
    
    .avatar-wrapper {
        position: relative;
        display: inline-block;
    }
    
    .avatar-image {
        width: 160px;
        height: 160px;
        border-radius: 50%;
        border: 5px solid white;
        box-shadow: 0 8px 25px rgba(0, 0, 0, 0.2);
        animation: gentle-pulse 3s ease-in-out infinite;
        transition: transform 0.3s ease;
    }
    
    .avatar-image:hover {
        transform: scale(1.1) rotate(5deg);
    }
    
    @keyframes gentle-pulse {
        0%, 100% { 
            transform: scale(1);
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.2);
        }
        50% { 
            transform: scale(1.03);
            box-shadow: 0 8px 35px rgba(102, 126, 234, 0.4);
        }
    }
    
    .status-indicator {
        position: absolute;
        bottom: 10px;
        right: 10px;
        width: 20px;
        height: 20px;
        background: #4ade80;
        border-radius: 50%;
        border: 3px solid white;
        animation: blink 2s ease-in-out infinite;
    }
    
    @keyframes blink {
        0%, 100% { opacity: 1; }
        50% { opacity: 0.5; }
    }
    
    .avatar-speech {
        background: white;
        border-radius: 20px;
        padding: 1.5rem;
        margin-top: 1.5rem;
        position: relative;
        font-size: 1.1rem;
        color: #333;
        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
        animation: fade-in 0.5s ease;
    }
    
    @keyframes fade-in {
        from { opacity: 0; transform: translateY(-10px); }
        to { opacity: 1; transform: translateY(0); }
    }
    
    .avatar-speech::before {
        content: '';
        position: absolute;
        top: -10px;
        left: 50%;
        transform: translateX(-50%);
        width: 0;
        height: 0;
        border-left: 12px solid transparent;
        border-right: 12px solid transparent;
        border-bottom: 12px solid white;
    }
    
    /* èŠå¤©æ¶ˆæ¯ */
    .chat-message {
        margin-bottom: 1rem;
        display: flex;
    }
    
    .chat-message.ai {
        justify-content: flex-start;
    }
    
    .chat-message.user {
        justify-content: flex-end;
    }
    
    .message-bubble {
        max-width: 70%;
        padding: 0.8rem 1rem;
        border-radius: 12px;
    }
    
    .chat-message.ai .message-bubble {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        box-shadow: 0 3px 10px rgba(102, 126, 234, 0.3);
        border-bottom-left-radius: 4px;
    }
    
    .chat-message.user .message-bubble {
        background: white;
        color: #333;
        border: 1px solid #e0e0e0;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
        border-bottom-right-radius: 4px;
    }
    
    .score-card {
        background: white;
        border-radius: 12px;
        padding: 1rem;
        margin-bottom: 1rem;
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
</style>
""", unsafe_allow_html=True)

# ==================== å®šä¹‰æ¨¡å‹ ====================
class MultiTaskRoBERTa(nn.Module):
    """RoBERTaå¤šä»»åŠ¡æ¨¡å‹"""
    def __init__(self, model_name, num_labels=4):
        super().__init__()
        self.roberta = AutoModel.from_pretrained(model_name)
        hidden_size = self.roberta.config.hidden_size
        
        self.classification_head = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, num_labels)
        )
        
        self.regression_head = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, 1),
            nn.Sigmoid()
        )
    
    def forward(self, input_ids, attention_mask):
        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0, :]
        
        classification_logits = self.classification_head(pooled_output)
        regression_score = self.regression_head(pooled_output).squeeze(-1)
        
        return classification_logits, regression_score

# ==================== åŠ è½½æ¨¡å‹ ====================
@st.cache_resource
def load_models():
    """åŠ è½½æ‰€æœ‰å¾®è°ƒåçš„æ¨¡å‹"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 1. BERTå†³ç­–æ¨¡å‹
    bert_path = "./checkpoints/follow_up_classifier_1500"
    bert_tokenizer = AutoTokenizer.from_pretrained(bert_path)
    bert_model = AutoModelForSequenceClassification.from_pretrained(bert_path)
    bert_model.to(device)
    bert_model.eval()
    
    # 2. RoBERTaè¯„ä¼°æ¨¡å‹
    roberta_path = "./checkpoints/answer_evaluator"
    roberta_base = "./models/chinese-roberta-wwm-ext"
    roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_path)
    roberta_model = MultiTaskRoBERTa(roberta_base, num_labels=4)
    
    model_file = Path(roberta_path) / "pytorch_model.bin"
    state_dict = torch.load(model_file, map_location='cpu', weights_only=False)
    roberta_model.load_state_dict(state_dict)
    roberta_model.to(device)
    roberta_model.eval()
    
    # 3. Qwen LoRAæ¨¡å‹
    qwen_base = "Qwen/Qwen2-1.5B-Instruct"
    lora_path = "./checkpoints/qwen_interviewer_lora"
    
    qwen_tokenizer = None
    qwen_model = None
    
    if Path(lora_path).exists():
        try:
            # ç®€åŒ–é…ç½®ï¼Œä½¿ç”¨fp16è€Œä¸æ˜¯8bité‡åŒ–
            qwen_tokenizer = AutoTokenizer.from_pretrained(qwen_base, trust_remote_code=True)
            qwen_base_model = AutoModelForCausalLM.from_pretrained(
                qwen_base,
                device_map="auto",
                trust_remote_code=True,
                torch_dtype=torch.float16
            )
            
            qwen_model = PeftModel.from_pretrained(qwen_base_model, lora_path)
            qwen_model.eval()
            # åŠ è½½æˆåŠŸï¼ˆåœ¨ç¼“å­˜å¤–æ˜¾ç¤ºçŠ¶æ€ï¼‰
        except Exception as e:
            # åŠ è½½å¤±è´¥ï¼ˆåœ¨ç¼“å­˜å¤–æ˜¾ç¤ºçŠ¶æ€ï¼‰
            print(f"[ERROR] Qwen loading failed: {str(e)}")
            qwen_model = None
    
    return {
        'bert_model': bert_model,
        'bert_tokenizer': bert_tokenizer,
        'roberta_model': roberta_model,
        'roberta_tokenizer': roberta_tokenizer,
        'qwen_model': qwen_model,
        'qwen_tokenizer': qwen_tokenizer,
        'device': device
    }

# ==================== è¾…åŠ©å‡½æ•° ====================
def generate_initial_question(models, skills, job_position):
    """ä½¿ç”¨Qwenç”Ÿæˆç¬¬ä¸€ä¸ªé¢è¯•é—®é¢˜"""
    if not models['qwen_model'] or not models['qwen_tokenizer']:
        return "[é”™è¯¯] Qwenæ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆå¼€åœºé—®é¢˜ã€‚è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ã€‚"
    
    try:
        # æ ¹æ®ç®€å†æŠ€èƒ½å’ŒèŒä½ç”Ÿæˆå‹å¥½çš„å¼€åœºç™½
        skills_str = 'ã€'.join(skills[:3]) if skills else 'æŠ€æœ¯'
        
        system_msg = f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šã€å‹å¥½çš„æŠ€æœ¯é¢è¯•å®˜ï¼Œæ­£åœ¨é¢è¯•åº”è˜{job_position}çš„å€™é€‰äººã€‚"
        
        prompt = f"""å€™é€‰äººç®€å†æŠ€èƒ½ï¼š{skills_str}
åº”è˜èŒä½ï¼š{job_position}

ä»»åŠ¡ï¼šç”Ÿæˆé¢è¯•å¼€åœºé—®å€™ã€‚è¦æ±‚ï¼š
1. ä»¥"æ‚¨å¥½"æˆ–"æ¬¢è¿"å¼€å¤´
2. ç®€çŸ­ç¤¼è²Œåœ°æ¬¢è¿å€™é€‰äºº
3. è¯·å€™é€‰äººåšè‡ªæˆ‘ä»‹ç»æˆ–ä»‹ç»é¡¹ç›®ç»å†
4. 30å­—ä»¥å†…ï¼Œè¯­æ°”äº²åˆ‡è‡ªç„¶

ç¤ºä¾‹æ ¼å¼ï¼š
- "æ‚¨å¥½ï¼Œæ¬¢è¿å‚åŠ é¢è¯•ï¼å…ˆè¯·æ‚¨ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±å’Œæœ€è¿‘çš„é¡¹ç›®ç»å†å§ã€‚"
- "æ‚¨å¥½ï¼å¾ˆé«˜å…´è§åˆ°æ‚¨ã€‚èƒ½å…ˆè°ˆè°ˆæ‚¨æœ€è¿‘åœ¨{skills_str}æ–¹é¢çš„é¡¹ç›®ç»éªŒå—ï¼Ÿ"

è¯·ç”Ÿæˆç±»ä¼¼é£æ ¼çš„å¼€åœºé—®å€™ï¼š"""
        
        messages = [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": prompt}
        ]
        
        text = models['qwen_tokenizer'].apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = models['qwen_tokenizer'](
            text,
            return_tensors='pt',
            truncation=True,
            max_length=512
        ).to(models['device'])
        
        with torch.no_grad():
            outputs = models['qwen_model'].generate(
                **inputs,
                max_new_tokens=80,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.15,
                do_sample=True,
                pad_token_id=models['qwen_tokenizer'].eos_token_id
            )
        
        generated = models['qwen_tokenizer'].decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        ).strip()
        
        if not generated:
            return "[é”™è¯¯] Qwenç”Ÿæˆå¤±è´¥ï¼Œè¯·é‡è¯•ã€‚"
        return generated
    
    except Exception as e:
        print(f"[ERROR] Initial question generation failed: {e}")
        return f"[é”™è¯¯] Qwenç”Ÿæˆå¼‚å¸¸: {str(e)}"

def generate_qwen_question(models, context, question_type='follow_up'):
    """ä½¿ç”¨Qwen LoRAç”Ÿæˆé—®é¢˜"""
    if not models['qwen_model'] or not models['qwen_tokenizer']:
        return "[é”™è¯¯] Qwenæ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆé—®é¢˜ã€‚"
    
    try:
        # æ„å»ºpromptï¼ˆä¸¥æ ¼æŒ‰ç…§è®­ç»ƒæ•°æ®æ ¼å¼ï¼‰
        if question_type == 'follow_up':
            # è¿½é—®ï¼šå’Œè®­ç»ƒæ•°æ®æ ¼å¼å®Œå…¨ä¸€è‡´
            answer = context['last_answer']
            question = context['question']
            topic = context.get('topic', 'æŠ€æœ¯')
            score = context.get('score', 70)  # è·å–å½“å‰å›ç­”è´¨é‡
            
            # æ ¹æ®å›ç­”è´¨é‡å†³å®šä»»åŠ¡æè¿°
            if score >= 80:
                # é¼“åŠ±+è¿½é—®
                task = "å¯¹å€™é€‰äººçš„å›ç­”ç»™äºˆè‚¯å®šå’Œé¼“åŠ±ï¼Œç„¶åç»§ç»­è¿½é—®"
            else:
                # ç›´æ¥è¿½é—®
                task = f"æ ¹æ®å€™é€‰äººçš„å›ç­”ï¼Œç”Ÿæˆä¸€ä¸ªè¿½é—®é—®é¢˜ï¼Œæ·±å…¥è€ƒå¯Ÿå€™é€‰äººå¯¹{topic}çš„ç†è§£"
            
            prompt = f"""é¢è¯•å®˜é—®é¢˜ï¼š{question}
å€™é€‰äººå›ç­”ï¼š{answer}

ä»»åŠ¡ï¼š{task}"""
            
            system_msg = f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šã€å‹å¥½çš„{topic}é¢è¯•å®˜ï¼Œæ­£åœ¨é¢è¯•å€™é€‰äººã€‚ä½ éœ€è¦æ ¹æ®å€™é€‰äººçš„å›ç­”ï¼Œå†³å®šæ˜¯ç»§ç»­æ·±å…¥è¿½é—®ã€æ¢è¯é¢˜ï¼Œè¿˜æ˜¯ç»™äºˆé¼“åŠ±ã€‚"
        
        else:
            # æ¢è¯é¢˜ï¼ˆtopic_changeï¼‰
            answer = context.get('last_answer', '')
            question = context.get('question', '')
            topic = context.get('topic', 'æŠ€æœ¯')
            score = context.get('score', 70)
            skills = context.get('skills', ['æŠ€æœ¯'])
            history = context.get('history', [])
            
            # æ‰¾å‡ºè¿˜æ²¡é—®è¿‡çš„æŠ€èƒ½
            asked_topics = set()
            for qa in history[-5:]:  # åªçœ‹æœ€è¿‘5è½®
                q = qa.get('question', '')
                for skill in skills:
                    if skill in q:
                        asked_topics.add(skill)
            
            remaining_skills = [s for s in skills if s not in asked_topics]
            next_skill = remaining_skills[0] if remaining_skills else skills[0]
            
            # æ ¹æ®å›ç­”è´¨é‡å†³å®šä»»åŠ¡æè¿°
            if score >= 80:
                # å›ç­”å¾—å¾ˆå¥½ï¼Œæ¢è¯é¢˜
                task = f"å€™é€‰äººå¯¹{topic}å›ç­”å¾—å¾ˆå¥½ï¼Œå·²ç»å……åˆ†è€ƒå¯Ÿï¼Œå¯ä»¥æ¢ä¸€ä¸ªæ–°è¯é¢˜"
            else:
                # å›ç­”ä¸å¥½ï¼Œæ¢è¯é¢˜
                task = f"å€™é€‰äººå¯¹{topic}ä¸äº†è§£æˆ–ç­”ä¸ä¸Šæ¥ï¼Œéœ€è¦å‹å¥½åœ°æ¢ä¸€ä¸ªè¯é¢˜"
            
            prompt = f"""é¢è¯•å®˜é—®é¢˜ï¼š{question}
å€™é€‰äººå›ç­”ï¼š{answer}

ä»»åŠ¡ï¼š{task}"""
            
            system_msg = f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šã€å‹å¥½çš„æŠ€æœ¯é¢è¯•å®˜ï¼Œæ­£åœ¨é¢è¯•å€™é€‰äººã€‚ä½ éœ€è¦æ ¹æ®å€™é€‰äººçš„å›ç­”ï¼Œå†³å®šæ˜¯ç»§ç»­æ·±å…¥è¿½é—®ã€æ¢è¯é¢˜ï¼Œè¿˜æ˜¯ç»™äºˆé¼“åŠ±ã€‚"
        
        messages = [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": prompt}
        ]
        
        text = models['qwen_tokenizer'].apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = models['qwen_tokenizer'](
            [text],
            return_tensors="pt",
            padding=True
        ).to(models['device'])
        
        with torch.no_grad():
            outputs = models['qwen_model'].generate(
                **inputs,
                max_new_tokens=60,
                temperature=0.5,  # é™ä½æ¸©åº¦ï¼Œæ›´èšç„¦
                top_p=0.85,       # å‡å°‘éšæœºæ€§
                do_sample=True,
                repetition_penalty=1.1,  # é¿å…é‡å¤
                pad_token_id=models['qwen_tokenizer'].pad_token_id
            )
        
        response = models['qwen_tokenizer'].decode(
            outputs[0][len(inputs.input_ids[0]):],
            skip_special_tokens=True
        ).strip()
        
        # æ¸…ç†è¾“å‡º
        response = response.split('\n')[0][:100]
        
        if not response:
            return "[é”™è¯¯] Qwenç”Ÿæˆä¸ºç©ºï¼Œè¯·é‡è¯•ã€‚"
        return response
        
    except Exception as e:
        print(f"[ERROR] Qwen generation failed: {str(e)}")
        return f"[é”™è¯¯] Qwenç”Ÿæˆå¼‚å¸¸: {str(e)}"

def evaluate_answer(models, question, answer, history_qa):
    """RoBERTaè¯„ä¼°å›ç­”"""
    input_parts = []
    if history_qa:
        input_parts.append("[å†å²é—®ç­”]")
        for i, h in enumerate(history_qa[-3:], 1):
            input_parts.append(f"Q{i}: {h['question']}")
            input_parts.append(f"A{i}: {h['answer'][:100]}")
            input_parts.append(f"è´¨é‡: {h['quality']}")
    
    input_parts.append("[å½“å‰é—®ç­”]")
    input_parts.append(f"é—®é¢˜: {question}")
    input_parts.append(f"å›ç­”: {answer}")
    input_parts.append(f"æµç•…åº¦: 0.85")
    
    input_text = "\n".join(input_parts)
    
    inputs = models['roberta_tokenizer'](
        input_text,
        return_tensors='pt',
        truncation=True,
        max_length=256,
        padding=True
    ).to(models['device'])
    
    with torch.no_grad():
        cls_logits, reg_score = models['roberta_model'](
            inputs['input_ids'],
            inputs['attention_mask']
        )
        
        cls_probs = torch.softmax(cls_logits, dim=-1)
        predicted_idx = cls_probs.argmax().item()
        confidence = cls_probs.max().item()
        overall_score = reg_score.item() * 100
    
    label_names = ["å·®", "ä¸€èˆ¬", "è‰¯å¥½", "ä¼˜ç§€"]
    score_mapping = [50, 70, 85, 95]
    
    return {
        'current_label': label_names[predicted_idx],
        'current_score': score_mapping[predicted_idx],
        'overall_score': overall_score,
        'confidence': confidence
    }

def decide_next_action(models, question, answer, follow_up_depth, topic):
    """BERTå†³ç­–ä¸‹ä¸€æ­¥ï¼ˆä¸¥æ ¼æŒ‰ç…§è®­ç»ƒæ ¼å¼ï¼‰"""
    answer_length = len(answer)
    
    # è®¡ç®—çŠ¹è±«åº¦ï¼ˆåŸºäºè¯­æ°”è¯å’Œåœé¡¿ï¼‰
    hesitation_words = ['å—¯', 'å•Š', 'è¿™ä¸ª', 'é‚£ä¸ª', 'å°±æ˜¯', 'æ€ä¹ˆè¯´', '...']
    hesitation_count = sum(answer.count(word) for word in hesitation_words)
    hesitation_score = min(0.9, hesitation_count * 0.15)
    
    # å’Œè®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´çš„æ ¼å¼
    features = f"è¿½é—®æ·±åº¦:{follow_up_depth} " \
              f"çŠ¹è±«åº¦:{hesitation_score:.2f} " \
              f"é•¿åº¦:{answer_length}å­— " \
              f"è¯é¢˜:{topic}"
    
    bert_input = f"{question}[SEP]{answer}[SEP]{features}"
    
    inputs = models['bert_tokenizer'](
        bert_input,
        return_tensors='pt',
        truncation=True,
        max_length=512,
        padding=True
    ).to(models['device'])
    
    with torch.no_grad():
        outputs = models['bert_model'](**inputs)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=-1)
        predicted = probs.argmax().item()
        conf = probs.max().item()
    
    bert_labels = ["FOLLOW_UP", "NEXT_TOPIC"]
    
    # æ¨æ–­å†³ç­–ç†ç”±ï¼ˆåŸºäºè®­ç»ƒæ•°æ®ä¸­çš„reasonæ¨¡å¼ï¼‰
    reason = _infer_decision_reason(answer, answer_length, hesitation_score, 
                                     follow_up_depth, bert_labels[predicted], topic)
    
    return {
        'action': bert_labels[predicted],
        'confidence': conf,
        'probs': probs[0].tolist(),
        'reason': reason,
        'hesitation_score': hesitation_score
    }

def _infer_decision_reason(answer, answer_length, hesitation_score, follow_up_depth, action, topic):
    """æ¨æ–­å†³ç­–ç†ç”±ï¼ˆåŸºäºè®­ç»ƒæ•°æ®ä¸­çš„reasonæ¨¡å¼ï¼‰"""
    
    if action == "NEXT_TOPIC":
        # NEXT_TOPICçš„å¸¸è§åŸå› 
        if any(word in answer for word in ['ä¸äº†è§£', 'ä¸æ‡‚', 'æ²¡å­¦è¿‡', 'æ²¡æ¥è§¦']):
            return f"å€™é€‰è€…æ˜ç¡®è¡¨ç¤ºä¸äº†è§£{topic}ï¼Œåº”æ¢å…¶ä»–è¯é¢˜"
        elif follow_up_depth >= 2 and hesitation_score > 0.4:
            return f"ç»è¿‡{follow_up_depth}è½®è¿½é—®ï¼Œå€™é€‰è€…å¯¹{topic}çš„ç†è§£ä»ç„¶æ¨¡ç³Š/ä¸è¶³ï¼Œå»ºè®®æ¢è¯é¢˜"
        elif answer_length < 20:
            return f"å€™é€‰è€…å›ç­”è¿‡äºç®€çŸ­ä¸”ç¼ºä¹å®è´¨å†…å®¹ï¼Œå»ºè®®æ¢å…¶ä»–è¯é¢˜"
        else:
            return "æ ¹æ®ç»¼åˆåˆ†æï¼Œå»ºè®®æ¢è¯é¢˜è€ƒå¯Ÿå…¶ä»–æŠ€èƒ½ç‚¹"
    
    else:  # FOLLOW_UP
        # FOLLOW_UPçš„å¸¸è§åŸå› 
        if any(word in answer for word in ['ç”¨è¿‡', 'åšè¿‡', 'é¡¹ç›®']) and answer_length < 80:
            return f"å€™é€‰è€…æåˆ°äº†ä½¿ç”¨åœºæ™¯ä½†ç¼ºå°‘ç»†èŠ‚ï¼Œå¯ä»¥ç»§ç»­è¿½é—®æ›´æ·±å…¥çš„æŠ€æœ¯ç»†èŠ‚"
        elif hesitation_score > 0.3 and any(word in answer for word in ['æ¥è§¦è¿‡', 'äº†è§£', 'ç”¨åˆ°']):
            return f"å€™é€‰è€…æ‰¿è®¤ä½¿ç”¨è¿‡{topic}ä½†æœªå±•å¼€ç»†èŠ‚ï¼ˆæåˆ°äº†'æ‰¿è®¤ç”¨è¿‡ä½†è¯´ä¸æ¸…ç»†èŠ‚'ï¼‰ï¼Œéœ€è¿½é—®å…·ä½“å®ç°"
        elif answer_length >= 50:
            return "å€™é€‰è€…ç»™å‡ºäº†ä¸€å®šå†…å®¹ï¼Œå¯ä»¥é’ˆå¯¹å›ç­”ä¸­çš„å…³é”®ç‚¹ç»§ç»­æ·±å…¥è¿½é—®"
        else:
            return "å¯ä»¥ç»§ç»­è¿½é—®ä»¥æ›´å…¨é¢è¯„ä¼°å€™é€‰è€…çš„æŠ€æœ¯èƒ½åŠ›"

# ==================== ä¸»ç•Œé¢ ====================
st.title("ğŸ¯ AIé¢è¯•æ•™ç»ƒç³»ç»Ÿ")
st.markdown("### åŸºäºå¾®è°ƒTransformeræ¨¡å‹çš„æ™ºèƒ½é¢è¯•")

# åˆå§‹åŒ–
if 'stage' not in st.session_state:
    st.session_state.stage = 'upload'
if 'resume_data' not in st.session_state:
    st.session_state.resume_data = None
if 'current_question' not in st.session_state:
    st.session_state.current_question = None
if 'qa_history' not in st.session_state:
    st.session_state.qa_history = []
if 'follow_up_depth' not in st.session_state:
    st.session_state.follow_up_depth = 0
if 'current_topic' not in st.session_state:
    st.session_state.current_topic = 'Python'
if 'total_rounds' not in st.session_state:
    st.session_state.total_rounds = 0
if 'job_position' not in st.session_state:
    st.session_state.job_position = 'Pythonåç«¯å·¥ç¨‹å¸ˆ'
if 'digital_human' not in st.session_state:
    st.session_state.digital_human = DigitalHuman()
if 'linly_client' not in st.session_state:
    st.session_state.linly_client = LinlyTalkerClient()
if 'digital_human_mode' not in st.session_state:
    # é»˜è®¤ä½¿ç”¨è½»é‡çº§æ¨¡å¼ï¼Œå¦‚æœLinlyæœåŠ¡å¯ç”¨åˆ™å¯ä»¥åˆ‡æ¢
    st.session_state.digital_human_mode = 'lightweight'
if 'avatar_image_path' not in st.session_state:
    st.session_state.avatar_image_path = "Linly-Talker/examples/source_image/full_body_1.png"

# ==================== é˜¶æ®µ1: ä¸Šä¼ ç®€å† ====================
if st.session_state.stage == 'upload':
    st.markdown("---")
    st.subheader("ğŸ“„ æ­¥éª¤1ï¼šä¸Šä¼ ç®€å†")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        upload_method = st.radio(
            "é€‰æ‹©ä¸Šä¼ æ–¹å¼",
            ["ä¸Šä¼ æ–‡ä»¶ (PDF/DOCX)", "ç›´æ¥ç²˜è´´æ–‡æœ¬"],
            horizontal=True
        )
        
        resume_text = ""
        
        if upload_method == "ä¸Šä¼ æ–‡ä»¶ (PDF/DOCX)":
            uploaded_file = st.file_uploader(
                "ä¸Šä¼ ç®€å†æ–‡ä»¶",
                type=['pdf', 'docx'],
                help="æ”¯æŒPDFå’ŒDOCXæ ¼å¼"
            )
            
            if uploaded_file:
                with tempfile.NamedTemporaryFile(delete=False, suffix=Path(uploaded_file.name).suffix) as tmp_file:
                    tmp_file.write(uploaded_file.getbuffer())
                    tmp_path = tmp_file.name
                
                try:
                    with st.spinner("æ­£åœ¨è§£æç®€å†..."):
                        parser = ResumeParser()
                        resume_data = parser.parse(tmp_path)
                        st.session_state.resume_data = resume_data
                    
                    st.success(f"âœ… ç®€å†è§£ææˆåŠŸï¼")
                    
                    with st.expander("ğŸ“‹ æŸ¥çœ‹è§£æç»“æœ"):
                        st.markdown(f"**å§“åï¼š** {resume_data['name']}")
                        if resume_data['contact']:
                            st.markdown(f"**è”ç³»æ–¹å¼ï¼š** {resume_data['contact']}")
                        st.markdown(f"**æå–åˆ°çš„æŠ€èƒ½ï¼š** {', '.join(resume_data['skills'][:10])}")
                    
                    resume_text = resume_data['raw_text']
                    
                except Exception as e:
                    st.error(f"ç®€å†è§£æå¤±è´¥ï¼š{str(e)}")
                finally:
                    Path(tmp_path).unlink(missing_ok=True)
        else:
            resume_text = st.text_area(
                "ç²˜è´´ç®€å†å†…å®¹",
                height=300,
                placeholder="å§“åï¼šå¼ ä¸‰\nåº”è˜èŒä½ï¼šPythonåç«¯å·¥ç¨‹å¸ˆ\n\næŠ€èƒ½ï¼šPython, Django, MySQL..."
            )
        
        job_position = st.text_input("åº”è˜èŒä½", value="Pythonåç«¯å·¥ç¨‹å¸ˆ")
    
    with col2:
        st.info("ğŸ’¡ **æ™ºèƒ½ç‰¹æ€§**\n\nâœ… PDF/DOCXè§£æ\n\nâœ… BERTæ™ºèƒ½å†³ç­–\n\nâœ… **Qwen LoRAè¿½é—®**\n\nâœ… RoBERTaè¯„ä¼°\n\nâœ… **æ— å›ºå®šè½®æ•°**")
        
        if st.session_state.resume_data:
            st.success(f"**å·²è¯†åˆ«æŠ€èƒ½ï¼š**\n\n" + "\n".join([f"â€¢ {s}" for s in st.session_state.resume_data['skills'][:5]]))
    
    if st.button("ğŸš€ å¼€å§‹é¢è¯•", type="primary", use_container_width=True):
        if resume_text.strip() or st.session_state.resume_data:
            if not st.session_state.resume_data and resume_text:
                parser = ResumeParser()
                st.session_state.resume_data = {
                    'name': 'å€™é€‰äºº',
                    'skills': [s for s in ['Python', 'Java', 'JavaScript'] if s.lower() in resume_text.lower()],
                    'raw_text': resume_text
                }
            
            # æ ‡è®°ä¸ºè¿›å…¥é¢è¯•é˜¶æ®µï¼Œç¬¬ä¸€ä¸ªé—®é¢˜å°†åœ¨é¢è¯•é˜¶æ®µåŠ¨æ€ç”Ÿæˆ
            skills = st.session_state.resume_data['skills'] or ['æŠ€æœ¯']
            first_skill = skills[0] if skills else 'æŠ€æœ¯'
            st.session_state.current_topic = first_skill
            st.session_state.current_question = None  # æ ‡è®°ä¸ºéœ€è¦ç”Ÿæˆ
            st.session_state.job_position = job_position
            st.session_state.stage = 'interview'
            st.rerun()
        else:
            st.error("è¯·å…ˆä¸Šä¼ ç®€å†æˆ–ç²˜è´´æ–‡æœ¬")

# ==================== é˜¶æ®µ2: é¢è¯•è¿‡ç¨‹ ====================
elif st.session_state.stage == 'interview':
    with st.spinner("æ­£åœ¨åŠ è½½AIæ¨¡å‹..."):
        models = load_models()
    
    # åœ¨ä¾§è¾¹æ æ˜¾ç¤ºæ¨¡å‹åŠ è½½çŠ¶æ€ï¼ˆç¼“å­˜å¤–ï¼‰
    with st.sidebar:
        st.markdown("### ğŸ” æ¨¡å‹çŠ¶æ€")
        if models['qwen_model'] is not None:
            st.success("âœ… Qwen LoRAå·²åŠ è½½")
        else:
            st.error("âŒ QwenæœªåŠ è½½ - æ— æ³•ç»§ç»­é¢è¯•")
        st.success("âœ… BERTå†³ç­–æ¨¡å‹")
        st.success("âœ… RoBERTaè¯„ä¼°æ¨¡å‹")
        st.markdown("---")
        
        # æ•°å­—äººæ¨¡å¼é€‰æ‹©
        st.markdown("### ğŸ­ æ•°å­—äººæ¨¡å¼")
        
        # æ£€æŸ¥ Linly-Talker æœåŠ¡çŠ¶æ€
        linly_status = "âœ… å¯ç”¨" if st.session_state.linly_client.available else "âŒ æœªå¯åŠ¨"
        st.caption(f"Linly-Talker: {linly_status}")
        
        if st.session_state.linly_client.available:
            mode_options = {
                "lightweight": "ğŸ’¨ è½»é‡çº§ (Edge-TTS + é™æ€å¤´åƒ)",
                "linly": "ğŸ¬ é«˜è´¨é‡ (Linly-Talker + å”‡åŒæ­¥è§†é¢‘)"
            }
        else:
            mode_options = {
                "lightweight": "ğŸ’¨ è½»é‡çº§ (Edge-TTS + é™æ€å¤´åƒ)"
            }
        
        selected_mode = st.radio(
            "é€‰æ‹©æ•°å­—äººæ¨¡å¼",
            list(mode_options.keys()),
            format_func=lambda x: mode_options[x],
            index=0,
            key="dh_mode_selector"
        )
        
        if selected_mode != st.session_state.digital_human_mode:
            st.session_state.digital_human_mode = selected_mode
        
        if not st.session_state.linly_client.available:
            with st.expander("ğŸ”§ å¯åŠ¨ Linly-Talker"):
                st.code("python start_linly_services.py", language="bash")
                st.caption("åœ¨æ–°ç»ˆç«¯è¿è¡Œä¸Šè¿°å‘½ä»¤å¯åŠ¨ Linly-Talker æœåŠ¡")
        
        st.markdown("---")
    
    # ç”Ÿæˆç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
    if st.session_state.current_question is None:
        with st.spinner("Qwenæ­£åœ¨ç”Ÿæˆå¼€åœºé—®é¢˜..."):
            skills = st.session_state.resume_data.get('skills', ['æŠ€æœ¯'])
            job_position = st.session_state.get('job_position', 'Pythonåç«¯å·¥ç¨‹å¸ˆ')
            st.session_state.current_question = generate_initial_question(models, skills, job_position)
    
    # é¡¶éƒ¨ä¿¡æ¯
    st.markdown(f"<div style='text-align: center;'><h3>ğŸ¯ AIé¢è¯•æ•™ç»ƒ</h3></div>", unsafe_allow_html=True)
    if st.session_state.resume_data:
        st.caption(f"ğŸ‘¤ {st.session_state.resume_data.get('name', 'å€™é€‰äºº')} | åº”è˜ï¼š{st.session_state.job_position} | å·²å®Œæˆï¼š{st.session_state.total_rounds}è½®")
    
    # ä¸»å¸ƒå±€ï¼šå·¦ä¾§(æ•°å­—äºº+å¯¹è¯) å³ä¾§(è¯„åˆ†)
    col_main, col_score = st.columns([2.5, 1])
    
    with col_main:
        # ========== æ•°å­—äººé¢è¯•å®˜ ==========
        if st.session_state.digital_human_mode == 'linly':
            # ä½¿ç”¨ Linly-Talker ç”Ÿæˆè§†é¢‘
            st.markdown('<div style="text-align: center; margin-bottom: 1rem;">', unsafe_allow_html=True)
            
            # ç”Ÿæˆè§†é¢‘ï¼ˆå¸¦ç¼“å­˜keyé¿å…é‡å¤ç”Ÿæˆï¼‰
            video_cache_key = f"video_{hash(st.session_state.current_question)}"
            if video_cache_key not in st.session_state:
                with st.spinner("ğŸ¬ æ•°å­—äººæ­£åœ¨ç”Ÿæˆè§†é¢‘..."):
                    try:
                        video_path = st.session_state.linly_client.generate_video(
                            text=st.session_state.current_question,
                            avatar_image=st.session_state.avatar_image_path
                        )
                        st.session_state[video_cache_key] = video_path
                    except Exception as e:
                        st.error(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")
                        st.info("å·²è‡ªåŠ¨åˆ‡æ¢åˆ°è½»é‡çº§æ¨¡å¼")
                        st.session_state.digital_human_mode = 'lightweight'
                        video_path = None
            else:
                video_path = st.session_state[video_cache_key]
            
            if video_path and Path(video_path).exists():
                st.markdown(f"""
                <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                            border-radius: 20px; padding: 1.5rem; margin-bottom: 1rem;">
                    <div style="color: white; font-size: 1.2rem; font-weight: 600; margin-bottom: 1rem; text-align: center;">
                        ğŸ¯ AIé¢è¯•å®˜ Â· {st.session_state.current_topic}
                    </div>
                </div>
                """, unsafe_allow_html=True)
                st.video(video_path, autoplay=True)
            else:
                # é™çº§åˆ°è½»é‡çº§æ¨¡å¼
                avatar_html = st.session_state.digital_human.get_avatar_html(
                    question=st.session_state.current_question,
                    topic=st.session_state.current_topic,
                    include_audio=True
                )
            st.markdown('</div>', unsafe_allow_html=True)
        else:
            # è½»é‡çº§æ¨¡å¼ï¼šä½¿ç”¨ Streamlit åŸç”Ÿç»„ä»¶
            # æ•°å­—äººå¤´åƒå’Œä¿¡æ¯
            col_avatar, col_info = st.columns([1, 2])
            
            with col_avatar:
                st.image("https://i.pravatar.cc/300?img=33", width=150)
            
            with col_info:
                st.markdown(f"### ğŸ¯ AIé¢è¯•å®˜ - Alex Chen")
                st.markdown(f"**è¯é¢˜ï¼š** {st.session_state.current_topic}")
                st.info("ğŸ’¬ æ­£åœ¨è¯´è¯ä¸­...")
            
            # æ˜¾ç¤ºé—®é¢˜
            st.markdown("---")
            st.success(f"**é¢è¯•å®˜æé—®ï¼š**\n\n{st.session_state.current_question}")
            
            # ç”Ÿæˆå¹¶æ’­æ”¾è¯­éŸ³
            try:
                audio_file = st.session_state.digital_human.text_to_speech(st.session_state.current_question)
                if audio_file and Path(audio_file).exists():
                    with open(audio_file, 'rb') as f:
                        audio_bytes = f.read()
                    st.audio(audio_bytes, format='audio/mp3', autoplay=True)
            except Exception as e:
                st.caption(f"è¯­éŸ³ç”Ÿæˆå¤±è´¥: {e}")
        
        # ========== å¯¹è¯å†å² ==========
        st.markdown("### ğŸ’¬ å¯¹è¯è®°å½•")
        
        # èŠå¤©å¼å†å²è®°å½•ï¼ˆåªæ˜¾ç¤ºæœ€è¿‘5è½®ï¼‰
        with st.container():
            st.markdown('<div style="max-height: 350px; overflow-y: auto; padding: 1rem; background: #f8f9fa; border-radius: 10px; margin-bottom: 1rem;">', unsafe_allow_html=True)
            
            if not st.session_state.qa_history:
                st.markdown('<div style="text-align: center; color: #999; padding: 2rem;">æš‚æ— å¯¹è¯è®°å½•</div>', unsafe_allow_html=True)
            else:
                for i, qa in enumerate(st.session_state.qa_history[-5:]):
                    quality_emoji = {'å·®': 'ğŸ”´', 'ä¸€èˆ¬': 'ğŸŸ¡', 'è‰¯å¥½': 'ğŸ”µ', 'ä¼˜ç§€': 'ğŸŸ¢'}.get(qa.get('quality', 'ä¸€èˆ¬'), 'âšª')
                    
                    # AIæ¶ˆæ¯
                    st.markdown(f'''
                    <div class="chat-message ai">
                        <div class="message-bubble">
                            <div style="font-size: 0.85rem; opacity: 0.9;">ğŸ¤– é¢è¯•å®˜</div>
                            <div style="margin-top: 0.3rem;">{qa["question"]}</div>
                        </div>
                    </div>
                    ''', unsafe_allow_html=True)
                    
                    # ç”¨æˆ·æ¶ˆæ¯
                    st.markdown(f'''
                    <div class="chat-message user">
                        <div class="message-bubble">
                            <div style="font-size: 0.85rem; opacity: 0.7;">ğŸ‘¤ å€™é€‰äºº</div>
                            <div style="margin-top: 0.3rem;">{qa["answer"]}</div>
                            <div style="font-size: 0.75rem; margin-top: 0.5rem; opacity: 0.8;">{quality_emoji} {qa.get('quality', 'ä¸€èˆ¬')}</div>
                        </div>
                    </div>
                    ''', unsafe_allow_html=True)
            
            st.markdown('</div>', unsafe_allow_html=True)
        
        # ========== å›ç­”è¾“å…¥ ==========
        st.markdown("### âœï¸ æ‚¨çš„å›ç­”")
        answer = st.text_area(
            "è¯·è¾“å…¥æ‚¨çš„å›ç­”",
            height=120,
            key=f"answer_{st.session_state.total_rounds}",
            placeholder="åœ¨æ­¤è¾“å…¥æ‚¨çš„å›ç­”..."
        )
        
        col_s, col_e = st.columns([3, 1])
        
        with col_s:
                if st.button("âœ… æäº¤å›ç­”", type="primary", use_container_width=True):
                    if answer.strip():
                        # è¯„ä¼°
                        with st.spinner("æ­£åœ¨è¯„ä¼°å›ç­”..."):
                            eval_result = evaluate_answer(models, st.session_state.current_question, answer, st.session_state.qa_history)
                        
                        # æ˜¾ç¤ºè¯„ä¼°è¯¦æƒ…
                        st.sidebar.markdown("### ğŸ“Š æœ¬è½®è¯„ä¼°")
                        st.sidebar.write(f"è´¨é‡: {eval_result['current_label']}")
                        st.sidebar.write(f"å½“å‰åˆ†: {eval_result['current_score']}")
                        st.sidebar.write(f"æ•´ä½“åˆ†: {eval_result['overall_score']:.1f}")
                        st.sidebar.write(f"ç½®ä¿¡åº¦: {eval_result['confidence']:.1%}")
                        
                        # è®°å½•
                        st.session_state.qa_history.append({
                            'question': st.session_state.current_question,
                            'answer': answer,
                            'quality': eval_result['current_label'],
                            'eval': eval_result
                        })
                        
                        st.session_state.total_rounds += 1
                        
                        # BERTå†³ç­–
                        with st.spinner("BERTæ­£åœ¨å†³ç­–..."):
                            decision = decide_next_action(
                                models,
                                st.session_state.current_question,
                                answer,
                                st.session_state.follow_up_depth,
                                st.session_state.current_topic
                            )
                        
                        # æ˜¾ç¤ºå†³ç­–è¯¦æƒ…
                        st.sidebar.markdown("### ğŸ§  BERTå†³ç­–")
                        st.sidebar.write(f"**åŠ¨ä½œ**: {decision['action']}")
                        st.sidebar.write(f"**ç½®ä¿¡åº¦**: {decision['confidence']:.1%}")
                        st.sidebar.write(f"**ç†ç”±**: {decision['reason']}")
                        st.sidebar.caption(f"çŠ¹è±«åº¦: {decision['hesitation_score']:.2f} | FOLLOW_UPæ¦‚ç‡: {decision['probs'][0]:.1%}")
                        
                        # ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜
                        if decision['action'] == 'FOLLOW_UP':
                            st.session_state.follow_up_depth += 1
                            # ä½¿ç”¨Qwenç”Ÿæˆè¿½é—®
                            st.sidebar.markdown("### ğŸ¤– Qwenç”Ÿæˆè¿½é—®")
                            with st.spinner("Qwenæ­£åœ¨ç”Ÿæˆè¿½é—®..."):
                                context = {
                                    'last_answer': answer,
                                    'question': st.session_state.current_question,
                                    'topic': st.session_state.current_topic,
                                    'score': eval_result['current_score']
                                }
                                new_q = generate_qwen_question(models, context, 'follow_up')
                                st.session_state.current_question = new_q
                                st.sidebar.write(f"**ç±»å‹**: è¿½é—®")
                                st.sidebar.write(f"**é—®é¢˜**: {new_q}")
                                if '[é”™è¯¯]' not in new_q:
                                    with st.sidebar.expander("æŸ¥çœ‹æç¤ºè¯"):
                                        st.code(f"é—®é¢˜: {context['question']}\nå›ç­”: {context['last_answer'][:50]}...", language="text")
                        else:
                            st.session_state.follow_up_depth = 0
                            # ä½¿ç”¨Qwenç”Ÿæˆæ–°è¯é¢˜é—®é¢˜
                            st.sidebar.markdown("### ğŸ¤– Qwenç”Ÿæˆæ–°é¢˜")
                            with st.spinner("Qwenæ­£åœ¨ç”Ÿæˆæ–°é—®é¢˜..."):
                                context = {
                                    'last_answer': answer,
                                    'question': st.session_state.current_question,
                                    'topic': st.session_state.current_topic,
                                    'score': eval_result['current_score'],
                                    'skills': st.session_state.resume_data.get('skills', []),
                                    'history': st.session_state.qa_history
                                }
                                new_q = generate_qwen_question(models, context, 'new_topic')
                                st.session_state.current_question = new_q
                                
                                # æ›´æ–°å½“å‰è¯é¢˜
                                skills = context.get('skills', [])
                                for skill in skills:
                                    if skill in new_q:
                                        st.session_state.current_topic = skill
                                        break
                                
                                st.sidebar.write(f"**ç±»å‹**: æ–°è¯é¢˜")
                                st.sidebar.write(f"**é—®é¢˜**: {new_q}")
                                if '[é”™è¯¯]' not in new_q:
                                    with st.sidebar.expander("æŸ¥çœ‹æç¤ºè¯"):
                                        st.code(f"æŠ€èƒ½: {', '.join(context.get('skills', []))}\nå·²é—®è½®æ•°: {len(context.get('history', []))}", language="text")
                        
                        st.rerun()
                    else:
                        st.error("è¯·è¾“å…¥å›ç­”")
            
        with col_e:
            if st.button("ğŸ ç»“æŸé¢è¯•", use_container_width=True):
                if st.session_state.qa_history:
                    st.session_state.stage = 'summary'
                    st.rerun()
    
    with col_score:
        # ========== è¯„åˆ†é¢æ¿ ==========
        st.markdown("### ğŸ“Š å®æ—¶è¯„åˆ†")
        
        if st.session_state.qa_history:
            latest = st.session_state.qa_history[-1]['eval']
            
            st.markdown(f"""
            <div class="score-card">
                <div style="color: #999; font-size: 0.9rem;">å½“å‰å›ç­”</div>
                <div style="font-size: 2rem; font-weight: bold; color: #667eea;">{latest['current_label']}</div>
                <div style="color: #999; font-size: 0.85rem;">{latest['current_score']}åˆ†</div>
            </div>
            
            <div class="score-card">
                <div style="color: #999; font-size: 0.9rem;">æ•´ä½“è¡¨ç°</div>
                <div style="font-size: 2rem; font-weight: bold; color: #667eea;">{latest['overall_score']:.1f}</div>
                <div style="color: #999; font-size: 0.85rem;">æ»¡åˆ†100</div>
            </div>
            
            <div class="score-card">
                <div style="color: #999; font-size: 0.9rem;">å·²å®Œæˆè½®æ•°</div>
                <div style="font-size: 2rem; font-weight: bold; color: #667eea;">{st.session_state.total_rounds}</div>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.info("å›ç­”é—®é¢˜åå°†æ˜¾ç¤ºè¯„åˆ†")
        
        st.markdown("---")
        
        # AIæ¨¡å‹çŠ¶æ€
        with st.expander("ğŸ” AIå†³ç­–è¯¦æƒ…", expanded=False):
            st.caption("æŸ¥çœ‹BERTã€RoBERTaè¯¦ç»†åˆ†æ")
            if st.session_state.qa_history:
                last_qa = st.session_state.qa_history[-1]
                st.json(last_qa.get('eval', {}))

# ==================== é˜¶æ®µ3: æ€»ç»“ ====================
elif st.session_state.stage == 'summary':
    st.markdown("---")
    st.subheader("ğŸ“ˆ é¢è¯•æ€»ç»“æŠ¥å‘Š")
    
    if st.session_state.qa_history:
        final_eval = st.session_state.qa_history[-1]['eval']
        final_score = final_eval['overall_score']
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("æ€»ä½“è¯„åˆ†", f"{final_score:.1f}/100")
        with col2:
            st.metric("æ€»è½®æ•°", st.session_state.total_rounds)
        with col3:
            avg_current = sum(qa['eval']['current_score'] for qa in st.session_state.qa_history) / len(st.session_state.qa_history)
            st.metric("å¹³å‡åˆ†", f"{avg_current:.0f}")
        with col4:
            excellent = sum(1 for qa in st.session_state.qa_history if qa['quality'] == 'ä¼˜ç§€')
            st.metric("ä¼˜ç§€", f"{excellent}ä¸ª")
        
        st.markdown("---")
        if final_score >= 85:
            st.success(f"### ğŸŒŸ å¼ºçƒˆæ¨è\nå€™é€‰äººè¡¨ç°ä¼˜ç§€ï¼ŒæŠ€æœ¯åŠŸåº•æ‰å®ã€‚")
        elif final_score >= 70:
            st.success(f"### ğŸ‘ æ¨è\nå€™é€‰äººå…·å¤‡ç›¸åº”æŠ€èƒ½ï¼Œè¡¨ç°è‰¯å¥½ã€‚")
        elif final_score >= 50:
            st.warning(f"### ğŸ¤” å¾…å®š\nå€™é€‰äººåŸºç¡€ä¸€èˆ¬ï¼Œéœ€è¿›ä¸€æ­¥è€ƒå¯Ÿã€‚")
        else:
            st.error(f"### âŒ ä¸æ¨è\nå€™é€‰äººæŠ€æœ¯èƒ½åŠ›ä¸è¶³ã€‚")
        
        st.markdown("---")
        st.markdown("### ğŸ“ å¯¹è¯è¯¦æƒ…")
        
        for i, qa in enumerate(st.session_state.qa_history, 1):
            with st.expander(f"ç¬¬{i}è½® - {qa['quality']} ({qa['eval']['current_score']}åˆ†)"):
                st.markdown(f"**Q:** {qa['question']}")
                st.markdown(f"**A:** {qa['answer']}")
                st.markdown(f"**è¯„åˆ†:** {qa['eval']['current_label']} ({qa['eval']['current_score']}åˆ†) | æ•´ä½“: {qa['eval']['overall_score']:.1f}/100")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ”„ é‡æ–°å¼€å§‹", type="primary", use_container_width=True):
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.rerun()
    
    with col2:
        if st.button("ğŸ“¥ å¯¼å‡ºæŠ¥å‘Š", use_container_width=True):
            report = {
                'time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'candidate': st.session_state.resume_data.get('name', 'æœªçŸ¥'),
                'final_score': final_score,
                'total_rounds': st.session_state.total_rounds,
                'qa_history': st.session_state.qa_history
            }
            
            st.download_button(
                "ğŸ’¾ ä¸‹è½½æŠ¥å‘Š",
                data=json.dumps(report, ensure_ascii=False, indent=2),
                file_name=f"interview_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json",
                use_container_width=True
            )

# ä¾§è¾¹æ 
with st.sidebar:
    st.markdown("### ğŸ“Š è®­ç»ƒæ•°æ®")
    st.caption("""
    - BERT: 1500æ¡
    - Qwen: 2000æ¡
    - RoBERTa: 2000æ¡
    """)





