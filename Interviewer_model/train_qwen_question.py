"""
è®­ç»ƒ Qwen-Question æ¨¡å‹ï¼ˆåŸæœ‰åŠŸèƒ½ï¼‰
åŠŸèƒ½: æ ¹æ®è¯é¢˜ã€å†å²å¯¹è¯å’ŒæŒ‡å¯¼å»ºè®®ï¼Œç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜å’Œé‡è¦ç¨‹åº¦

é’ˆå¯¹ RTX 4060 8GB æ˜¾å­˜ä¼˜åŒ–:
- 4bité‡åŒ–
- å°batch size (2)
- æ¢¯åº¦ç´¯ç§¯ (4)
"""

import json
import sys
import io
import torch
from pathlib import Path
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    EarlyStoppingCallback
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType
)
import matplotlib.pyplot as plt

# ä¿®å¤Windowsæ§åˆ¶å°ç¼–ç 
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# é…ç½®ï¼ˆV3ä¼˜åŒ–ç‰ˆï¼šé’ˆå¯¹è¿‡æ‹Ÿåˆä¼˜åŒ–ï¼‰
CONFIG = {
    # æ¨¡å‹é…ç½®
    "base_model": "Qwen/Qwen2-1.5B-Instruct",  # ä½¿ç”¨Qwen2ï¼ˆä¸test_qwen.pyç›¸åŒï¼‰
    "max_length": 1024,
    
    # LoRAé…ç½®ï¼ˆå¢åŠ æ­£åˆ™åŒ–ï¼‰
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.15,  # V3: ä»0.1å¢è‡³0.15ï¼Œå¢å¼ºæ­£åˆ™åŒ–
    "lora_target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", 
                            "gate_proj", "up_proj", "down_proj"],
    
    # è®­ç»ƒé…ç½®V3ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    "batch_size": 2,
    "gradient_accumulation": 4,
    "num_epochs": 3,  # V3: ä»5é™è‡³3ï¼Œé¿å…åæœŸè¿‡æ‹Ÿåˆ
    "learning_rate": 1.0e-4,  # V3: ä»1.5e-4é™è‡³1.0e-4ï¼Œæ›´ç¨³å®šè®­ç»ƒ
    "warmup_steps": 30,  # V3: å‡å°‘é¢„çƒ­æ­¥æ•°
    "logging_steps": 5,  # V3: æ›´é¢‘ç¹çš„æ—¥å¿—
    "save_steps": 50,  # V3: æ›´é¢‘ç¹ä¿å­˜ï¼Œä¾¿äºé€‰æ‹©æœ€ä½³æ¨¡å‹
    "eval_steps": 10,  # V3: æ¯10æ­¥éªŒè¯
    
    # æ—©åœé…ç½®ï¼ˆæ–°å¢ï¼‰
    "early_stopping_patience": 10,  # V3: 10æ¬¡éªŒè¯ä¸æ”¹å–„åˆ™åœæ­¢
    "early_stopping_threshold": 0.001,  # V3: æ”¹å–„é˜ˆå€¼
    
    # 4bité‡åŒ–é…ç½®
    "use_4bit": True,
    "bnb_4bit_compute_dtype": torch.float16,
    "bnb_4bit_quant_type": "nf4",
    
    # è·¯å¾„
    "train_data": "dual_qwen_data/qwen_question_train_split.json",
    "val_data": "dual_qwen_data/qwen_question_val_split.json",
    "output_dir": "checkpoints/qwen_question_lora",
}

def load_data(filepath):
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return Dataset.from_list(data)

def format_prompt(example):
    """
    æ ¼å¼åŒ–ä¸ºQwençš„å¯¹è¯æ ¼å¼
    ä½¿ç”¨Qwençš„promptæ¨¡æ¿
    """
    instruction = example['instruction']
    input_text = example['input']
    output_text = example.get('output', '')
    
    # Qwen Chatæ ¼å¼
    prompt = f"""<|im_start|>system
{instruction}<|im_end|>
<|im_start|>user
{input_text}<|im_end|>
<|im_start|>assistant
{output_text}<|im_end|>"""
    
    return prompt

def tokenize_function(examples, tokenizer):
    """åˆ†è¯å‡½æ•°"""
    prompts = [format_prompt(ex) for ex in examples]
    
    # åˆ†è¯
    model_inputs = tokenizer(
        prompts,
        max_length=CONFIG["max_length"],
        truncation=True,
        padding=False,
        return_tensors=None
    )
    
    # è®¾ç½®labels
    model_inputs["labels"] = model_inputs["input_ids"].copy()
    
    return model_inputs

def print_trainable_parameters(model):
    """æ‰“å°å¯è®­ç»ƒå‚æ•°"""
    trainable_params = 0
    all_params = 0
    for _, param in model.named_parameters():
        all_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    
    print(f"\nğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({100 * trainable_params / all_params:.2f}%)")
    print(f"  æ€»å‚æ•°: {all_params:,}")
    print(f"  èŠ‚çœå‚æ•°: {100 * (all_params - trainable_params) / all_params:.2f}%")

def plot_training_history(trainer, output_path):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼ˆåŒ…å«è®­ç»ƒlosså’ŒéªŒè¯lossï¼‰"""
    log_history = trainer.state.log_history
    
    # æå–è®­ç»ƒloss
    train_steps = []
    train_losses = []
    
    # æå–éªŒè¯loss
    eval_steps = []
    eval_losses = []
    
    for log in log_history:
        if 'loss' in log and 'eval_loss' not in log:
            train_steps.append(log['step'])
            train_losses.append(log['loss'])
        if 'eval_loss' in log:
            eval_steps.append(log['step'])
            eval_losses.append(log['eval_loss'])
    
    # ç»˜å›¾
    fig, ax = plt.subplots(figsize=(12, 7))
    
    # ç»˜åˆ¶è®­ç»ƒloss
    ax.plot(train_steps, train_losses, label='Training Loss', 
            linewidth=2, color='#2E86AB', alpha=0.8)
    
    # ç»˜åˆ¶éªŒè¯loss
    ax.plot(eval_steps, eval_losses, label='Validation Loss', 
            linewidth=2.5, color='#E63946', alpha=0.9, marker='o', 
            markersize=3, markevery=5)
    
    ax.set_xlabel('Steps', fontsize=13, fontweight='bold')
    ax.set_ylabel('Loss', fontsize=13, fontweight='bold')
    ax.set_title('Qwen-Question Training History', fontsize=16, fontweight='bold', pad=20)
    ax.legend(fontsize=12, loc='upper right', framealpha=0.9)
    ax.grid(True, alpha=0.3, linestyle='--')
    
    # æ·»åŠ æœ€ç»ˆlosså€¼çš„æ–‡æœ¬æ ‡æ³¨
    if train_losses and eval_losses:
        final_train = train_losses[-1]
        final_eval = eval_losses[-1]
        ax.text(0.02, 0.98, f'Final Train Loss: {final_train:.4f}\nFinal Eval Loss: {final_eval:.4f}',
                transform=ax.transAxes, fontsize=11,
                verticalalignment='top', bbox=dict(boxstyle='round', 
                facecolor='wheat', alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\nâœ“ è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {output_path}")

def main():
    print("="*60)
    print("ğŸš€ è®­ç»ƒ Qwen-Question æ¨¡å‹ï¼ˆé—®é¢˜ç”Ÿæˆï¼‰")
    print("="*60)
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"\nâœ“ GPU: {gpu_name}")
        print(f"âœ“ æ˜¾å­˜: {gpu_memory:.1f} GB")
    else:
        print("\nâš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆä¼šå¾ˆæ…¢ï¼‰")
    
    # åŠ è½½æ•°æ®
    print(f"\nğŸ“¥ åŠ è½½è®­ç»ƒæ•°æ®...")
    train_dataset = load_data(CONFIG["train_data"])
    val_dataset = load_data(CONFIG["val_data"])
    
    print(f"âœ“ è®­ç»ƒé›†: {len(train_dataset)} æ¡")
    print(f"âœ“ éªŒè¯é›†: {len(val_dataset)} æ¡")
    
    # åŠ è½½åˆ†è¯å™¨
    print(f"\nğŸ“¥ åŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(
        CONFIG["base_model"],
        padding_side="right"
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print(f"âœ“ åˆ†è¯å™¨åŠ è½½å®Œæˆ")
    
    # åˆ†è¯
    print(f"\nğŸ”„ å¯¹æ•°æ®è¿›è¡Œåˆ†è¯...")
    
    def tokenize_batch(examples):
        batch = []
        for i in range(len(examples['instruction'])):
            batch.append({
                'instruction': examples['instruction'][i],
                'input': examples['input'][i],
                'output': examples['output'][i]
            })
        return tokenize_function(batch, tokenizer)
    
    train_dataset = train_dataset.map(
        tokenize_batch,
        batched=True,
        remove_columns=train_dataset.column_names,
        desc="åˆ†è¯è®­ç»ƒé›†"
    )
    
    val_dataset = val_dataset.map(
        tokenize_batch,
        batched=True,
        remove_columns=val_dataset.column_names,
        desc="åˆ†è¯éªŒè¯é›†"
    )
    
    print(f"âœ“ åˆ†è¯å®Œæˆ")
    
    # 4bité‡åŒ–é…ç½®
    print(f"\nâš™ï¸  é…ç½®4bité‡åŒ–...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=CONFIG["use_4bit"],
        bnb_4bit_compute_dtype=CONFIG["bnb_4bit_compute_dtype"],
        bnb_4bit_quant_type=CONFIG["bnb_4bit_quant_type"],
        bnb_4bit_use_double_quant=True,
    )
    
    # åŠ è½½åŸºåº§æ¨¡å‹
    print(f"\nğŸ“¥ åŠ è½½åŸºåº§æ¨¡å‹: {CONFIG['base_model']}")
    print(f"   ä½¿ç”¨4bité‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜...")
    
    model = AutoModelForCausalLM.from_pretrained(
        CONFIG["base_model"],
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.float16,
    )
    
    print(f"âœ“ åŸºåº§æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # å‡†å¤‡æ¨¡å‹
    model = prepare_model_for_kbit_training(model)
    
    # LoRAé…ç½®
    print(f"\nâš™ï¸  é…ç½®LoRA...")
    lora_config = LoraConfig(
        r=CONFIG["lora_r"],
        lora_alpha=CONFIG["lora_alpha"],
        target_modules=CONFIG["lora_target_modules"],
        lora_dropout=CONFIG["lora_dropout"],
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    
    # åº”ç”¨LoRA
    model = get_peft_model(model, lora_config)
    print_trainable_parameters(model)
    
    # è®­ç»ƒå‚æ•°
    print(f"\nâš™ï¸  é…ç½®è®­ç»ƒå‚æ•°...")
    training_args = TrainingArguments(
        output_dir=CONFIG["output_dir"],
        num_train_epochs=CONFIG["num_epochs"],
        per_device_train_batch_size=CONFIG["batch_size"],
        per_device_eval_batch_size=CONFIG["batch_size"],
        gradient_accumulation_steps=CONFIG["gradient_accumulation"],
        learning_rate=CONFIG["learning_rate"],
        warmup_steps=CONFIG["warmup_steps"],
        logging_steps=CONFIG["logging_steps"],
        save_steps=CONFIG["save_steps"],
        eval_steps=CONFIG["eval_steps"],  # V3: ä½¿ç”¨ç‹¬ç«‹çš„eval_steps
        eval_strategy="steps",  # æ–°ç‰ˆæœ¬ä½¿ç”¨eval_strategy
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",  # V3: æ˜ç¡®æŒ‡å®šç”¨eval_lossé€‰æœ€ä½³æ¨¡å‹
        greater_is_better=False,  # V3: lossè¶Šå°è¶Šå¥½
        fp16=True,
        gradient_checkpointing=True,
        optim="paged_adamw_8bit",
        max_grad_norm=0.3,
        weight_decay=0.01,  # V3: æ·»åŠ æƒé‡è¡°å‡
        logging_dir=f"{CONFIG['output_dir']}/logs",
        report_to="none",
    )
    
    print(f"âœ“ è®­ç»ƒé…ç½® (V3 - é˜²è¿‡æ‹Ÿåˆä¼˜åŒ–):")
    print(f"  - Batch size: {CONFIG['batch_size']}")
    print(f"  - æ¢¯åº¦ç´¯ç§¯: {CONFIG['gradient_accumulation']} (ç­‰æ•ˆbatch={CONFIG['batch_size']*CONFIG['gradient_accumulation']})")
    print(f"  - Epochs: {CONFIG['num_epochs']} (ä»5é™è‡³3)")
    print(f"  - Learning rate: {CONFIG['learning_rate']} (ä»1.5e-4é™è‡³1.0e-4)")
    print(f"  - LoRA dropout: 0.15 (ä»0.1å¢è‡³0.15)")
    print(f"  - æ—©åœ: patience={CONFIG['early_stopping_patience']}")
    print(f"  - 4bité‡åŒ–: âœ“")
    print(f"  - æ¢¯åº¦æ£€æŸ¥ç‚¹: âœ“")
    print(f"  - é¢„æœŸæ˜¾å­˜: 5-6GB")
    
    # Data Collator
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        padding=True
    )
    
    # åˆ›å»ºTrainerï¼ˆæ·»åŠ æ—©åœï¼‰
    print(f"\nğŸ”„ åˆå§‹åŒ–è®­ç»ƒå™¨ï¼ˆå«æ—©åœæœºåˆ¶ï¼‰...")
    
    # æ—©åœå›è°ƒ
    early_stopping = EarlyStoppingCallback(
        early_stopping_patience=CONFIG["early_stopping_patience"],
        early_stopping_threshold=CONFIG["early_stopping_threshold"]
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
        callbacks=[early_stopping],  # V3: æ·»åŠ æ—©åœå›è°ƒ
    )
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\n" + "="*60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ V3ï¼ˆé˜²è¿‡æ‹Ÿåˆä¼˜åŒ–ç‰ˆï¼‰...")
    print("="*60)
    print(f"\né…ç½®å˜åŒ–:")
    print(f"  - Epochs: 5 â†’ 3")
    print(f"  - Learning Rate: 1.5e-4 â†’ 1.0e-4")
    print(f"  - LoRA Dropout: 0.1 â†’ 0.15")
    print(f"  - æ—©åœ: æ–°å¢ï¼ˆpatience=10ï¼‰")
    print(f"  - æƒé‡è¡°å‡: æ–°å¢ï¼ˆ0.01ï¼‰")
    print(f"\né¢„è®¡è®­ç»ƒæ—¶é—´: 20-30åˆ†é’Ÿï¼ˆ3 epochs + æ—©åœå¯èƒ½æ›´å¿«ï¼‰")
    print(f"å¯ä»¥ä½¿ç”¨ nvidia-smi ç›‘æ§æ˜¾å­˜ä½¿ç”¨æƒ…å†µ\n")
    
    try:
        trainer.train()
        
        print(f"\n" + "="*60)
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        print("="*60)
        
        # ä¿å­˜æ¨¡å‹
        print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
        trainer.save_model()
        print(f"âœ“ æ¨¡å‹å·²ä¿å­˜è‡³: {CONFIG['output_dir']}")
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        plot_path = Path("plots")
        plot_path.mkdir(exist_ok=True)
        plot_training_history(trainer, plot_path / "qwen_question_training.png")
        
        # æ˜¾ç¤ºæœ€ç»ˆæŒ‡æ ‡
        print(f"\nğŸ“Š æœ€ç»ˆè®­ç»ƒæŒ‡æ ‡:")
        final_metrics = trainer.state.log_history[-1]
        for key, value in final_metrics.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.4f}")
        
        print(f"\nğŸ‰ Qwen-Question è®­ç»ƒæˆåŠŸï¼")
        print(f"\nâœ… åŒQwenè®­ç»ƒå…¨éƒ¨å®Œæˆï¼")
        print(f"\nä¸‹ä¸€æ­¥:")
        print(f"  ä½¿ç”¨: python test_dual_qwen.py è¿›è¡Œæµ‹è¯•")
        
    except KeyboardInterrupt:
        print(f"\n\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        print(f"æ¨¡å‹checkpointå·²ä¿å­˜è‡³: {CONFIG['output_dir']}")
    except Exception as e:
        print(f"\n\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

