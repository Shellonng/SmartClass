"""
æµ‹è¯•åŒQwenæ¨¡å‹æ¨ç†
åŒæ—¶åŠ è½½ä¸¤ä¸ªLoRAï¼Œæ¼”ç¤ºå®Œæ•´çš„é¢è¯•æµç¨‹
"""

import torch
import sys
import io
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

class DualQwenInterviewer:
    """åŒQwené¢è¯•ç³»ç»Ÿ"""
    
    def __init__(self, 
                 base_model="Qwen/Qwen2-1.5B-Instruct",
                 decision_lora="checkpoints/qwen_decision_lora",
                 question_lora="checkpoints/qwen_question_lora",
                 use_4bit=True):
        """
        åˆå§‹åŒ–åŒQwenç³»ç»Ÿ
        
        å‚æ•°:
            base_model: åŸºåº§æ¨¡å‹
            decision_lora: å†³ç­–LoRAè·¯å¾„
            question_lora: æé—®LoRAè·¯å¾„
            use_4bit: æ˜¯å¦ä½¿ç”¨4bité‡åŒ–ï¼ˆæ¨ç†æ—¶æ¨èï¼‰
        """
        print("="*60)
        print("ğŸš€ åˆå§‹åŒ–åŒQwené¢è¯•ç³»ç»Ÿ")
        print("="*60)
        
        # é…ç½®
        self.use_4bit = use_4bit
        
        # åŠ è½½åˆ†è¯å™¨
        print(f"\nğŸ“¥ åŠ è½½åˆ†è¯å™¨...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            base_model
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½åŸºåº§æ¨¡å‹
        print(f"\nğŸ“¥ åŠ è½½åŸºåº§æ¨¡å‹: {base_model}")
        if use_4bit:
            print(f"   ä½¿ç”¨4bité‡åŒ–ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰")
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
            )
            self.base_model = AutoModelForCausalLM.from_pretrained(
                base_model,
                quantization_config=bnb_config,
                device_map="auto"
            )
        else:
            self.base_model = AutoModelForCausalLM.from_pretrained(
                base_model,
                device_map="auto",
                torch_dtype=torch.float16
            )
        
        # åŠ è½½ä¸¤ä¸ªLoRAï¼ˆåŒæ—¶åŠ è½½åˆ°åŒä¸€ä¸ªåŸºåº§ä¸Šï¼‰
        print(f"\nğŸ“¥ åŠ è½½Qwen-Decision LoRA...")
        self.decision_model = PeftModel.from_pretrained(
            self.base_model,
            decision_lora
        )
        
        print(f"\nğŸ“¥ åŠ è½½Qwen-Question LoRA...")
        self.question_model = PeftModel.from_pretrained(
            self.base_model,
            question_lora
        )
        
        print(f"\nâœ… åŒQwenç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        
        # æ˜¾ç¤ºæ˜¾å­˜å ç”¨
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated() / 1024**3
            memory_reserved = torch.cuda.memory_reserved() / 1024**3
            print(f"\nğŸ’¾ æ˜¾å­˜å ç”¨:")
            print(f"   å·²åˆ†é…: {memory_allocated:.2f} GB")
            print(f"   å·²é¢„ç•™: {memory_reserved:.2f} GB")
    
    def make_decision(self, topic, round_number, history, scores):
        """
        ä½¿ç”¨Qwen-Decisionåšå‡ºå†³ç­–
        
        è¿”å›: (action, guidance)
        """
        # æ„å»ºè¾“å…¥
        history_text = ""
        recent_history = history[-3:] if len(history) > 3 else history
        for h in recent_history:
            history_text += f"é—®: {h['question']}\nç­”: {h['answer']}\nè¯„åˆ†: {h['score']}åˆ†\n\n"
        
        avg_score = sum(scores) / len(scores) if scores else 0
        recent_trend = "stable"  # ç®€åŒ–
        
        input_text = f"""å½“å‰è¯é¢˜: {topic}
å½“å‰è½®æ¬¡: ç¬¬{round_number}è½®

å¯¹è¯å†å²:
{history_text.strip()}

è¯„åˆ†æƒ…å†µ:
å¹³å‡åˆ†: {avg_score:.0f}åˆ†
åˆ†æ•°è¶‹åŠ¿: {recent_trend}
æœ€è¿‘3æ¬¡: {scores[-3:]}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œåšå‡ºé¢è¯•å†³ç­–å¹¶æä¾›æŒ‡å¯¼å»ºè®®ã€‚"""
        
        instruction = "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„æŠ€æœ¯é¢è¯•å®˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®å½“å‰é¢è¯•è¯é¢˜ã€å¯¹è¯å†å²å’Œå€™é€‰äººçš„è¡¨ç°è¯„åˆ†ï¼Œåšå‡ºé¢è¯•å†³ç­–ï¼ˆFOLLOW_UPç»§ç»­æ·±å…¥ æˆ– SWITCH_TOPICåˆ‡æ¢è¯é¢˜ï¼‰ï¼Œå¹¶ä¸ºé—®é¢˜ç”Ÿæˆå™¨æä¾›è¯¦ç»†çš„æŒ‡å¯¼å»ºè®®ã€‚"
        
        # æ ¼å¼åŒ–prompt
        prompt = f"""<|im_start|>system
{instruction}<|im_end|>
<|im_start|>user
{input_text}<|im_end|>
<|im_start|>assistant
"""
        
        # ç”Ÿæˆ
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.decision_model.device)
        
        with torch.no_grad():
            outputs = self.decision_model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                top_p=0.9
            )
        
        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–å†³ç­–å’ŒæŒ‡å¯¼
        assistant_response = result.split("<|im_start|>assistant")[-1].strip()
        
        action = "SWITCH_TOPIC"  # é»˜è®¤
        guidance = assistant_response
        
        if "å†³ç­–:" in assistant_response:
            parts = assistant_response.split("æŒ‡å¯¼å»ºè®®:")
            action_part = parts[0].replace("å†³ç­–:", "").strip()
            if "FOLLOW_UP" in action_part:
                action = "FOLLOW_UP"
            elif "SWITCH_TOPIC" in action_part:
                action = "SWITCH_TOPIC"
            
            if len(parts) > 1:
                guidance = parts[1].strip()
        
        return action, guidance
    
    def generate_question(self, topic, history, guidance):
        """
        ä½¿ç”¨Qwen-Questionç”Ÿæˆé—®é¢˜
        
        è¿”å›: (question, importance)
        """
        # æ„å»ºè¾“å…¥
        history_text = ""
        for h in history:
            history_text += f"Q: {h['question']}\nA: {h['answer']}\n\n"
        
        input_text = f"""é¢è¯•è¯é¢˜: {topic}

å®Œæ•´å¯¹è¯å†å²:
{history_text.strip() if history_text else 'ï¼ˆè¿™æ˜¯ç¬¬ä¸€ä¸ªé—®é¢˜ï¼‰'}

å†³ç­–æŒ‡å¯¼:
{guidance}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ç”Ÿæˆä¸‹ä¸€ä¸ªé¢è¯•é—®é¢˜ï¼Œå¹¶è¯„ä¼°å…¶é‡è¦ç¨‹åº¦ï¼ˆ1-5åˆ†ï¼‰ã€‚"""
        
        instruction = "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„æŠ€æœ¯é¢è¯•å®˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®é¢è¯•è¯é¢˜ã€å®Œæ•´å¯¹è¯å†å²å’Œå†³ç­–æŒ‡å¯¼ï¼Œç”Ÿæˆä¸‹ä¸€ä¸ªåˆé€‚çš„é¢è¯•é—®é¢˜ï¼Œå¹¶è¯„ä¼°è¯¥é—®é¢˜çš„é‡è¦ç¨‹åº¦ï¼ˆ1-5åˆ†ï¼Œå…¶ä¸­1åˆ†ä¸ºé—²èŠï¼Œ5åˆ†ä¸ºæ ¸å¿ƒæŠ€èƒ½è€ƒå¯Ÿï¼‰ã€‚"
        
        # æ ¼å¼åŒ–prompt
        prompt = f"""<|im_start|>system
{instruction}<|im_end|>
<|im_start|>user
{input_text}<|im_end|>
<|im_start|>assistant
"""
        
        # ç”Ÿæˆ
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.question_model.device)
        
        with torch.no_grad():
            outputs = self.question_model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.8,
                do_sample=True,
                top_p=0.95
            )
        
        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–é—®é¢˜å’Œé‡è¦ç¨‹åº¦
        assistant_response = result.split("<|im_start|>assistant")[-1].strip()
        
        question = assistant_response
        importance = 3  # é»˜è®¤
        
        if "é—®é¢˜:" in assistant_response and "é‡è¦ç¨‹åº¦:" in assistant_response:
            parts = assistant_response.split("é‡è¦ç¨‹åº¦:")
            question = parts[0].replace("é—®é¢˜:", "").strip()
            
            if len(parts) > 1:
                importance_str = parts[1].strip().split("åˆ†")[0].strip()
                try:
                    importance = int(importance_str)
                except:
                    importance = 3
        
        return question, importance

def test_interview_flow():
    """æµ‹è¯•å®Œæ•´é¢è¯•æµç¨‹"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•åŒQwené¢è¯•æµç¨‹")
    print("="*60)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    interviewer = DualQwenInterviewer(use_4bit=True)
    
    # æ¨¡æ‹Ÿé¢è¯•åœºæ™¯
    topic = "Spring Bootæ¡†æ¶"
    history = [
        {
            "question": "è¯·ä»‹ç»ä¸€ä¸‹Spring Bootçš„æ ¸å¿ƒç‰¹æ€§",
            "answer": "Spring Bootä¸»è¦æä¾›äº†è‡ªåŠ¨é…ç½®ã€èµ·æ­¥ä¾èµ–ã€å†…åµŒæœåŠ¡å™¨ç­‰ç‰¹æ€§ï¼Œå¯ä»¥å¿«é€Ÿæ„å»ºSpringåº”ç”¨ã€‚",
            "score": 75
        },
        {
            "question": "Spring Bootçš„è‡ªåŠ¨é…ç½®åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "å—¯...æˆ‘çŸ¥é“æ˜¯é€šè¿‡æ³¨è§£å®ç°çš„ï¼Œä½†å…·ä½“åŸç†ä¸å¤ªæ¸…æ¥šã€‚",
            "score": 60
        }
    ]
    scores = [75, 60]
    
    print(f"\n{'='*60}")
    print("ğŸ“ æµ‹è¯•åœºæ™¯")
    print(f"{'='*60}")
    print(f"è¯é¢˜: {topic}")
    print(f"è½®æ¬¡: 3")
    print(f"å†å²è¯„åˆ†: {scores}")
    print(f"å¯¹è¯å†å²: {len(history)} è½®")
    
    # æ­¥éª¤1: å†³ç­–
    print(f"\n{'='*60}")
    print("æ­¥éª¤1: Qwen-Decision åšå‡ºå†³ç­–")
    print(f"{'='*60}")
    
    action, guidance = interviewer.make_decision(topic, 3, history, scores)
    
    print(f"\nå†³ç­–ç»“æœ:")
    print(f"  Action: {action}")
    print(f"  Guidance: {guidance[:200]}...")
    
    # æ­¥éª¤2: ç”Ÿæˆé—®é¢˜
    print(f"\n{'='*60}")
    print("æ­¥éª¤2: Qwen-Question ç”Ÿæˆé—®é¢˜")
    print(f"{'='*60}")
    
    question, importance = interviewer.generate_question(topic, history, guidance)
    
    print(f"\nç”Ÿæˆç»“æœ:")
    print(f"  é—®é¢˜: {question}")
    print(f"  é‡è¦ç¨‹åº¦: {importance}åˆ†")
    
    # æ€»ç»“
    print(f"\n{'='*60}")
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print(f"{'='*60}")
    
    print(f"\nå®Œæ•´æµç¨‹:")
    print(f"  1. è¾“å…¥: è¯é¢˜ã€å†å²ã€è¯„åˆ†")
    print(f"  2. Qwen-Decision â†’ {action} + æŒ‡å¯¼å»ºè®®")
    print(f"  3. Qwen-Question â†’ æ–°é—®é¢˜ + é‡è¦ç¨‹åº¦{importance}åˆ†")
    
    if torch.cuda.is_available():
        memory_allocated = torch.cuda.memory_allocated() / 1024**3
        print(f"\nğŸ’¾ æ¨ç†æ˜¾å­˜å ç”¨: {memory_allocated:.2f} GB")

if __name__ == "__main__":
    test_interview_flow()

