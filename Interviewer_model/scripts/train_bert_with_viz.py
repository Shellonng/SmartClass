"""
BERTè¿½é—®å†³ç­–åˆ†ç±»å™¨è®­ç»ƒï¼ˆå¸¦å®Œæ•´å¯è§†åŒ–ï¼‰
å®æ—¶æ˜¾ç¤ºï¼š
1. è®­ç»ƒé›†æŸå¤± vs éªŒè¯é›†æŸå¤±ï¼ˆæ£€æµ‹è¿‡æ‹Ÿåˆï¼‰
2. è®­ç»ƒé›†å‡†ç¡®ç‡ vs éªŒè¯é›†å‡†ç¡®ç‡
3. è®­ç»ƒé›†F1 vs éªŒè¯é›†F1
"""
import json
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    get_linear_schedule_with_warmup
)
from torch.optim import AdamW
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report
import numpy as np
from tqdm import tqdm
import os
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import threading

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

# ========== é…ç½® ==========
CONFIG = {
    "model_name": "bert-base-chinese",
    "train_data_path": "./data/bert_training_1500.json",  # ä½¿ç”¨1500æ¡æ•°æ®
    "output_dir": "./checkpoints/follow_up_classifier_1500",
    "max_length": 256,
    "batch_size": 8,
    "epochs": 5,  # å¢åŠ åˆ°5ä¸ªepochè§‚å¯Ÿè¿‡æ‹Ÿåˆ
    "learning_rate": 2e-5,
    "warmup_ratio": 0.1,
    "num_labels": 2,
    "label_map": {
        "FOLLOW_UP": 0,
        "NEXT_TOPIC": 1
    }
}

# ========== å…¨å±€å¯è§†åŒ–æ•°æ® ==========
viz_data = {
    "epoch": [],
    "train_loss": [],
    "val_loss": [],
    "train_acc": [],
    "val_acc": [],
    "train_f1": [],
    "val_f1": []
}

# ========== æ•°æ®é›†ç±» ==========
class FollowUpDataset(Dataset):
    """è¿½é—®å†³ç­–æ•°æ®é›†"""
    
    def __init__(self, data, tokenizer, label_map, max_length=256):
        self.data = data
        self.tokenizer = tokenizer
        self.label_map = label_map
        self.max_length = max_length
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        text = self._build_input_text(item)
        
        # Tokenize
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        label = self.label_map[item['label']]
        
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'labels': torch.tensor(label, dtype=torch.long)
        }
    
    def _build_input_text(self, item):
        """æ„å»ºBERTçš„è¾“å…¥æ–‡æœ¬"""
        context = item['context']
        
        features = f"è¿½é—®æ·±åº¦:{context['follow_up_depth']} " \
                  f"çŠ¹è±«åº¦:{context['hesitation_score']:.2f} " \
                  f"é•¿åº¦:{context['answer_length']}å­— " \
                  f"è¯é¢˜:{context.get('topic', 'æŠ€æœ¯')}"
        
        text = f"{item['question']}[SEP]{item['answer']}[SEP]{features}"
        return text

# ========== è®­ç»ƒå’Œè¯„ä¼°å‡½æ•° ==========
def train_epoch(model, dataloader, optimizer, scheduler, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    all_preds = []
    all_labels = []
    
    progress_bar = tqdm(dataloader, desc="Training")
    for batch in progress_bar:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        
        optimizer.zero_grad()
        
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        scheduler.step()
        
        total_loss += loss.item()
        
        # é¢„æµ‹
        preds = torch.argmax(outputs.logits, dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
        
        progress_bar.set_postfix({'loss': loss.item()})
    
    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='weighted')
    
    return avg_loss, accuracy, f1

def evaluate(model, dataloader, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        progress_bar = tqdm(dataloader, desc="Evaluating")
        for batch in progress_bar:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            
            loss = outputs.loss
            total_loss += loss.item()
            
            preds = torch.argmax(outputs.logits, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='weighted')
    
    return avg_loss, accuracy, f1, all_preds, all_labels

# ========== å¯è§†åŒ–å‡½æ•° ==========
def setup_visualization():
    """è®¾ç½®å¯è§†åŒ–çª—å£"""
    plt.ion()  # å¼€å¯äº¤äº’æ¨¡å¼
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('BERTè¿½é—®å†³ç­–æ¨¡å‹è®­ç»ƒç›‘æ§', fontsize=16, fontweight='bold')
    
    return fig, axes

def update_visualization(fig, axes, epoch):
    """æ›´æ–°å¯è§†åŒ–å›¾è¡¨"""
    # æ¸…ç©ºæ‰€æœ‰å­å›¾
    for ax in axes.flat:
        ax.clear()
    
    epochs = viz_data["epoch"]
    
    # å­å›¾1: æŸå¤±æ›²çº¿
    ax1 = axes[0, 0]
    ax1.plot(epochs, viz_data["train_loss"], 'b-o', label='è®­ç»ƒé›†æŸå¤±', linewidth=2)
    ax1.plot(epochs, viz_data["val_loss"], 'r-s', label='éªŒè¯é›†æŸå¤±', linewidth=2)
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.set_title('æŸå¤±æ›²çº¿ï¼ˆæ£€æµ‹è¿‡æ‹Ÿåˆï¼‰', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=11)
    ax1.grid(True, alpha=0.3)
    
    # æ ‡æ³¨è¿‡æ‹Ÿåˆè­¦å‘Š
    if len(epochs) >= 2:
        if viz_data["val_loss"][-1] > viz_data["val_loss"][-2]:
            ax1.text(0.5, 0.95, 'âš ï¸ éªŒè¯é›†æŸå¤±ä¸Šå‡ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ', 
                    transform=ax1.transAxes, ha='center', va='top',
                    bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5),
                    fontsize=10)
    
    # å­å›¾2: å‡†ç¡®ç‡æ›²çº¿
    ax2 = axes[0, 1]
    ax2.plot(epochs, viz_data["train_acc"], 'b-o', label='è®­ç»ƒé›†å‡†ç¡®ç‡', linewidth=2)
    ax2.plot(epochs, viz_data["val_acc"], 'r-s', label='éªŒè¯é›†å‡†ç¡®ç‡', linewidth=2)
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Accuracy', fontsize=12)
    ax2.set_title('å‡†ç¡®ç‡æ›²çº¿', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=11)
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim([0, 1])
    
    # å­å›¾3: F1åˆ†æ•°æ›²çº¿
    ax3 = axes[1, 0]
    ax3.plot(epochs, viz_data["train_f1"], 'b-o', label='è®­ç»ƒé›†F1', linewidth=2)
    ax3.plot(epochs, viz_data["val_f1"], 'r-s', label='éªŒè¯é›†F1', linewidth=2)
    ax3.set_xlabel('Epoch', fontsize=12)
    ax3.set_ylabel('F1 Score', fontsize=12)
    ax3.set_title('F1åˆ†æ•°æ›²çº¿', fontsize=14, fontweight='bold')
    ax3.legend(fontsize=11)
    ax3.grid(True, alpha=0.3)
    ax3.set_ylim([0, 1])
    
    # å­å›¾4: è®­ç»ƒ/éªŒè¯å·®è·åˆ†æ
    ax4 = axes[1, 1]
    if len(epochs) > 0:
        loss_gap = [t - v for t, v in zip(viz_data["train_loss"], viz_data["val_loss"])]
        acc_gap = [v - t for t, v in zip(viz_data["train_acc"], viz_data["val_acc"])]
        f1_gap = [v - t for t, v in zip(viz_data["train_f1"], viz_data["val_f1"])]
        
        ax4.plot(epochs, loss_gap, 'g-o', label='æŸå¤±å·®è·(è®­ç»ƒ-éªŒè¯)', linewidth=2)
        ax4.axhline(y=0, color='k', linestyle='--', alpha=0.3)
        ax4.set_xlabel('Epoch', fontsize=12)
        ax4.set_ylabel('Gap', fontsize=12)
        ax4.set_title('è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆåˆ†æ', fontsize=14, fontweight='bold')
        ax4.legend(fontsize=11)
        ax4.grid(True, alpha=0.3)
        
        # æ·»åŠ åˆ†ææ–‡æœ¬
        latest_loss_gap = loss_gap[-1]
        latest_acc_gap = acc_gap[-1]
        
        status_text = ""
        if latest_loss_gap < -0.1:
            status_text = "ğŸ”´ è¿‡æ‹Ÿåˆï¼šè®­ç»ƒæŸå¤±è¿œä½äºéªŒè¯æŸå¤±"
            color = 'red'
        elif latest_loss_gap > 0.05:
            status_text = "ğŸŸ¡ æ¬ æ‹Ÿåˆï¼šéªŒè¯æŸå¤±ä½äºè®­ç»ƒæŸå¤±"
            color = 'orange'
        else:
            status_text = "ğŸŸ¢ æ‹Ÿåˆè‰¯å¥½ï¼šè®­ç»ƒå’ŒéªŒè¯æŸå¤±æ¥è¿‘"
            color = 'green'
        
        ax4.text(0.5, 0.95, status_text, 
                transform=ax4.transAxes, ha='center', va='top',
                bbox=dict(boxstyle='round', facecolor=color, alpha=0.3),
                fontsize=10)
        
        # æ˜¾ç¤ºæœ€æ–°æŒ‡æ ‡
        metrics_text = f"æœ€æ–°æŒ‡æ ‡ (Epoch {epoch}):\n"
        metrics_text += f"è®­ç»ƒ: Loss={viz_data['train_loss'][-1]:.4f}, Acc={viz_data['train_acc'][-1]:.4f}, F1={viz_data['train_f1'][-1]:.4f}\n"
        metrics_text += f"éªŒè¯: Loss={viz_data['val_loss'][-1]:.4f}, Acc={viz_data['val_acc'][-1]:.4f}, F1={viz_data['val_f1'][-1]:.4f}"
        
        ax4.text(0.5, 0.5, metrics_text,
                transform=ax4.transAxes, ha='center', va='center',
                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5),
                fontsize=9, family='monospace')
    
    plt.tight_layout()
    plt.pause(0.1)

# ========== ä¸»å‡½æ•° ==========
def main():
    print("="*50)
    print("BERTè¿½é—®å†³ç­–åˆ†ç±»å™¨å¾®è°ƒï¼ˆå¸¦å¯è§†åŒ–ï¼‰")
    print("="*50)
    
    # æ£€æŸ¥è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. åŠ è½½æ•°æ®
    print("\n1. åŠ è½½è®­ç»ƒæ•°æ®...")
    with open(CONFIG['train_data_path'], 'r', encoding='utf-8') as f:
        all_data = json.load(f)
    
    print(f"æ•°æ®æ€»æ•°: {len(all_data)}")
    
    # åˆ†å‰²æ•°æ®é›†ï¼ˆ80%è®­ç»ƒï¼Œ20%éªŒè¯ï¼‰
    train_data, val_data = train_test_split(all_data, test_size=0.2, random_state=42)
    print(f"è®­ç»ƒé›†: {len(train_data)}, éªŒè¯é›†: {len(val_data)}")
    
    # 2. åŠ è½½æ¨¡å‹
    print("\n2. åŠ è½½æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])
    model = AutoModelForSequenceClassification.from_pretrained(
        CONFIG['model_name'],
        num_labels=CONFIG['num_labels']
    )
    model.to(device)
    print(f"æ¨¡å‹: {CONFIG['model_name']}")
    
    # 3. åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    print("\n3. åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨...")
    train_dataset = FollowUpDataset(train_data, tokenizer, CONFIG['label_map'], CONFIG['max_length'])
    val_dataset = FollowUpDataset(val_data, tokenizer, CONFIG['label_map'], CONFIG['max_length'])
    
    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'])
    
    # 4. è®¾ç½®ä¼˜åŒ–å™¨
    print("\n4. è®¾ç½®ä¼˜åŒ–å™¨...")
    optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'])
    
    total_steps = len(train_loader) * CONFIG['epochs']
    warmup_steps = int(total_steps * CONFIG['warmup_ratio'])
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )
    
    # 5. è®¾ç½®å¯è§†åŒ–
    print("\n5. è®¾ç½®å¯è§†åŒ–çª—å£...")
    fig, axes = setup_visualization()
    
    # 6. å¼€å§‹è®­ç»ƒ
    print("\n6. å¼€å§‹è®­ç»ƒ...")
    print("="*50)
    
    best_f1 = 0
    
    for epoch in range(1, CONFIG['epochs'] + 1):
        print(f"\nEpoch {epoch}/{CONFIG['epochs']}")
        print("-"*50)
        
        # è®­ç»ƒ
        train_loss, train_acc, train_f1 = train_epoch(
            model, train_loader, optimizer, scheduler, device
        )
        print(f"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}")
        
        # éªŒè¯
        val_loss, val_acc, val_f1, _, _ = evaluate(model, val_loader, device)
        print(f"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}")
        
        # æ›´æ–°å¯è§†åŒ–æ•°æ®
        viz_data["epoch"].append(epoch)
        viz_data["train_loss"].append(train_loss)
        viz_data["val_loss"].append(val_loss)
        viz_data["train_acc"].append(train_acc)
        viz_data["val_acc"].append(val_acc)
        viz_data["train_f1"].append(train_f1)
        viz_data["val_f1"].append(val_f1)
        
        # æ›´æ–°å¯è§†åŒ–
        update_visualization(fig, axes, epoch)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_f1 > best_f1:
            best_f1 = val_f1
            print(f"[BEST] æ–°çš„æœ€ä½³F1: {best_f1:.4f}ï¼Œä¿å­˜æ¨¡å‹...")
            
            os.makedirs(CONFIG['output_dir'], exist_ok=True)
            model.save_pretrained(CONFIG['output_dir'])
            tokenizer.save_pretrained(CONFIG['output_dir'])
        
        # è¿‡æ‹Ÿåˆæ£€æµ‹
        if epoch > 1 and val_loss > viz_data["val_loss"][-2]:
            print("[WARNING] éªŒè¯é›†æŸå¤±ä¸Šå‡ï¼Œå¯èƒ½å‡ºç°è¿‡æ‹Ÿåˆï¼")
    
    # 7. æœ€ç»ˆè¯„ä¼°
    print("\n" + "="*50)
    print("7. æœ€ç»ˆè¯„ä¼°ï¼ˆä½¿ç”¨æœ€ä½³æ¨¡å‹ï¼‰ï¼š")
    print("="*50)
    
    model = AutoModelForSequenceClassification.from_pretrained(CONFIG['output_dir'])
    model.to(device)
    
    _, _, _, final_preds, final_labels = evaluate(model, val_loader, device)
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    label_names = ['FOLLOW_UP', 'NEXT_TOPIC']
    print("\nåˆ†ç±»æŠ¥å‘Šï¼š")
    print(classification_report(final_labels, final_preds, target_names=label_names))
    
    print(f"\n[DONE] è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {CONFIG['output_dir']}")
    print(f"æœ€ä½³éªŒè¯F1: {best_f1:.4f}")
    
    # ä¿å­˜å¯è§†åŒ–å›¾è¡¨
    viz_path = "./training_visualization.png"
    plt.savefig(viz_path, dpi=300, bbox_inches='tight')
    print(f"\nå¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {viz_path}")
    
    print("\næŒ‰ä»»æ„é”®å…³é—­å¯è§†åŒ–çª—å£...")
    plt.ioff()
    plt.show()

if __name__ == "__main__":
    main()

