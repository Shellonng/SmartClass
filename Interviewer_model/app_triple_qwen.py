"""
AI Interviewer - Triple Qwenç‰ˆï¼ˆå…¨Qwenæ¶æ„ï¼‰
ä½¿ç”¨ä¸‰ä¸ªå¾®è°ƒçš„Qwenæ¨¡å‹ï¼šDecisionã€Questionã€Scorer
"""
import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from pathlib import Path
import json
import tempfile
from datetime import datetime
import sys
import importlib.util
import re

# ç›´æ¥å¯¼å…¥ResumeParser
spec = importlib.util.spec_from_file_location("resume_parser", "models/resume_parser.py")
resume_parser_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(resume_parser_module)
ResumeParser = resume_parser_module.ResumeParser

# å¯¼å…¥æ•°å­—äºº
spec_dh = importlib.util.spec_from_file_location("digital_human", "models/digital_human.py")
digital_human_module = importlib.util.module_from_spec(spec_dh)
spec_dh.loader.exec_module(digital_human_module)
DigitalHuman = digital_human_module.DigitalHuman

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="AI Interviewer - Triple Qwen",
    page_icon="ğŸš€",
    layout="wide"
)

# CSSæ ·å¼
st.markdown("""
<style>
    .avatar-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 20px;
        padding: 2rem;
        margin-bottom: 1.5rem;
        text-align: center;
        box-shadow: 0 10px 40px rgba(102, 126, 234, 0.3);
    }
    
    .virtual-avatar {
        width: 180px;
        height: 180px;
        border-radius: 50%;
        background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
        border: 5px solid white;
        box-shadow: 0 8px 25px rgba(0, 0, 0, 0.2);
        animation: float 3s ease-in-out infinite;
        display: inline-flex;
        align-items: center;
        justify-content: center;
        font-size: 80px;
    }
    
    @keyframes float {
        0%, 100% { transform: translateY(0px); }
        50% { transform: translateY(-10px); }
    }
    
    .avatar-speech {
        background: white;
        border-radius: 20px;
        padding: 1.5rem;
        margin-top: 1.5rem;
        font-size: 1.1rem;
        color: #333;
        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
    }
    
    .chat-message {
        margin-bottom: 1rem;
        display: flex;
    }
    
    .chat-message.ai { justify-content: flex-start; }
    .chat-message.user { justify-content: flex-end; }
    
    .message-bubble {
        max-width: 70%;
        padding: 0.8rem 1rem;
        border-radius: 12px;
    }
    
    .chat-message.ai .message-bubble {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        box-shadow: 0 3px 10px rgba(102, 126, 234, 0.3);
    }
    
    .chat-message.user .message-bubble {
        background: white;
        color: #333;
        border: 1px solid #e0e0e0;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
    }
    
    .score-card {
        background: white;
        border-radius: 12px;
        padding: 1rem;
        margin-bottom: 1rem;
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 12px;
        padding: 1.5rem;
        text-align: center;
        margin-bottom: 1rem;
    }
</style>
""", unsafe_allow_html=True)

# ==================== åŠ è½½Triple Qwenæ¨¡å‹ ====================
@st.cache_resource
def load_triple_qwen_models():
    """åŠ è½½ä¸‰ä¸ªå¾®è°ƒçš„Qwenæ¨¡å‹"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    base_model_name = "Qwen/Qwen2-1.5B-Instruct"
    
    print("[INFO] Loading Triple Qwen models...")
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # 4bité‡åŒ–é…ç½®ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )
    
    models = {}
    
    # åŠ è½½ä¸‰ä¸ªæ¨¡å‹
    model_configs = [
        ("decision", "checkpoints/qwen_decision_lora", "Qwen-Decision"),
        ("question", "checkpoints/qwen_question_lora", "Qwen-Question"),
        ("scorer", "checkpoints/qwen_scorer_lora", "Qwen-Scorer")
    ]
    
    for model_key, lora_path, model_name in model_configs:
        try:
            # åŠ è½½åŸºåº§æ¨¡å‹
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                quantization_config=bnb_config,
                device_map="auto"
            )
            
            # åŠ è½½LoRAæƒé‡
            if Path(lora_path).exists():
                model = PeftModel.from_pretrained(base_model, lora_path)
                model.eval()
                models[model_key] = model
                print(f"[INFO] {model_name} loaded successfully")
            else:
                print(f"[ERROR] LoRA path not found: {lora_path}")
                models[model_key] = None
        except Exception as e:
            print(f"[ERROR] Failed to load {model_name}: {str(e)}")
            models[model_key] = None
    
    return {
        'tokenizer': tokenizer,
        'decision_model': models['decision'],
        'question_model': models['question'],
        'scorer_model': models['scorer'],
        'device': device
    }

# ==================== Triple Qwenæ¨ç†å‡½æ•° ====================

def generate_with_qwen(model, tokenizer, instruction, input_text, max_tokens=256, temperature=0.7, device='cuda'):
    """é€šç”¨Qwenç”Ÿæˆå‡½æ•°"""
    if model is None:
        return "[é”™è¯¯] æ¨¡å‹æœªåŠ è½½"
    
    try:
        # æ„å»ºprompt
        messages = [
            {"role": "system", "content": instruction},
            {"role": "user", "content": input_text}
        ]
        
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = tokenizer(text, return_tensors='pt').to(device)
        input_length = inputs['input_ids'].shape[1]
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                do_sample=True,
                top_p=0.9,
                pad_token_id=tokenizer.pad_token_id
            )
        
        # åªè§£ç ç”Ÿæˆçš„æ–°token
        generated_tokens = outputs[0][input_length:]
        response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()
        
        return response
    
    except Exception as e:
        print(f"[ERROR] Generation failed: {str(e)}")
        return f"[é”™è¯¯] ç”Ÿæˆå¤±è´¥: {str(e)}"

def decision_make(models, resume_data, history, scores, current_topic, next_topic):
    """Qwen-Decision: åšå‡ºå†³ç­–ï¼Œç»™å‡ºè¯¦ç»†æŒ‡å¯¼"""
    
    # æ„å»ºå¯¹è¯å†å²
    history_text = ""
    recent_history = history[-3:] if len(history) > 3 else history
    for h in recent_history:
        history_text += f"é—®: {h['question']}\nç­”: {h['answer'][:100]}...\nè¯„åˆ†: {h.get('score', 70)}åˆ†\n\n"
    
    avg_score = sum(scores) / len(scores) if scores else 0
    round_number = len(history) + 1
    
    # å‡†å¤‡next_topicçš„æè¿°
    next_topic_desc = ""
    if next_topic:
        if next_topic.startswith("é¡¹ç›®:"):
            proj_name = next_topic.replace("é¡¹ç›®:", "")
            # ä»ç®€å†ä¸­æ‰¾é¡¹ç›®è¯¦æƒ…
            for proj in resume_data.get('projects', []):
                if proj.get('name') == proj_name:
                    tech_stack = ', '.join(proj.get('tech_stack', [])[:3])
                    next_topic_desc = f"'{proj_name}'é¡¹ç›®ï¼ˆæŠ€æœ¯æ ˆï¼š{tech_stack}ï¼‰"
                    break
        elif next_topic.startswith("æŠ€èƒ½:"):
            skill = next_topic.replace("æŠ€èƒ½:", "")
            next_topic_desc = f"'{skill}'æŠ€èƒ½"
    
    input_text = f"""å½“å‰è¯é¢˜: {current_topic}

å¯¹è¯å†å²:
{history_text.strip() if history_text else 'ï¼ˆè¿™æ˜¯ç¬¬ä¸€è½®ï¼‰'}

è¯„åˆ†: å¹³å‡{avg_score:.0f}åˆ†

å¦‚éœ€åˆ‡æ¢è¯é¢˜ï¼Œä¸‹ä¸€ä¸ªè¯é¢˜æ˜¯: {next_topic_desc if next_topic_desc else 'æ— '}"""
    
    instruction = """ä½ æ˜¯æŠ€æœ¯é¢è¯•å®˜ã€‚æ ¹æ®å¯¹è¯å†å²å’Œè¯„åˆ†ï¼Œåšå‡ºå†³ç­–ã€‚

è¾“å‡ºæ ¼å¼ï¼š
å†³ç­–: FOLLOW_UP æˆ– SWITCH_TOPIC
æŒ‡å¯¼å»ºè®®: [ç®€çŸ­è¯´æ˜ï¼Œå¦‚"ç»§ç»­æ·±å…¥XXX"æˆ–"åˆ‡æ¢åˆ°XXXé¡¹ç›®"]"""
    
    # åœ¨è°ƒç”¨æ¨¡å‹å‰ï¼Œå…ˆåšè§„åˆ™åˆ¤æ–­
    force_switch = False
    force_reason = ""
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶åˆ‡æ¢è¯é¢˜
    if len(history) >= 2:
        recent_2 = history[-2:]
        
        # è§„åˆ™1: è¿ç»­2æ¬¡ä½åˆ† (<60åˆ†)
        if all(h.get('score', 70) < 60 for h in recent_2):
            force_switch = True
            force_reason = f"å€™é€‰äººè¿ç»­2æ¬¡ä½åˆ†({recent_2[-2]['score']}, {recent_2[-1]['score']}åˆ†)ï¼Œå»ºè®®åˆ‡æ¢è¯é¢˜"
        
        # è§„åˆ™2: å€™é€‰äººæ˜ç¡®è¡¨ç¤ºä¸æ‡‚
        negative_keywords = ['ä¸çŸ¥é“', 'å¿˜äº†', 'ä¸ä¼š', 'æ²¡åšè¿‡', 'ä¸æ¸…æ¥š', 'ä¸äº†è§£', 'ä¸å¤ªæ‡‚']
        last_answer = history[-1]['answer']
        if any(keyword in last_answer for keyword in negative_keywords) and history[-1].get('score', 70) < 70:
            force_switch = True
            force_reason = f"å€™é€‰äººè¡¨ç¤ºä¸äº†è§£å½“å‰è¯é¢˜(è¯„åˆ†{history[-1]['score']}åˆ†)ï¼Œå»ºè®®åˆ‡æ¢åˆ°ç®€å†ä¸­çš„å…¶ä»–é¡¹ç›®"
    
    # å¦‚æœéœ€è¦å¼ºåˆ¶åˆ‡æ¢ï¼Œç›´æ¥è¿”å›
    if force_switch:
        # ä½¿ç”¨æä¾›çš„next_topicä¿¡æ¯
        if next_topic_desc:
            guidance = f"{force_reason}ã€‚åˆ‡æ¢åˆ°{next_topic_desc}ã€‚"
        else:
            guidance = f"{force_reason}ã€‚å»ºè®®ç»“æŸé¢è¯•æˆ–æ€»ç»“ã€‚"
        
        return {
            'action': 'SWITCH_TOPIC',
            'guidance': guidance,
            'raw_response': f"[è§„åˆ™å¼ºåˆ¶] {guidance}",
            'force_switch': True
        }
    
    # å¦åˆ™è°ƒç”¨æ¨¡å‹å†³ç­–
    response = generate_with_qwen(
        models['decision_model'],
        models['tokenizer'],
        instruction,
        input_text,
        max_tokens=100,  # å‡å°‘tokenæ•°ï¼Œè¦æ±‚ç®€çŸ­
        temperature=0.5,  # é™ä½temperatureï¼Œè®©å†³ç­–æ›´ç¨³å®š
        device=models['device']
    )
    
    # è§£æå†³ç­–å’ŒæŒ‡å¯¼
    action = "SWITCH_TOPIC"
    guidance = response
    
    if "å†³ç­–:" in response or "å†³ç­–ï¼š" in response:
        parts = re.split(r'æŒ‡å¯¼å»ºè®®[ï¼š:]', response)
        action_part = parts[0].replace("å†³ç­–:", "").replace("å†³ç­–ï¼š", "").strip()
        
        if "FOLLOW_UP" in action_part.upper():
            action = "FOLLOW_UP"
        elif "SWITCH_TOPIC" in action_part.upper() or "SWITCH" in action_part.upper():
            action = "SWITCH_TOPIC"
        
        if len(parts) > 1:
            guidance = parts[1].strip()
    
    return {
        'action': action,
        'guidance': guidance,
        'raw_response': response
    }

def question_generate(models, history, guidance, topic):
    """Qwen-Question: æ ¹æ®guidanceå’Œå½“å‰topicç”Ÿæˆé—®é¢˜"""
    
    # æ„å»ºå¯¹è¯å†å²ï¼ˆæœ€è¿‘1è½®å®Œæ•´å›ç­”ï¼‰
    history_text = ""
    last_answer = ""
    if history:
        last_qa = history[-1]
        last_answer = last_qa['answer'][:150]  # ä¿ç•™æ›´å¤šå›ç­”å†…å®¹
        history_text = f"æœ€è¿‘ä¸€è½®:\nQ: {last_qa['question'][:80]}...\nA: {last_answer}\n"
    
    # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯ä»"è‡ªæˆ‘ä»‹ç»"åˆ‡æ¢åˆ°ç¬¬ä¸€ä¸ªé¡¹ç›®ï¼Œä½¿ç”¨å¼€æ”¾æ€§å¼•å…¥
    if (len(history) == 1 and 
        history[0]['question'].startswith("ä½ å¥½ï¼é¦–å…ˆ") and 
        topic.startswith("é¡¹ç›®:")):
        # ç¬¬ä¸€ä¸ªé¡¹ç›®çš„å¼•å…¥æ€§é—®é¢˜
        proj_name = topic.replace("é¡¹ç›®:", "")
        input_text = f"""è¯é¢˜: {topic}
æŒ‡å¯¼: {guidance}

è¿™æ˜¯ä»è‡ªæˆ‘ä»‹ç»åˆ‡æ¢åˆ°ç¬¬ä¸€ä¸ªé¡¹ç›®ï¼Œç”Ÿæˆä¸€ä¸ªå¼€æ”¾æ€§çš„å¼•å…¥é—®é¢˜ã€‚"""
        
        instruction = """ç”Ÿæˆé¢è¯•é—®é¢˜ã€‚å¯¹äºé¡¹ç›®ï¼Œå¯ä»¥ç”¨"æˆ‘çœ‹åˆ°ä½ çš„ç®€å†ä¸­æœ‰XXXé¡¹ç›®ï¼Œèƒ½è¯¦ç»†ä»‹ç»ä¸€ä¸‹å—ï¼Ÿ"è¿™æ ·çš„å¼€æ”¾æ€§é—®é¢˜ã€‚

è¾“å‡ºï¼š
é—®é¢˜: [é—®é¢˜å†…å®¹]
é‡è¦ç¨‹åº¦: [1-5åˆ†]"""
    else:
        # æ­£å¸¸æµç¨‹
        input_text = f"""è¯é¢˜: {topic}
{history_text}
æŒ‡å¯¼: {guidance}

æ³¨æ„ï¼šæ ¹æ®å€™é€‰äººçš„å®é™…å›ç­”æé—®ï¼Œä¸è¦å‡è®¾å€™é€‰äººè¯´è¿‡æŸäº›è¯ã€‚"""
        
        instruction = """æ ¹æ®æŒ‡å¯¼ç”Ÿæˆé¢è¯•é—®é¢˜ã€‚ä¸è¦ç¼–é€ å€™é€‰äººæ²¡è¯´è¿‡çš„å†…å®¹ã€‚

è¾“å‡ºï¼š
é—®é¢˜: [é—®é¢˜å†…å®¹]
é‡è¦ç¨‹åº¦: [1-5åˆ†]"""
    
    response = generate_with_qwen(
        models['question_model'],
        models['tokenizer'],
        instruction,
        input_text,
        max_tokens=150,
        temperature=0.5,  # é™ä½temperatureï¼Œå‡å°‘"åˆ›é€ æ€§"
        device=models['device']
    )
    
    # è§£æé—®é¢˜å’Œé‡è¦ç¨‹åº¦
    question = response
    importance = 3
    
    if "é—®é¢˜:" in response or "é—®é¢˜ï¼š" in response:
        parts = re.split(r'é‡è¦ç¨‹åº¦[ï¼š:]', response)
        question = parts[0].replace("é—®é¢˜:", "").replace("é—®é¢˜ï¼š", "").strip()
        
        if len(parts) > 1:
            importance_str = parts[1].strip().split("åˆ†")[0].strip()
            try:
                importance = int(importance_str)
            except:
                # å°è¯•æå–æ•°å­—
                nums = re.findall(r'\d+', importance_str)
                if nums:
                    importance = int(nums[0])
                else:
                    importance = 3
    
    return {
        'question': question,
        'importance': importance,
        'raw_response': response
    }

def answer_evaluate(models, question, answer):
    """Qwen-Scorer: è¯„ä¼°å›ç­”"""
    input_text = f"""é¢è¯•é—®é¢˜: {question}

å€™é€‰äººå›ç­”:
{answer}

è¯·è¯„ä¼°è¿™ä¸ªå›ç­”çš„è´¨é‡ï¼Œç»™å‡ºè¯„åˆ†ï¼ˆ0-100åˆ†ï¼‰ã€æ ‡ç­¾ï¼ˆexcellent/good/average/poorï¼‰å’Œè¯„ä»·ã€‚"""
    
    instruction = "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„æŠ€æœ¯é¢è¯•å®˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯è¯„ä¼°å€™é€‰äººå¯¹æŠ€æœ¯é—®é¢˜çš„å›ç­”è´¨é‡ï¼Œç»™å‡ºè¯„åˆ†ï¼ˆ0-100åˆ†ï¼‰ã€æ ‡ç­¾ï¼ˆexcellent/good/average/poorï¼‰å’Œè¯¦ç»†è¯„ä»·ã€‚è¯„åˆ†æ ‡å‡†ï¼šexcellent(85-100)è¡¨ç¤ºå›ç­”å‡†ç¡®ã€æ·±å…¥ã€æœ‰å®æˆ˜ç»éªŒï¼›good(70-84)è¡¨ç¤ºå›ç­”æ­£ç¡®ä½†ä¸å¤Ÿæ·±å…¥ï¼›average(50-69)è¡¨ç¤ºå›ç­”éƒ¨åˆ†æ­£ç¡®æˆ–è¾ƒæµ…ï¼›poor(0-49)è¡¨ç¤ºå›ç­”é”™è¯¯æˆ–å®Œå…¨ä¸ä¼šã€‚"
    
    response = generate_with_qwen(
        models['scorer_model'],
        models['tokenizer'],
        instruction,
        input_text,
        max_tokens=256,
        temperature=0.7,
        device=models['device']
    )
    
    # è§£æè¯„åˆ†ã€æ ‡ç­¾å’Œè¯„ä»·
    score = 70
    label = "average"
    comment = response
    
    # æå–è¯„åˆ†
    if "è¯„åˆ†:" in response or "è¯„åˆ†ï¼š" in response:
        score_match = re.search(r'è¯„åˆ†[ï¼š:]\s*(\d+)\s*åˆ†?', response)
        if score_match:
            score = int(score_match.group(1))
    
    # æå–æ ‡ç­¾
    if "æ ‡ç­¾:" in response or "æ ‡ç­¾ï¼š" in response:
        label_match = re.search(r'æ ‡ç­¾[ï¼š:]\s*(\w+)', response)
        if label_match:
            label_text = label_match.group(1).lower()
            if 'excellent' in label_text or 'ä¼˜ç§€' in label_text:
                label = 'excellent'
            elif 'good' in label_text or 'è‰¯å¥½' in label_text:
                label = 'good'
            elif 'poor' in label_text or 'å·®' in label_text:
                label = 'poor'
            else:
                label = 'average'
    
    # æå–è¯„ä»·
    if "è¯„ä»·:" in response or "è¯„ä»·ï¼š" in response:
        comment_parts = re.split(r'è¯„ä»·[ï¼š:]', response)
        if len(comment_parts) > 1:
            comment = comment_parts[1].strip()
    
    # æ ‡ç­¾æ˜ å°„ä¸ºä¸­æ–‡
    label_map = {
        'excellent': 'ä¼˜ç§€',
        'good': 'è‰¯å¥½',
        'average': 'ä¸€èˆ¬',
        'poor': 'å·®'
    }
    
    return {
        'score': score,
        'label': label,
        'label_cn': label_map.get(label, 'ä¸€èˆ¬'),
        'comment': comment,
        'raw_response': response
    }

# ==================== ä¸»ç•Œé¢ ====================
st.title("ğŸš€ AI Interviewer - Triple Qwen")
st.caption("åŸºäºå…¨Qwenæ¶æ„çš„æ™ºèƒ½é¢è¯•ç³»ç»Ÿ | Decision + Question + Scorer")

# åˆå§‹åŒ–session_state
if 'stage' not in st.session_state:
    st.session_state.stage = 'upload'
if 'resume_data' not in st.session_state:
    st.session_state.resume_data = None
if 'current_question' not in st.session_state:
    st.session_state.current_question = None
if 'current_importance' not in st.session_state:
    st.session_state.current_importance = 3
if 'qa_history' not in st.session_state:
    st.session_state.qa_history = []
if 'total_rounds' not in st.session_state:
    st.session_state.total_rounds = 0
if 'job_position' not in st.session_state:
    st.session_state.job_position = 'Pythonåç«¯å·¥ç¨‹å¸ˆ'
if 'digital_human' not in st.session_state:
    st.session_state.digital_human = DigitalHuman()
if 'current_guidance' not in st.session_state:
    st.session_state.current_guidance = None
if 'current_action' not in st.session_state:
    st.session_state.current_action = None
if 'current_topic' not in st.session_state:
    st.session_state.current_topic = "è‡ªæˆ‘ä»‹ç»"
if 'topic_queue' not in st.session_state:
    st.session_state.topic_queue = []
if 'topic_index' not in st.session_state:
    st.session_state.topic_index = 0

# ==================== é˜¶æ®µ1: ä¸Šä¼ ç®€å† ====================
if st.session_state.stage == 'upload':
    st.markdown("---")
    st.subheader("ğŸ“„ æ­¥éª¤1ï¼šä¸Šä¼ ç®€å†")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        upload_method = st.radio(
            "é€‰æ‹©ä¸Šä¼ æ–¹å¼",
            ["ä¸Šä¼ æ–‡ä»¶ (PDF/DOCX)", "ç›´æ¥ç²˜è´´æ–‡æœ¬"],
            horizontal=True
        )
        
        resume_text = ""
        
        if upload_method == "ä¸Šä¼ æ–‡ä»¶ (PDF/DOCX)":
            uploaded_file = st.file_uploader(
                "ä¸Šä¼ ç®€å†æ–‡ä»¶",
                type=['pdf', 'docx'],
                help="æ”¯æŒPDFå’ŒDOCXæ ¼å¼"
            )
            
            if uploaded_file:
                with tempfile.NamedTemporaryFile(delete=False, suffix=Path(uploaded_file.name).suffix) as tmp_file:
                    tmp_file.write(uploaded_file.getbuffer())
                    tmp_path = tmp_file.name
                
                try:
                    with st.spinner("æ­£åœ¨è§£æç®€å†..."):
                        parser = ResumeParser()
                        resume_data = parser.parse(tmp_path)
                        st.session_state.resume_data = resume_data
                    
                    st.success(f"âœ… ç®€å†è§£ææˆåŠŸï¼")
                    
                    with st.expander("ğŸ“‹ æŸ¥çœ‹è§£æç»“æœ", expanded=True):
                        # åŸºæœ¬ä¿¡æ¯
                        st.markdown("### ğŸ‘¤ åŸºæœ¬ä¿¡æ¯")
                        col_info1, col_info2 = st.columns(2)
                        with col_info1:
                            st.markdown(f"**å§“åï¼š** {resume_data['name']}")
                            if resume_data.get('basic_info', {}).get('gender'):
                                st.markdown(f"**æ€§åˆ«ï¼š** {resume_data['basic_info']['gender']}")
                            if resume_data.get('basic_info', {}).get('birth_date'):
                                st.markdown(f"**å‡ºç”Ÿå¹´æœˆï¼š** {resume_data['basic_info']['birth_date']}")
                        with col_info2:
                            if resume_data.get('contact', {}).get('phone'):
                                st.markdown(f"**ç”µè¯ï¼š** {resume_data['contact']['phone']}")
                            if resume_data.get('contact', {}).get('email'):
                                st.markdown(f"**é‚®ç®±ï¼š** {resume_data['contact']['email']}")
                            if resume_data.get('basic_info', {}).get('origin'):
                                st.markdown(f"**ç±è´¯ï¼š** {resume_data['basic_info']['origin']}")
                        
                        # æ•™è‚²èƒŒæ™¯
                        if resume_data.get('education'):
                            st.markdown("### ğŸ“ æ•™è‚²èƒŒæ™¯")
                            for edu in resume_data['education']:
                                edu_parts = []
                                if edu.get('school'):
                                    edu_parts.append(edu['school'])
                                if edu.get('degree'):
                                    edu_parts.append(edu['degree'])
                                if edu.get('major'):
                                    edu_parts.append(edu['major'])
                                if edu.get('graduation_year'):
                                    edu_parts.append(f"{edu['graduation_year']}å¹´")
                                
                                st.markdown(f"**{' Â· '.join(edu_parts)}**")
                                if edu.get('gpa'):
                                    st.markdown(f"  æˆç»©ï¼š{edu['gpa']}")
                        
                        # æŠ€èƒ½
                        st.markdown("### ğŸ’¼ æŠ€èƒ½")
                        if resume_data['skills']:
                            st.markdown(f"{', '.join(resume_data['skills'][:15])}")
                            if len(resume_data['skills']) > 15:
                                st.caption(f"ç­‰{len(resume_data['skills'])}é¡¹æŠ€èƒ½")
                        else:
                            st.caption("æœªæå–åˆ°æŠ€èƒ½ä¿¡æ¯")
                        
                        # é¡¹ç›®ç»å†
                        if resume_data.get('projects'):
                            st.markdown("### ğŸš€ é¡¹ç›®ç»å†")
                            for i, proj in enumerate(resume_data['projects'][:3], 1):
                                st.markdown(f"**{i}. {proj.get('name', 'é¡¹ç›®' + str(i))}**")
                                if proj.get('tech_stack'):
                                    st.caption(f"æŠ€æœ¯æ ˆ: {', '.join(proj['tech_stack'][:5])}")
                                if proj.get('responsibilities'):
                                    st.caption(f"{proj['responsibilities'][:100]}...")
                                elif proj.get('description'):
                                    st.caption(f"{proj['description'][:100]}...")
                        
                        # å·¥ä½œç»å†
                        if resume_data.get('experience'):
                            st.markdown("### ğŸ’¼ å·¥ä½œç»å†")
                            for exp in resume_data['experience'][:3]:
                                exp_parts = []
                                if exp.get('company'):
                                    exp_parts.append(exp['company'])
                                if exp.get('position'):
                                    exp_parts.append(exp['position'])
                                if exp.get('start_date') and exp.get('end_date'):
                                    exp_parts.append(f"({exp['start_date']} - {exp['end_date']})")
                                
                                st.markdown(f"**{' Â· '.join(exp_parts)}**")
                                if exp.get('description'):
                                    st.caption(f"{exp['description'][:100]}...")
                    
                    resume_text = resume_data['raw_text']
                    
                except Exception as e:
                    st.error(f"ç®€å†è§£æå¤±è´¥ï¼š{str(e)}")
                finally:
                    Path(tmp_path).unlink(missing_ok=True)
        else:
            resume_text = st.text_area(
                "ç²˜è´´ç®€å†å†…å®¹",
                height=300,
                placeholder="å§“åï¼šå¼ ä¸‰\nåº”è˜èŒä½ï¼šPythonåç«¯å·¥ç¨‹å¸ˆ\n\næŠ€èƒ½ï¼šPython, Django, Redis, MySQL..."
            )
        
        job_position = st.text_input("åº”è˜èŒä½", value="Pythonåç«¯å·¥ç¨‹å¸ˆ")
    
    with col2:
        st.markdown("""
        <div class="metric-card">
            <h3>ğŸš€ Triple Qwen</h3>
            <p style="margin-top: 1rem; font-size: 0.9rem;">
            âœ… Qwen-Decision<br/>
            âœ… Qwen-Question<br/>
            âœ… Qwen-Scorer<br/><br/>
            <strong>å…¨Qwenæ¶æ„</strong><br/>
            ä¸“ä¸šÂ·æ™ºèƒ½Â·é«˜æ•ˆ
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        if st.session_state.resume_data:
            st.info(f"**ğŸ‘¤ å€™é€‰äººï¼š** {st.session_state.resume_data['name']}")
            if st.session_state.resume_data.get('education'):
                edu = st.session_state.resume_data['education'][0]
                if edu.get('school'):
                    st.info(f"**ğŸ“ å­¦æ ¡ï¼š** {edu['school'][:10]}...")
            st.success(f"**ğŸ’¼ æŠ€èƒ½ï¼š**\n\n" + "\n".join([f"â€¢ {s}" for s in st.session_state.resume_data['skills'][:5]]))
    
    if st.button("ğŸš€ å¼€å§‹é¢è¯•", type="primary", use_container_width=True):
        if resume_text.strip() or st.session_state.resume_data:
            if not st.session_state.resume_data and resume_text:
                # ä½¿ç”¨ResumeParserè§£ææ–‡æœ¬
                parser = ResumeParser()
                try:
                    # å°è¯•å®Œæ•´è§£æ
                    import tempfile
                    from pathlib import Path
                    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8') as tmp:
                        tmp.write(resume_text)
                        tmp_path = tmp.name
                    st.session_state.resume_data = parser.parse(tmp_path)
                    Path(tmp_path).unlink(missing_ok=True)
                except:
                    # è§£æå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
                    st.session_state.resume_data = {
                        'name': 'å€™é€‰äºº',
                        'skills': [s for s in ['Python', 'Java', 'JavaScript', 'Redis', 'MySQL'] if s.lower() in resume_text.lower()],
                        'projects': [],
                        'raw_text': resume_text
                    }
            
            # åˆå§‹åŒ–topicé˜Ÿåˆ—ï¼ˆä»ç®€å†ä¸­æå–ï¼‰
            topic_queue = ["è‡ªæˆ‘ä»‹ç»"]  # ç¬¬ä¸€ä¸ªtopicå›ºå®šä¸ºè‡ªæˆ‘ä»‹ç»
            
            # ä»é¡¹ç›®å¼€å§‹æ·»åŠ 
            if st.session_state.resume_data.get('projects'):
                for proj in st.session_state.resume_data['projects']:
                    if proj.get('name'):
                        topic_queue.append(f"é¡¹ç›®:{proj['name']}")
            
            # ç„¶åæ·»åŠ æ ¸å¿ƒæŠ€èƒ½
            if st.session_state.resume_data.get('skills'):
                core_skills = st.session_state.resume_data['skills'][:5]  # æœ€å¤š5ä¸ªæ ¸å¿ƒæŠ€èƒ½
                for skill in core_skills:
                    topic_queue.append(f"æŠ€èƒ½:{skill}")
            
            st.session_state.topic_queue = topic_queue
            st.session_state.topic_index = 0
            st.session_state.current_topic = topic_queue[0]
            st.session_state.current_question = None
            st.session_state.job_position = job_position
            st.session_state.stage = 'interview'
            st.rerun()
        else:
            st.error("è¯·å…ˆä¸Šä¼ ç®€å†æˆ–ç²˜è´´æ–‡æœ¬")

# ==================== é˜¶æ®µ2: é¢è¯•è¿‡ç¨‹ ====================
elif st.session_state.stage == 'interview':
    with st.spinner("æ­£åœ¨åŠ è½½Triple Qwenæ¨¡å‹..."):
        models = load_triple_qwen_models()
    
    # ä¾§è¾¹æ  - æ¨¡å‹çŠ¶æ€
    with st.sidebar:
        st.markdown("### ğŸ” Triple QwençŠ¶æ€")
        
        if models['decision_model']:
            st.success("âœ… Qwen-Decision")
        else:
            st.error("âŒ Qwen-Decision")
        
        if models['question_model']:
            st.success("âœ… Qwen-Question")
        else:
            st.error("âŒ Qwen-Question")
        
        if models['scorer_model']:
            st.success("âœ… Qwen-Scorer")
        else:
            st.error("âŒ Qwen-Scorer")
        
        st.markdown("---")
        
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated() / 1024**3
            st.metric("æ˜¾å­˜å ç”¨", f"{memory_allocated:.2f} GB")
        
        # æ˜¾ç¤ºè¯é¢˜é˜Ÿåˆ—
        st.markdown("---")
        st.markdown("### ğŸ“‹ è¯é¢˜è¿›åº¦")
        
        for i, topic in enumerate(st.session_state.topic_queue):
            if i == st.session_state.topic_index:
                st.markdown(f"**â¤ {topic}** â¬…ï¸ å½“å‰")
            elif i < st.session_state.topic_index:
                st.markdown(f"âœ… ~~{topic}~~")
            else:
                st.markdown(f"â³ {topic}")
        
        # æ˜¾ç¤ºå½“å‰Decisionçš„æŒ‡å¯¼
        if st.session_state.current_guidance:
            st.markdown("---")
            st.markdown("### ğŸ¯ DecisionæŒ‡å¯¼")
            
            if st.session_state.current_action:
                action_color = "ğŸŸ¢" if st.session_state.current_action == "FOLLOW_UP" else "ğŸ”µ"
                st.markdown(f"**åŠ¨ä½œ**: {action_color} {st.session_state.current_action}")
            
            with st.expander("ğŸ“ è¯¦ç»†æŒ‡å¯¼", expanded=True):
                st.markdown(st.session_state.current_guidance)
    
    # ç”Ÿæˆç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
    if st.session_state.current_question is None:
        # ç¬¬ä¸€ä¸ªé—®é¢˜å›ºå®šä¸ºè‡ªæˆ‘ä»‹ç»
        st.session_state.current_question = "ä½ å¥½ï¼é¦–å…ˆè¯·ä½ ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ï¼ŒåŒ…æ‹¬ä½ çš„æ•™è‚²èƒŒæ™¯ã€ä¸»è¦æŠ€èƒ½å’Œé¡¹ç›®ç»éªŒã€‚"
        st.session_state.current_importance = 2  # è‡ªæˆ‘ä»‹ç»é‡è¦åº¦ä¸º2åˆ†ï¼ˆå¼€åœºé—²èŠï¼‰
    
    # é¡¶éƒ¨ä¿¡æ¯
    if st.session_state.resume_data:
        topic_progress = f"è¯é¢˜ {st.session_state.topic_index + 1}/{len(st.session_state.topic_queue)}: {st.session_state.current_topic}"
        st.caption(f"ğŸ‘¤ {st.session_state.resume_data.get('name', 'å€™é€‰äºº')} | åº”è˜ï¼š{st.session_state.job_position} | å·²å®Œæˆï¼š{st.session_state.total_rounds}è½® | {topic_progress}")
    
    # ä¸»å¸ƒå±€
    col_main, col_score = st.columns([2.5, 1])
    
    with col_main:
        # è™šæ‹Ÿå½¢è±¡
        st.markdown(f"""
        <div class="avatar-container">
            <div class="virtual-avatar">ğŸ¤–</div>
            <div style="margin-top: 2rem; color: white;">
                <h3 style="margin: 0;">Alice Â· Triple Qwen</h3>
                <p style="margin: 0.5rem 0; opacity: 0.9;">ğŸ“Œ å½“å‰è¯é¢˜ï¼š{st.session_state.current_topic}</p>
                <p style="margin: 0.5rem 0; opacity: 0.9;">é—®é¢˜é‡è¦åº¦ï¼š{'â­' * st.session_state.current_importance} | ç¬¬{st.session_state.total_rounds + 1}è½®</p>
            </div>
        </div>
        
        <div class="avatar-speech">
            <div style="font-size: 0.9rem; color: #999; margin-bottom: 0.5rem;">ğŸ’¬ é¢è¯•å®˜æé—®ï¼š</div>
            <div style="font-size: 1.1rem; line-height: 1.6;">{st.session_state.current_question}</div>
        </div>
        """, unsafe_allow_html=True)
        
        # ç”Ÿæˆå¹¶æ’­æ”¾è¯­éŸ³
        try:
            audio_file = st.session_state.digital_human.text_to_speech(st.session_state.current_question)
            if audio_file and Path(audio_file).exists():
                import base64
                import hashlib
                
                with open(audio_file, 'rb') as f:
                    audio_bytes = f.read()
                audio_base64 = base64.b64encode(audio_bytes).decode()
                
                question_hash = hashlib.md5(st.session_state.current_question.encode()).hexdigest()[:8]
                audio_id = f"tts_{question_hash}"
                
                if 'last_audio_id' not in st.session_state or st.session_state.last_audio_id != audio_id:
                    st.session_state.last_audio_id = audio_id
                    
                    st.components.v1.html(f"""
                    <audio id="{audio_id}" autoplay style="display:none;">
                        <source src="data:audio/mp3;base64,{audio_base64}" type="audio/mp3">
                    </audio>
                    """, height=0)
        except Exception as e:
            print(f"[WARNING] Audio generation failed: {e}")
        
        # å¯¹è¯å†å²
        st.markdown("### ğŸ’¬ å¯¹è¯è®°å½•")
        
        with st.container():
            st.markdown('<div style="max-height: 350px; overflow-y: auto; padding: 1rem; background: #f8f9fa; border-radius: 10px; margin-bottom: 1rem;">', unsafe_allow_html=True)
            
            if not st.session_state.qa_history:
                st.markdown('<div style="text-align: center; color: #999; padding: 2rem;">æš‚æ— å¯¹è¯è®°å½•</div>', unsafe_allow_html=True)
            else:
                for qa in st.session_state.qa_history[-5:]:
                    # AIæ¶ˆæ¯
                    st.markdown(f'''
                    <div class="chat-message ai">
                        <div class="message-bubble">
                            <div style="font-size: 0.85rem; opacity: 0.9;">ğŸ¤– é¢è¯•å®˜</div>
                            <div style="margin-top: 0.3rem;">{qa["question"]}</div>
                        </div>
                    </div>
                    ''', unsafe_allow_html=True)
                    
                    # ç”¨æˆ·æ¶ˆæ¯
                    score_emoji = {'ä¼˜ç§€': 'ğŸŸ¢', 'è‰¯å¥½': 'ğŸ”µ', 'ä¸€èˆ¬': 'ğŸŸ¡', 'å·®': 'ğŸ”´'}.get(qa.get('label_cn', 'ä¸€èˆ¬'), 'âšª')
                    st.markdown(f'''
                    <div class="chat-message user">
                        <div class="message-bubble">
                            <div style="font-size: 0.85rem; opacity: 0.7;">ğŸ‘¤ å€™é€‰äºº</div>
                            <div style="margin-top: 0.3rem;">{qa["answer"]}</div>
                            <div style="font-size: 0.75rem; margin-top: 0.5rem; opacity: 0.8;">{score_emoji} {qa.get('label_cn', 'ä¸€èˆ¬')} ({qa.get('score', 70)}åˆ†)</div>
                        </div>
                    </div>
                    ''', unsafe_allow_html=True)
            
            st.markdown('</div>', unsafe_allow_html=True)
        
        # å›ç­”è¾“å…¥
        st.markdown("### âœï¸ æ‚¨çš„å›ç­”")
        answer = st.text_area(
            "è¯·è¾“å…¥æ‚¨çš„å›ç­”",
            height=120,
            key=f"answer_{st.session_state.total_rounds}",
            placeholder="åœ¨æ­¤è¾“å…¥æ‚¨çš„å›ç­”..."
        )
        
        col_s, col_e = st.columns([3, 1])
        
        with col_s:
            if st.button("âœ… æäº¤å›ç­”", type="primary", use_container_width=True):
                if answer.strip():
                    # 1. Qwen-Scorerè¯„ä¼°
                    with st.spinner("ğŸ” Qwen-Scoreræ­£åœ¨è¯„ä¼°..."):
                        eval_result = answer_evaluate(models, st.session_state.current_question, answer)
                    
                    st.sidebar.markdown("### ğŸ“Š Scorerè¯„ä¼°")
                    st.sidebar.write(f"**è¯„åˆ†**: {eval_result['score']}åˆ†")
                    st.sidebar.write(f"**æ ‡ç­¾**: {eval_result['label_cn']}")
                    st.sidebar.write(f"**è¯„ä»·**: {eval_result['comment'][:100]}...")
                    
                    # è®°å½•
                    st.session_state.qa_history.append({
                        'question': st.session_state.current_question,
                        'answer': answer,
                        'score': eval_result['score'],
                        'label': eval_result['label'],
                        'label_cn': eval_result['label_cn'],
                        'comment': eval_result['comment']
                    })
                    
                    st.session_state.total_rounds += 1
                    
                    # 2. Qwen-Decisionå†³ç­–
                    with st.spinner("ğŸ§  Qwen-Decisionæ­£åœ¨å†³ç­–..."):
                        scores = [qa['score'] for qa in st.session_state.qa_history]
                        
                        # è·å–ä¸‹ä¸€ä¸ªtopic
                        next_topic = None
                        if st.session_state.topic_index < len(st.session_state.topic_queue) - 1:
                            next_topic = st.session_state.topic_queue[st.session_state.topic_index + 1]
                        
                        decision_result = decision_make(
                            models,
                            st.session_state.resume_data,
                            st.session_state.qa_history,
                            scores,
                            st.session_state.current_topic,
                            next_topic
                        )
                    
                    # ä¿å­˜Decisionç»“æœåˆ°session_state
                    st.session_state.current_action = decision_result['action']
                    st.session_state.current_guidance = decision_result['guidance']
                    
                    # å¦‚æœå†³ç­–æ˜¯SWITCH_TOPICï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªtopic
                    if decision_result['action'] == 'SWITCH_TOPIC':
                        # ä»topicé˜Ÿåˆ—ä¸­å–ä¸‹ä¸€ä¸ª
                        if st.session_state.topic_index < len(st.session_state.topic_queue) - 1:
                            st.session_state.topic_index += 1
                            st.session_state.current_topic = st.session_state.topic_queue[st.session_state.topic_index]
                        # guidanceå·²ç»åŒ…å«äº†topicä¿¡æ¯ï¼Œä¸éœ€è¦å†æ›´æ–°
                    
                    # 3. Qwen-Questionç”Ÿæˆé—®é¢˜
                    with st.spinner("â“ Qwen-Questionæ­£åœ¨ç”Ÿæˆ..."):
                        question_result = question_generate(
                            models,
                            st.session_state.qa_history,
                            st.session_state.current_guidance,
                            st.session_state.current_topic
                        )
                    
                    # æ›´æ–°çŠ¶æ€
                    st.session_state.current_question = question_result['question']
                    st.session_state.current_importance = question_result['importance']
                    
                    st.rerun()
                else:
                    st.error("è¯·è¾“å…¥å›ç­”")
        
        with col_e:
            if st.button("ğŸ ç»“æŸé¢è¯•", use_container_width=True):
                if st.session_state.qa_history:
                    st.session_state.stage = 'summary'
                    st.rerun()
    
    with col_score:
        st.markdown("### ğŸ“Š å®æ—¶è¯„åˆ†")
        
        if st.session_state.qa_history:
            latest = st.session_state.qa_history[-1]
            
            st.markdown(f"""
            <div class="score-card">
                <div style="color: #999; font-size: 0.9rem;">å½“å‰å›ç­”</div>
                <div style="font-size: 2rem; font-weight: bold; color: #667eea;">{latest['label_cn']}</div>
                <div style="color: #999; font-size: 0.85rem;">{latest['score']}åˆ†</div>
            </div>
            
            <div class="score-card">
                <div style="color: #999; font-size: 0.9rem;">å¹³å‡åˆ†</div>
                <div style="font-size: 2rem; font-weight: bold; color: #667eea;">{sum(qa['score'] for qa in st.session_state.qa_history) / len(st.session_state.qa_history):.1f}</div>
                <div style="color: #999; font-size: 0.85rem;">æ»¡åˆ†100</div>
            </div>
            
            <div class="score-card">
                <div style="color: #999; font-size: 0.9rem;">å·²å®Œæˆè½®æ•°</div>
                <div style="font-size: 2rem; font-weight: bold; color: #667eea;">{st.session_state.total_rounds}</div>
            </div>
            """, unsafe_allow_html=True)
            
            # æ˜¾ç¤ºè¯„ä»·
            if latest.get('comment'):
                with st.expander("ğŸ’¬ AIè¯„ä»·"):
                    st.write(latest['comment'])
        else:
            st.info("å›ç­”é—®é¢˜åå°†æ˜¾ç¤ºè¯„åˆ†")

# ==================== é˜¶æ®µ3: æ€»ç»“ ====================
elif st.session_state.stage == 'summary':
    st.markdown("---")
    st.subheader("ğŸ“ˆ é¢è¯•æ€»ç»“æŠ¥å‘Š")
    
    if st.session_state.qa_history:
        avg_score = sum(qa['score'] for qa in st.session_state.qa_history) / len(st.session_state.qa_history)
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("å¹³å‡è¯„åˆ†", f"{avg_score:.1f}/100")
        with col2:
            st.metric("æ€»è½®æ•°", st.session_state.total_rounds)
        with col3:
            excellent_count = sum(1 for qa in st.session_state.qa_history if qa['label'] == 'excellent')
            st.metric("ä¼˜ç§€", f"{excellent_count}ä¸ª")
        with col4:
            good_count = sum(1 for qa in st.session_state.qa_history if qa['label'] in ['excellent', 'good'])
            st.metric("è‰¯å¥½ä»¥ä¸Š", f"{good_count}ä¸ª")
        
        st.markdown("---")
        
        if avg_score >= 85:
            st.success(f"### ğŸŒŸ å¼ºçƒˆæ¨è\nå€™é€‰äººè¡¨ç°ä¼˜ç§€ï¼ŒæŠ€æœ¯åŠŸåº•æ‰å®ã€‚")
        elif avg_score >= 70:
            st.success(f"### ğŸ‘ æ¨è\nå€™é€‰äººå…·å¤‡ç›¸åº”æŠ€èƒ½ï¼Œè¡¨ç°è‰¯å¥½ã€‚")
        elif avg_score >= 50:
            st.warning(f"### ğŸ¤” å¾…å®š\nå€™é€‰äººåŸºç¡€ä¸€èˆ¬ï¼Œéœ€è¿›ä¸€æ­¥è€ƒå¯Ÿã€‚")
        else:
            st.error(f"### âŒ ä¸æ¨è\nå€™é€‰äººæŠ€æœ¯èƒ½åŠ›ä¸è¶³ã€‚")
        
        st.markdown("---")
        st.markdown("### ğŸ“ å¯¹è¯è¯¦æƒ…")
        
        for i, qa in enumerate(st.session_state.qa_history, 1):
            with st.expander(f"ç¬¬{i}è½® - {qa['label_cn']} ({qa['score']}åˆ†)"):
                st.markdown(f"**Q:** {qa['question']}")
                st.markdown(f"**A:** {qa['answer']}")
                st.markdown(f"**è¯„åˆ†:** {qa['score']}åˆ† ({qa['label_cn']})")
                st.markdown(f"**AIè¯„ä»·:** {qa.get('comment', 'æ— ')}")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ”„ é‡æ–°å¼€å§‹", type="primary", use_container_width=True):
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.rerun()
    
    with col2:
        if st.button("ğŸ“¥ å¯¼å‡ºæŠ¥å‘Š", use_container_width=True):
            report = {
                'time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'candidate': st.session_state.resume_data.get('name', 'æœªçŸ¥'),
                'job_position': st.session_state.job_position,
                'avg_score': avg_score,
                'total_rounds': st.session_state.total_rounds,
                'qa_history': st.session_state.qa_history
            }
            
            st.download_button(
                "ğŸ’¾ ä¸‹è½½æŠ¥å‘Š",
                data=json.dumps(report, ensure_ascii=False, indent=2),
                file_name=f"interview_triple_qwen_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json",
                use_container_width=True
            )

