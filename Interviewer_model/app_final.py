"""
AI Interviewer - å®Œæ•´ç‰ˆï¼ˆä½¿ç”¨Qwen LoRAç”Ÿæˆè¿½é—®ï¼‰
"""
import streamlit as st
import torch
import torch.nn as nn
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    AutoModel,
    AutoModelForCausalLM,
    BitsAndBytesConfig
)
from peft import PeftModel
from pathlib import Path
import json
import tempfile
from datetime import datetime
import sys
import importlib.util

# ç›´æ¥å¯¼å…¥ResumeParser
spec = importlib.util.spec_from_file_location("resume_parser", "models/resume_parser.py")
resume_parser_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(resume_parser_module)
ResumeParser = resume_parser_module.ResumeParser

# å¯¼å…¥æ•°å­—äºº
spec_dh = importlib.util.spec_from_file_location("digital_human", "models/digital_human.py")
digital_human_module = importlib.util.module_from_spec(spec_dh)
spec_dh.loader.exec_module(digital_human_module)
DigitalHuman = digital_human_module.DigitalHuman

# å¯¼å…¥ Linly-Talker å®¢æˆ·ç«¯
spec_lt = importlib.util.spec_from_file_location("linly_talker_client", "models/linly_talker_client.py")
linly_module = importlib.util.module_from_spec(spec_lt)
spec_lt.loader.exec_module(linly_module)
LinlyTalkerClient = linly_module.LinlyTalkerClient

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="AI Interviewer",
    page_icon="ğŸ¯",
    layout="wide"
)

# CSSæ ·å¼ - æ•°å­—äºº+èŠå¤©ç•Œé¢
st.markdown("""
<style>
    /* è™šæ‹Ÿå½¢è±¡å®¹å™¨ */
    .avatar-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 20px;
        padding: 2rem;
        margin-bottom: 1.5rem;
        text-align: center;
        box-shadow: 0 10px 40px rgba(102, 126, 234, 0.3);
        position: relative;
        overflow: hidden;
    }
    
    .avatar-wrapper {
        position: relative;
        display: inline-block;
    }
    
    /* è™šæ‹Ÿå½¢è±¡ - åŠ¨ç”»æ•ˆæœ */
    .virtual-avatar {
        width: 180px;
        height: 180px;
        border-radius: 50%;
        background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
        border: 5px solid white;
        box-shadow: 0 8px 25px rgba(0, 0, 0, 0.2);
        position: relative;
        animation: float 3s ease-in-out infinite;
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 80px;
    }
    
    @keyframes float {
        0%, 100% { 
            transform: translateY(0px);
        }
        50% { 
            transform: translateY(-10px);
        }
    }
    
    /* å£°çº¹æ•ˆæœ */
    .sound-wave {
        position: absolute;
        bottom: -20px;
        left: 50%;
        transform: translateX(-50%);
        display: flex;
        gap: 4px;
        align-items: flex-end;
        height: 60px;
    }
    
    .sound-bar {
        width: 6px;
        background: linear-gradient(to top, #4ade80, #22c55e);
        border-radius: 3px;
        animation: sound-wave 0.8s ease-in-out infinite;
    }
    
    .sound-bar:nth-child(1) { animation-delay: 0s; }
    .sound-bar:nth-child(2) { animation-delay: 0.1s; }
    .sound-bar:nth-child(3) { animation-delay: 0.2s; }
    .sound-bar:nth-child(4) { animation-delay: 0.3s; }
    .sound-bar:nth-child(5) { animation-delay: 0.4s; }
    .sound-bar:nth-child(6) { animation-delay: 0.3s; }
    .sound-bar:nth-child(7) { animation-delay: 0.2s; }
    .sound-bar:nth-child(8) { animation-delay: 0.1s; }
    
    @keyframes sound-wave {
        0%, 100% { height: 15px; }
        50% { height: 45px; }
    }
    
    /* è¯´è¯çŠ¶æ€æŒ‡ç¤ºå™¨ */
    .status-indicator {
        position: absolute;
        bottom: 15px;
        right: 15px;
        width: 25px;
        height: 25px;
        background: #4ade80;
        border-radius: 50%;
        border: 3px solid white;
        animation: pulse-glow 1.5s ease-in-out infinite;
        box-shadow: 0 0 15px rgba(74, 222, 128, 0.6);
    }
    
    @keyframes pulse-glow {
        0%, 100% { 
            transform: scale(1);
            opacity: 1;
        }
        50% { 
            transform: scale(1.2);
            opacity: 0.7;
        }
    }
    
    .avatar-speech {
        background: white;
        border-radius: 20px;
        padding: 1.5rem;
        margin-top: 1.5rem;
        position: relative;
        font-size: 1.1rem;
        color: #333;
        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
        animation: fade-in 0.5s ease;
    }
    
    @keyframes fade-in {
        from { opacity: 0; transform: translateY(-10px); }
        to { opacity: 1; transform: translateY(0); }
    }
    
    .avatar-speech::before {
        content: '';
        position: absolute;
        top: -10px;
        left: 50%;
        transform: translateX(-50%);
        width: 0;
        height: 0;
        border-left: 12px solid transparent;
        border-right: 12px solid transparent;
        border-bottom: 12px solid white;
    }
    
    /* èŠå¤©æ¶ˆæ¯ */
    .chat-message {
        margin-bottom: 1rem;
        display: flex;
    }
    
    .chat-message.ai {
        justify-content: flex-start;
    }
    
    .chat-message.user {
        justify-content: flex-end;
    }
    
    .message-bubble {
        max-width: 70%;
        padding: 0.8rem 1rem;
        border-radius: 12px;
    }
    
    .chat-message.ai .message-bubble {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        box-shadow: 0 3px 10px rgba(102, 126, 234, 0.3);
        border-bottom-left-radius: 4px;
    }
    
    .chat-message.user .message-bubble {
        background: white;
        color: #333;
        border: 1px solid #e0e0e0;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
        border-bottom-right-radius: 4px;
    }
    
    .score-card {
        background: white;
        border-radius: 12px;
        padding: 1rem;
        margin-bottom: 1rem;
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    
    /* éšè—éŸ³é¢‘æ’­æ”¾å™¨ */
    audio {
        display: none !important;
    }
</style>
""", unsafe_allow_html=True)

# ==================== å®šä¹‰æ¨¡å‹ ====================
class MultiTaskRoBERTa(nn.Module):
    """RoBERTaå¤šä»»åŠ¡æ¨¡å‹"""
    def __init__(self, model_name, num_labels=4):
        super().__init__()
        self.roberta = AutoModel.from_pretrained(model_name)
        hidden_size = self.roberta.config.hidden_size
        
        self.classification_head = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, num_labels)
        )
        
        self.regression_head = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, 1),
            nn.Sigmoid()
        )
    
    def forward(self, input_ids, attention_mask):
        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0, :]
        
        classification_logits = self.classification_head(pooled_output)
        regression_score = self.regression_head(pooled_output).squeeze(-1)
        
        return classification_logits, regression_score

# ==================== åŠ è½½æ¨¡å‹ ====================
@st.cache_resource
def load_models():
    """åŠ è½½æ‰€æœ‰å¾®è°ƒåçš„æ¨¡å‹"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 1. BERTå†³ç­–æ¨¡å‹
    bert_path = "./checkpoints/follow_up_classifier_1500"
    bert_tokenizer = AutoTokenizer.from_pretrained(bert_path)
    bert_model = AutoModelForSequenceClassification.from_pretrained(bert_path)
    bert_model.to(device)
    bert_model.eval()
    
    # 2. RoBERTaè¯„ä¼°æ¨¡å‹
    roberta_path = "./checkpoints/answer_evaluator"
    roberta_base = "./models/chinese-roberta-wwm-ext"
    roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_path)
    roberta_model = MultiTaskRoBERTa(roberta_base, num_labels=4)
    
    model_file = Path(roberta_path) / "pytorch_model.bin"
    state_dict = torch.load(model_file, map_location='cpu', weights_only=False)
    roberta_model.load_state_dict(state_dict)
    roberta_model.to(device)
    roberta_model.eval()
    
    # 3. Qwenæ¨¡å‹ï¼ˆæ”¯æŒåŸºåº§/LoRAåˆ‡æ¢ï¼‰
    qwen_base = "Qwen/Qwen2-1.5B-Instruct"
    lora_path = "./checkpoints/qwen_interviewer_lora"
    
    qwen_tokenizer = None
    qwen_base_model = None
    qwen_lora_model = None
    
    try:
        # åŠ è½½tokenizerå’ŒåŸºåº§æ¨¡å‹
        qwen_tokenizer = AutoTokenizer.from_pretrained(qwen_base, trust_remote_code=True)
        qwen_base_model = AutoModelForCausalLM.from_pretrained(
            qwen_base,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16
        )
        qwen_base_model.eval()
        print(f"[INFO] Qwen base model loaded successfully")
        
        # å°è¯•åŠ è½½LoRAæƒé‡ï¼ˆå¯é€‰ï¼‰
        if Path(lora_path).exists():
            try:
                qwen_lora_model = PeftModel.from_pretrained(qwen_base_model, lora_path)
                qwen_lora_model.eval()
                print(f"[INFO] Qwen LoRA model loaded successfully")
            except Exception as e:
                print(f"[WARNING] LoRA loading failed: {str(e)}, will use base model only")
                qwen_lora_model = None
        else:
            print(f"[INFO] LoRA checkpoint not found, using base model only")
    except Exception as e:
        print(f"[ERROR] Qwen loading failed: {str(e)}")
        qwen_base_model = None
    
    return {
        'bert_model': bert_model,
        'bert_tokenizer': bert_tokenizer,
        'roberta_model': roberta_model,
        'roberta_tokenizer': roberta_tokenizer,
        'qwen_base_model': qwen_base_model,     # åŸºåº§æ¨¡å‹
        'qwen_lora_model': qwen_lora_model,     # LoRAæ¨¡å‹ï¼ˆå¯èƒ½ä¸ºNoneï¼‰
        'qwen_tokenizer': qwen_tokenizer,
        'device': device
    }

# ==================== è¾…åŠ©å‡½æ•° ====================
def generate_initial_question(models, skills, job_position, use_lora=False):
    """ä½¿ç”¨Qwenç”Ÿæˆç¬¬ä¸€ä¸ªé¢è¯•é—®é¢˜
    
    Args:
        models: æ¨¡å‹å­—å…¸
        skills: å€™é€‰äººæŠ€èƒ½åˆ—è¡¨
        job_position: åº”è˜èŒä½
        use_lora: æ˜¯å¦ä½¿ç”¨LoRAæ¨¡å‹ï¼ˆFalse=åŸºåº§æ¨¡å‹ï¼Œæ›´è‡ªç„¶ï¼‰
    """
    # é€‰æ‹©ä½¿ç”¨çš„æ¨¡å‹
    if use_lora and models.get('qwen_lora_model'):
        qwen_model = models['qwen_lora_model']
        model_name = "LoRA"
    elif models.get('qwen_base_model'):
        qwen_model = models['qwen_base_model']
        model_name = "Base"
    else:
        return "[é”™è¯¯] Qwenæ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆå¼€åœºé—®é¢˜ã€‚è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ã€‚"
    
    if not models['qwen_tokenizer']:
        return "[é”™è¯¯] Qwen TokenizeræœªåŠ è½½ã€‚"
    
    print(f"[INFO] Using Qwen {model_name} model for initial question")
    
    try:
        # æ ¹æ®ç®€å†æŠ€èƒ½å’ŒèŒä½ç”Ÿæˆå‹å¥½çš„å¼€åœºç™½
        skills_str = 'ã€'.join(skills[:3]) if skills else 'æŠ€æœ¯'
        
        # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒæ•´æç¤ºè¯
        if use_lora:
            # LoRAæ¨¡å‹ï¼šç®€æ´æŒ‡ä»¤ï¼ˆè®­ç»ƒæ—¶çš„æ ¼å¼ï¼‰
            system_msg = f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šã€å‹å¥½çš„æŠ€æœ¯é¢è¯•å®˜ï¼Œæ­£åœ¨é¢è¯•åº”è˜{job_position}çš„å€™é€‰äººã€‚è¯·ç”¨è‡ªç„¶ã€å¤šæ ·åŒ–çš„æ–¹å¼å¼€åœºã€‚"
            prompt = f"""å€™é€‰äººç®€å†æŠ€èƒ½ï¼š{skills_str}
åº”è˜èŒä½ï¼š{job_position}

ä»»åŠ¡ï¼šç”¨å‹å¥½ã€è‡ªç„¶çš„æ–¹å¼å¼€åœºï¼Œè¯·å€™é€‰äººä»‹ç»è‡ªå·±æˆ–ç›¸å…³é¡¹ç›®ç»éªŒã€‚é£æ ¼è¦å¤šæ ·åŒ–ï¼Œå¯ä»¥ï¼š
- ç›´æ¥è¯¢é—®é¡¹ç›®ç»éªŒ
- è®©å€™é€‰äººè‡ªæˆ‘ä»‹ç»
- è¯¢é—®æœ€è¿‘çš„å·¥ä½œå†…å®¹
- è¯¢é—®æ„Ÿå…´è¶£çš„æŠ€æœ¯é¢†åŸŸ

è¯·ç”Ÿæˆä¸€ä¸ªå‹å¥½çš„å¼€åœºé—®å€™ï¼ˆ30-50å­—ï¼‰ï¼š"""
        else:
            # åŸºåº§æ¨¡å‹ï¼šæ›´è‡ªç„¶çš„å¯¹è¯å¼æŒ‡ä»¤
            system_msg = f"ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„{job_position}é¢è¯•å®˜ã€‚ä½ çš„é¢è¯•é£æ ¼ä¸“ä¸šä½†ä¸å¤±äº²å’ŒåŠ›ï¼Œå–„äºé€šè¿‡å¯¹è¯äº†è§£å€™é€‰äººçš„çœŸå®èƒ½åŠ›ã€‚"
            prompt = f"""ç°åœ¨è¦é¢è¯•ä¸€ä½åº”è˜{job_position}çš„å€™é€‰äººï¼Œä»–çš„ç®€å†ä¸Šåˆ—å‡ºäº†è¿™äº›æŠ€èƒ½ï¼š{skills_str}ã€‚

è¯·ä½œä¸ºé¢è¯•å®˜ï¼Œç”¨è‡ªç„¶ã€è½»æ¾çš„æ–¹å¼å¼€åœºï¼Œè®©å€™é€‰äººä»‹ç»è‡ªå·±æˆ–åˆ†äº«ç›¸å…³ç»éªŒã€‚æ³¨æ„ï¼š
- è¯­æ°”è‡ªç„¶ï¼ŒåƒçœŸå®çš„é¢è¯•å¯¹è¯
- å¯ä»¥é€‚å½“è¡¨è¾¾å¯¹å€™é€‰äººçš„å…´è¶£
- ä¸è¦ä½¿ç”¨è¿‡äºå®¢å¥—çš„å¯’æš„ï¼ˆå¦‚"éå¸¸è£å¹¸"ç­‰ï¼‰
- é•¿åº¦é€‚ä¸­ï¼ˆ50-80å­—ï¼‰

ç°åœ¨è¯·å¼€å§‹é¢è¯•ï¼š"""
        
        messages = [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": prompt}
        ]
        
        text = models['qwen_tokenizer'].apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = models['qwen_tokenizer'](
            text,
            return_tensors='pt',
            truncation=True,
            max_length=512
        ).to(models['device'])
        
        with torch.no_grad():
            outputs = qwen_model.generate(
                **inputs,
                max_new_tokens=100 if not use_lora else 80,  # åŸºåº§æ¨¡å‹ç”Ÿæˆæ›´é•¿
                temperature=0.7 if not use_lora else 0.9,     # åŸºåº§æ¨¡å‹é™ä½æ¸©åº¦
                top_p=0.9 if not use_lora else 0.92,
                top_k=50,
                repetition_penalty=1.15 if not use_lora else 1.2,
                do_sample=True,
                pad_token_id=models['qwen_tokenizer'].eos_token_id
            )
        
        # å®‰å…¨è§£ç ç”Ÿæˆç»“æœ
        input_length = inputs['input_ids'].shape[1]
        if outputs.shape[1] > input_length:
            generated = models['qwen_tokenizer'].decode(
                outputs[0][input_length:],
                skip_special_tokens=True
            ).strip()
        else:
            generated = models['qwen_tokenizer'].decode(
                outputs[0],
                skip_special_tokens=True
            ).strip()
        
        if not generated:
            return "[é”™è¯¯] Qwenç”Ÿæˆå¤±è´¥ï¼Œè¯·é‡è¯•ã€‚"
        return generated
    
    except Exception as e:
        print(f"[ERROR] Initial question generation failed: {e}")
        return f"[é”™è¯¯] Qwenç”Ÿæˆå¼‚å¸¸: {str(e)}"

def generate_qwen_question(models, context, question_type='follow_up', use_lora=False):
    """ä½¿ç”¨Qwenç”Ÿæˆé—®é¢˜
    
    Args:
        models: æ¨¡å‹å­—å…¸
        context: ä¸Šä¸‹æ–‡ä¿¡æ¯
        question_type: é—®é¢˜ç±»å‹ ('follow_up' æˆ– 'topic_change')
        use_lora: æ˜¯å¦ä½¿ç”¨LoRAæ¨¡å‹ï¼ˆFalse=åŸºåº§æ¨¡å‹ï¼Œæ›´è‡ªç„¶ï¼‰
    """
    # é€‰æ‹©ä½¿ç”¨çš„æ¨¡å‹
    if use_lora and models.get('qwen_lora_model'):
        qwen_model = models['qwen_lora_model']
        model_name = "LoRA"
    elif models.get('qwen_base_model'):
        qwen_model = models['qwen_base_model']
        model_name = "Base"
    else:
        return "[é”™è¯¯] Qwenæ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆé—®é¢˜ã€‚"
    
    if not models['qwen_tokenizer']:
        return "[é”™è¯¯] Qwen TokenizeræœªåŠ è½½ã€‚"
    
    print(f"[INFO] Using Qwen {model_name} model for {question_type}")
    
    try:
        # æ„å»ºprompt
        if question_type == 'follow_up':
            answer = context['last_answer']
            question = context['question']
            topic = context.get('topic', 'æŠ€æœ¯')
            score = context.get('score', 70)
            
            if use_lora:
                # LoRAæ¨¡å‹ï¼šä½¿ç”¨è®­ç»ƒæ•°æ®æ ¼å¼ï¼ˆç®€æ´ï¼‰
                if score >= 80:
                    task = "å¯¹å€™é€‰äººçš„å›ç­”ç»™äºˆè‚¯å®šå’Œé¼“åŠ±ï¼Œç„¶åç»§ç»­è¿½é—®"
                else:
                    task = f"æ ¹æ®å€™é€‰äººçš„å›ç­”ï¼Œç”Ÿæˆä¸€ä¸ªè¿½é—®é—®é¢˜ï¼Œæ·±å…¥è€ƒå¯Ÿå€™é€‰äººå¯¹{topic}çš„ç†è§£"
                
                prompt = f"""é¢è¯•å®˜é—®é¢˜ï¼š{question}
å€™é€‰äººå›ç­”ï¼š{answer}

ä»»åŠ¡ï¼š{task}"""
                system_msg = f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šã€å‹å¥½çš„{topic}é¢è¯•å®˜ï¼Œæ­£åœ¨é¢è¯•å€™é€‰äººã€‚ä½ éœ€è¦æ ¹æ®å€™é€‰äººçš„å›ç­”ï¼Œå†³å®šæ˜¯ç»§ç»­æ·±å…¥è¿½é—®ã€æ¢è¯é¢˜ï¼Œè¿˜æ˜¯ç»™äºˆé¼“åŠ±ã€‚"
            else:
                # åŸºåº§æ¨¡å‹ï¼šæ›´è‡ªç„¶çš„å¯¹è¯å¼æŒ‡ä»¤
                if score >= 80:
                    feedback_guide = "å€™é€‰äººå›ç­”å¾—ä¸é”™ï¼Œå¯ä»¥é€‚å½“è‚¯å®šï¼ˆä¸è¦è¿‡äºå®¢å¥—ï¼‰ï¼Œç„¶åè¿½é—®æ›´æ·±å…¥çš„é—®é¢˜"
                elif score >= 60:
                    feedback_guide = "å€™é€‰äººçš„å›ç­”æ¯”è¾ƒç¬¼ç»Ÿï¼Œéœ€è¦å¼•å¯¼ä»–è¯´å¾—æ›´å…·ä½“"
                else:
                    feedback_guide = "å€™é€‰äººçš„å›ç­”ä¸å¤ªç†æƒ³ï¼Œå¯ä»¥æ¢ä¸ªè§’åº¦é—®ï¼Œæˆ–è€…ç»™ä¸ªæç¤º"
                
                prompt = f"""ä½ åˆšæ‰é—®äº†å€™é€‰äººï¼š"{question}"

å€™é€‰äººçš„å›ç­”æ˜¯ï¼š"{answer}"

{feedback_guide}ã€‚è¯·ç”Ÿæˆä½ çš„ä¸‹ä¸€ä¸ªé—®é¢˜æˆ–å›å¤ã€‚æ³¨æ„ï¼š
- è¯­æ°”è‡ªç„¶ï¼Œåƒæ­£å¸¸å¯¹è¯
- ä¸è¦ä½¿ç”¨"éå¸¸æ£’ï¼"ã€"å¾ˆå¥½ï¼"è¿™ç§è¿‡äºçƒ­æƒ…çš„è¡¨æ‰¬
- å¦‚æœéœ€è¦è‚¯å®šï¼Œå¯ä»¥è¯´"å—¯ï¼Œç†è§£äº†"ã€"å¯ä»¥"ã€"å¬èµ·æ¥ä¸é”™"ç­‰
- ç›´æ¥é—®ä¸‹ä¸€ä¸ªé—®é¢˜ï¼Œä¸è¦å•°å—¦
- é•¿åº¦é€‚ä¸­ï¼ˆ40-80å­—ï¼‰

ä½ çš„å›å¤ï¼š"""
                system_msg = f"ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„{topic}æŠ€æœ¯é¢è¯•å®˜ï¼Œé¢è¯•é£æ ¼ä¸“ä¸šã€å¹³å’Œï¼Œå–„äºå¼•å¯¼å€™é€‰äººå±•ç¤ºçœŸå®æ°´å¹³ã€‚"
        
        else:
            # æ¢è¯é¢˜ï¼ˆtopic_changeï¼‰
            answer = context.get('last_answer', '')
            question = context.get('question', '')
            topic = context.get('topic', 'æŠ€æœ¯')
            score = context.get('score', 70)
            skills = context.get('skills', ['æŠ€æœ¯'])
            history = context.get('history', [])
            
            # æ‰¾å‡ºè¿˜æ²¡é—®è¿‡çš„æŠ€èƒ½
            asked_topics = set()
            for qa in history[-5:]:
                q = qa.get('question', '')
                for skill in skills:
                    if skill in q:
                        asked_topics.add(skill)
            
            remaining_skills = [s for s in skills if s not in asked_topics]
            next_skill = remaining_skills[0] if remaining_skills else skills[0]
            
            if use_lora:
                # LoRAæ¨¡å‹ï¼šä½¿ç”¨è®­ç»ƒæ•°æ®æ ¼å¼
                if score >= 80:
                    task = f"å€™é€‰äººå¯¹{topic}å›ç­”å¾—å¾ˆå¥½ï¼Œå·²ç»å……åˆ†è€ƒå¯Ÿï¼Œå¯ä»¥æ¢ä¸€ä¸ªæ–°è¯é¢˜"
                else:
                    task = f"å€™é€‰äººå¯¹{topic}ä¸äº†è§£æˆ–ç­”ä¸ä¸Šæ¥ï¼Œéœ€è¦å‹å¥½åœ°æ¢ä¸€ä¸ªè¯é¢˜"
                
                prompt = f"""é¢è¯•å®˜é—®é¢˜ï¼š{question}
å€™é€‰äººå›ç­”ï¼š{answer}

ä»»åŠ¡ï¼š{task}"""
                system_msg = f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šã€å‹å¥½çš„æŠ€æœ¯é¢è¯•å®˜ï¼Œæ­£åœ¨é¢è¯•å€™é€‰äººã€‚ä½ éœ€è¦æ ¹æ®å€™é€‰äººçš„å›ç­”ï¼Œå†³å®šæ˜¯ç»§ç»­æ·±å…¥è¿½é—®ã€æ¢è¯é¢˜ï¼Œè¿˜æ˜¯ç»™äºˆé¼“åŠ±ã€‚"
            else:
                # åŸºåº§æ¨¡å‹ï¼šæ›´è‡ªç„¶çš„æ¢è¯é¢˜æŒ‡ä»¤
                if score >= 80:
                    transition_guide = f"å€™é€‰äººå¯¹{topic}çš„ç†è§£å·²ç»è€ƒå¯Ÿå¾—å·®ä¸å¤šäº†ï¼Œå¯ä»¥è‡ªç„¶åœ°è¿‡æ¸¡åˆ°{next_skill}è¿™ä¸ªæ–°è¯é¢˜"
                else:
                    transition_guide = f"å€™é€‰äººå¯¹{topic}ä¸å¤ªç†Ÿæ‚‰ï¼Œä¸è¦ä¸ºéš¾ä»–ï¼Œè‡ªç„¶åœ°åˆ‡æ¢åˆ°{next_skill}è¯é¢˜"
                
                prompt = f"""ä½ åˆšæ‰é—®äº†ï¼š"{question}"
å€™é€‰äººå›ç­”ï¼š"{answer}"

{transition_guide}ã€‚è¯·ç”Ÿæˆä½ çš„ä¸‹ä¸€ä¸ªé—®é¢˜ã€‚æ³¨æ„ï¼š
- ä¸è¦ä½¿ç”¨"æ²¡å…³ç³»"ã€"æ²¡é—®é¢˜"è¿™ç§è¿‡äºå®‰æ…°çš„è¯
- å¦‚æœéœ€è¦è¿‡æ¸¡ï¼Œå¯ä»¥ç®€å•åœ°è¯´"å¥½çš„"ã€"é‚£æˆ‘ä»¬èŠèŠ..."ã€"æ¢ä¸ªæ–¹å‘"ç­‰
- ç›´æ¥å¼•å…¥æ–°è¯é¢˜ï¼Œä¸è¦å•°å—¦
- é•¿åº¦é€‚ä¸­ï¼ˆ30-60å­—ï¼‰

ä½ çš„å›å¤ï¼š"""
                system_msg = "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„æŠ€æœ¯é¢è¯•å®˜ï¼Œé¢è¯•é£æ ¼ä¸“ä¸šã€å¹³å’Œï¼Œå–„äºè‡ªç„¶åœ°è½¬æ¢è¯é¢˜ã€‚"
        
        messages = [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": prompt}
        ]
        
        text = models['qwen_tokenizer'].apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = models['qwen_tokenizer'](
            [text],
            return_tensors="pt",
            padding=True
        ).to(models['device'])
        
        with torch.no_grad():
            outputs = qwen_model.generate(
                **inputs,
                max_new_tokens=100 if not use_lora else 60,  # åŸºåº§æ¨¡å‹ç”Ÿæˆæ›´é•¿
                temperature=0.7 if not use_lora else 0.8,     # åŸºåº§æ¨¡å‹é™ä½æ¸©åº¦
                top_p=0.9 if not use_lora else 0.9,
                top_k=40,
                do_sample=True,
                repetition_penalty=1.1 if not use_lora else 1.15,
                pad_token_id=models['qwen_tokenizer'].pad_token_id
            )
        
        # å®‰å…¨è§£ç ç”Ÿæˆç»“æœ
        input_length = len(inputs.input_ids[0])
        if outputs.shape[1] > input_length:
            response = models['qwen_tokenizer'].decode(
                outputs[0][input_length:],
                skip_special_tokens=True
            ).strip()
        else:
            response = models['qwen_tokenizer'].decode(
                outputs[0],
                skip_special_tokens=True
            ).strip()
        
        # æ¸…ç†è¾“å‡º
        response = response.split('\n')[0][:100]
        
        if not response:
            return "[é”™è¯¯] Qwenç”Ÿæˆä¸ºç©ºï¼Œè¯·é‡è¯•ã€‚"
        return response
        
    except Exception as e:
        print(f"[ERROR] Qwen generation failed: {str(e)}")
        return f"[é”™è¯¯] Qwenç”Ÿæˆå¼‚å¸¸: {str(e)}"

def evaluate_answer(models, question, answer, history_qa):
    """RoBERTaè¯„ä¼°å›ç­”"""
    input_parts = []
    if history_qa:
        input_parts.append("[å†å²é—®ç­”]")
        for i, h in enumerate(history_qa[-3:], 1):
            input_parts.append(f"Q{i}: {h['question']}")
            input_parts.append(f"A{i}: {h['answer'][:100]}")
            input_parts.append(f"è´¨é‡: {h['quality']}")
    
    input_parts.append("[å½“å‰é—®ç­”]")
    input_parts.append(f"é—®é¢˜: {question}")
    input_parts.append(f"å›ç­”: {answer}")
    input_parts.append(f"æµç•…åº¦: 0.85")
    
    input_text = "\n".join(input_parts)
    
    inputs = models['roberta_tokenizer'](
        input_text,
        return_tensors='pt',
        truncation=True,
        max_length=256,
        padding=True
    ).to(models['device'])
    
    with torch.no_grad():
        cls_logits, reg_score = models['roberta_model'](
            inputs['input_ids'],
            inputs['attention_mask']
        )
        
        cls_probs = torch.softmax(cls_logits, dim=-1)
        predicted_idx = cls_probs.argmax().item()
        confidence = cls_probs.max().item()
        overall_score = reg_score.item() * 100
    
    label_names = ["å·®", "ä¸€èˆ¬", "è‰¯å¥½", "ä¼˜ç§€"]
    score_mapping = [50, 70, 85, 95]
    
    return {
        'current_label': label_names[predicted_idx],
        'current_score': score_mapping[predicted_idx],
        'overall_score': overall_score,
        'confidence': confidence
    }

def decide_next_action(models, question, answer, follow_up_depth, topic):
    """BERTå†³ç­–ä¸‹ä¸€æ­¥ï¼ˆä¸¥æ ¼æŒ‰ç…§è®­ç»ƒæ ¼å¼ï¼‰"""
    answer_length = len(answer)
    
    # è®¡ç®—çŠ¹è±«åº¦ï¼ˆåŸºäºè¯­æ°”è¯å’Œåœé¡¿ï¼‰
    hesitation_words = ['å—¯', 'å•Š', 'è¿™ä¸ª', 'é‚£ä¸ª', 'å°±æ˜¯', 'æ€ä¹ˆè¯´', '...']
    hesitation_count = sum(answer.count(word) for word in hesitation_words)
    hesitation_score = min(0.9, hesitation_count * 0.15)
    
    # å’Œè®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´çš„æ ¼å¼
    features = f"è¿½é—®æ·±åº¦:{follow_up_depth} " \
              f"çŠ¹è±«åº¦:{hesitation_score:.2f} " \
              f"é•¿åº¦:{answer_length}å­— " \
              f"è¯é¢˜:{topic}"
    
    bert_input = f"{question}[SEP]{answer}[SEP]{features}"
    
    inputs = models['bert_tokenizer'](
        bert_input,
        return_tensors='pt',
        truncation=True,
        max_length=512,
        padding=True
    ).to(models['device'])
    
    with torch.no_grad():
        outputs = models['bert_model'](**inputs)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=-1)
        predicted = probs.argmax().item()
        conf = probs.max().item()
    
    bert_labels = ["FOLLOW_UP", "NEXT_TOPIC"]
    
    # æ¨æ–­å†³ç­–ç†ç”±ï¼ˆåŸºäºè®­ç»ƒæ•°æ®ä¸­çš„reasonæ¨¡å¼ï¼‰
    reason = _infer_decision_reason(answer, answer_length, hesitation_score, 
                                     follow_up_depth, bert_labels[predicted], topic)
    
    return {
        'action': bert_labels[predicted],
        'confidence': conf,
        'probs': probs[0].tolist(),
        'reason': reason,
        'hesitation_score': hesitation_score
    }

def _infer_decision_reason(answer, answer_length, hesitation_score, follow_up_depth, action, topic):
    """æ¨æ–­å†³ç­–ç†ç”±ï¼ˆåŸºäºè®­ç»ƒæ•°æ®ä¸­çš„reasonæ¨¡å¼ï¼‰"""
    
    if action == "NEXT_TOPIC":
        # NEXT_TOPICçš„å¸¸è§åŸå› 
        if any(word in answer for word in ['ä¸äº†è§£', 'ä¸æ‡‚', 'æ²¡å­¦è¿‡', 'æ²¡æ¥è§¦']):
            return f"å€™é€‰è€…æ˜ç¡®è¡¨ç¤ºä¸äº†è§£{topic}ï¼Œåº”æ¢å…¶ä»–è¯é¢˜"
        elif follow_up_depth >= 2 and hesitation_score > 0.4:
            return f"ç»è¿‡{follow_up_depth}è½®è¿½é—®ï¼Œå€™é€‰è€…å¯¹{topic}çš„ç†è§£ä»ç„¶æ¨¡ç³Š/ä¸è¶³ï¼Œå»ºè®®æ¢è¯é¢˜"
        elif answer_length < 20:
            return f"å€™é€‰è€…å›ç­”è¿‡äºç®€çŸ­ä¸”ç¼ºä¹å®è´¨å†…å®¹ï¼Œå»ºè®®æ¢å…¶ä»–è¯é¢˜"
        else:
            return "æ ¹æ®ç»¼åˆåˆ†æï¼Œå»ºè®®æ¢è¯é¢˜è€ƒå¯Ÿå…¶ä»–æŠ€èƒ½ç‚¹"
    
    else:  # FOLLOW_UP
        # FOLLOW_UPçš„å¸¸è§åŸå› 
        if any(word in answer for word in ['ç”¨è¿‡', 'åšè¿‡', 'é¡¹ç›®']) and answer_length < 80:
            return f"å€™é€‰è€…æåˆ°äº†ä½¿ç”¨åœºæ™¯ä½†ç¼ºå°‘ç»†èŠ‚ï¼Œå¯ä»¥ç»§ç»­è¿½é—®æ›´æ·±å…¥çš„æŠ€æœ¯ç»†èŠ‚"
        elif hesitation_score > 0.3 and any(word in answer for word in ['æ¥è§¦è¿‡', 'äº†è§£', 'ç”¨åˆ°']):
            return f"å€™é€‰è€…æ‰¿è®¤ä½¿ç”¨è¿‡{topic}ä½†æœªå±•å¼€ç»†èŠ‚ï¼ˆæåˆ°äº†'æ‰¿è®¤ç”¨è¿‡ä½†è¯´ä¸æ¸…ç»†èŠ‚'ï¼‰ï¼Œéœ€è¿½é—®å…·ä½“å®ç°"
        elif answer_length >= 50:
            return "å€™é€‰è€…ç»™å‡ºäº†ä¸€å®šå†…å®¹ï¼Œå¯ä»¥é’ˆå¯¹å›ç­”ä¸­çš„å…³é”®ç‚¹ç»§ç»­æ·±å…¥è¿½é—®"
        else:
            return "å¯ä»¥ç»§ç»­è¿½é—®ä»¥æ›´å…¨é¢è¯„ä¼°å€™é€‰è€…çš„æŠ€æœ¯èƒ½åŠ›"

# ==================== ä¸»ç•Œé¢ ====================
st.title("ğŸ¯ AI Interviewer")

# åˆå§‹åŒ–
if 'stage' not in st.session_state:
    st.session_state.stage = 'upload'
if 'resume_data' not in st.session_state:
    st.session_state.resume_data = None
if 'current_question' not in st.session_state:
    st.session_state.current_question = None
if 'qa_history' not in st.session_state:
    st.session_state.qa_history = []
if 'follow_up_depth' not in st.session_state:
    st.session_state.follow_up_depth = 0
if 'current_topic' not in st.session_state:
    st.session_state.current_topic = 'Python'
if 'total_rounds' not in st.session_state:
    st.session_state.total_rounds = 0
if 'job_position' not in st.session_state:
    st.session_state.job_position = 'Pythonåç«¯å·¥ç¨‹å¸ˆ'
if 'digital_human' not in st.session_state:
    st.session_state.digital_human = DigitalHuman()
if 'linly_client' not in st.session_state:
    st.session_state.linly_client = LinlyTalkerClient()
if 'digital_human_mode' not in st.session_state:
    # é»˜è®¤ä½¿ç”¨è½»é‡çº§æ¨¡å¼ï¼Œå¦‚æœLinlyæœåŠ¡å¯ç”¨åˆ™å¯ä»¥åˆ‡æ¢
    st.session_state.digital_human_mode = 'lightweight'
if 'avatar_image_path' not in st.session_state:
    st.session_state.avatar_image_path = "Linly-Talker/examples/source_image/full_body_1.png"

# ==================== é˜¶æ®µ1: ä¸Šä¼ ç®€å† ====================
if st.session_state.stage == 'upload':
    st.markdown("---")
    st.subheader("ğŸ“„ æ­¥éª¤1ï¼šä¸Šä¼ ç®€å†")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        upload_method = st.radio(
            "é€‰æ‹©ä¸Šä¼ æ–¹å¼",
            ["ä¸Šä¼ æ–‡ä»¶ (PDF/DOCX)", "ç›´æ¥ç²˜è´´æ–‡æœ¬"],
            horizontal=True
        )
        
        resume_text = ""
        
        if upload_method == "ä¸Šä¼ æ–‡ä»¶ (PDF/DOCX)":
            uploaded_file = st.file_uploader(
                "ä¸Šä¼ ç®€å†æ–‡ä»¶",
                type=['pdf', 'docx'],
                help="æ”¯æŒPDFå’ŒDOCXæ ¼å¼"
            )
            
            if uploaded_file:
                with tempfile.NamedTemporaryFile(delete=False, suffix=Path(uploaded_file.name).suffix) as tmp_file:
                    tmp_file.write(uploaded_file.getbuffer())
                    tmp_path = tmp_file.name
                
                try:
                    with st.spinner("æ­£åœ¨è§£æç®€å†..."):
                        parser = ResumeParser()
                        resume_data = parser.parse(tmp_path)
                        st.session_state.resume_data = resume_data
                    
                    st.success(f"âœ… ç®€å†è§£ææˆåŠŸï¼")
                    
                    with st.expander("ğŸ“‹ æŸ¥çœ‹è§£æç»“æœ"):
                        st.markdown(f"**å§“åï¼š** {resume_data['name']}")
                        if resume_data['contact']:
                            st.markdown(f"**è”ç³»æ–¹å¼ï¼š** {resume_data['contact']}")
                        st.markdown(f"**æå–åˆ°çš„æŠ€èƒ½ï¼š** {', '.join(resume_data['skills'][:10])}")
                    
                    resume_text = resume_data['raw_text']
                    
                except Exception as e:
                    st.error(f"ç®€å†è§£æå¤±è´¥ï¼š{str(e)}")
                finally:
                    Path(tmp_path).unlink(missing_ok=True)
        else:
            resume_text = st.text_area(
                "ç²˜è´´ç®€å†å†…å®¹",
                height=300,
                placeholder="å§“åï¼šå¼ ä¸‰\nåº”è˜èŒä½ï¼šPythonåç«¯å·¥ç¨‹å¸ˆ\n\næŠ€èƒ½ï¼šPython, Django, MySQL..."
            )
        
        job_position = st.text_input("åº”è˜èŒä½", value="Pythonåç«¯å·¥ç¨‹å¸ˆ")
    
    with col2:
        st.info("ğŸ’¡ **æ™ºèƒ½ç‰¹æ€§**\n\nâœ… PDF/DOCXè§£æ\n\nâœ… BERTæ™ºèƒ½å†³ç­–\n\nâœ… **Qwen LoRAè¿½é—®**\n\nâœ… RoBERTaè¯„ä¼°\n\nâœ… **æ— å›ºå®šè½®æ•°**")
        
        if st.session_state.resume_data:
            st.success(f"**å·²è¯†åˆ«æŠ€èƒ½ï¼š**\n\n" + "\n".join([f"â€¢ {s}" for s in st.session_state.resume_data['skills'][:5]]))
    
    if st.button("ğŸš€ å¼€å§‹é¢è¯•", type="primary", use_container_width=True):
        if resume_text.strip() or st.session_state.resume_data:
            if not st.session_state.resume_data and resume_text:
                parser = ResumeParser()
                st.session_state.resume_data = {
                    'name': 'å€™é€‰äºº',
                    'skills': [s for s in ['Python', 'Java', 'JavaScript'] if s.lower() in resume_text.lower()],
                    'raw_text': resume_text
                }
            
            # æ ‡è®°ä¸ºè¿›å…¥é¢è¯•é˜¶æ®µï¼Œç¬¬ä¸€ä¸ªé—®é¢˜å°†åœ¨é¢è¯•é˜¶æ®µåŠ¨æ€ç”Ÿæˆ
            skills = st.session_state.resume_data['skills'] or ['æŠ€æœ¯']
            first_skill = skills[0] if skills else 'æŠ€æœ¯'
            st.session_state.current_topic = first_skill
            st.session_state.current_question = None  # æ ‡è®°ä¸ºéœ€è¦ç”Ÿæˆ
            st.session_state.job_position = job_position
            st.session_state.stage = 'interview'
            st.rerun()
        else:
            st.error("è¯·å…ˆä¸Šä¼ ç®€å†æˆ–ç²˜è´´æ–‡æœ¬")

# ==================== é˜¶æ®µ2: é¢è¯•è¿‡ç¨‹ ====================
elif st.session_state.stage == 'interview':
    with st.spinner("æ­£åœ¨åŠ è½½AIæ¨¡å‹..."):
        models = load_models()
    
    # åœ¨ä¾§è¾¹æ æ˜¾ç¤ºæ¨¡å‹åŠ è½½çŠ¶æ€å’Œé…ç½®ï¼ˆç¼“å­˜å¤–ï¼‰
    with st.sidebar:
        st.markdown("### ğŸ” æ¨¡å‹çŠ¶æ€")
        if models['qwen_base_model'] is not None:
            st.success("âœ… Qwen Baseæ¨¡å‹å·²åŠ è½½")
        if models['qwen_lora_model'] is not None:
            st.success("âœ… Qwen LoRAæ¨¡å‹å·²åŠ è½½")
        if models['qwen_base_model'] is None:
            st.error("âŒ QwenæœªåŠ è½½ - æ— æ³•ç»§ç»­é¢è¯•")
        st.success("âœ… BERTå†³ç­–æ¨¡å‹")
        st.success("âœ… RoBERTaè¯„ä¼°æ¨¡å‹")
        st.markdown("---")
        
        # æ¨¡å‹é€‰æ‹©
        st.markdown("### âš™ï¸ é¢è¯•é£æ ¼")
        if 'use_lora' not in st.session_state:
            st.session_state.use_lora = False  # é»˜è®¤ä½¿ç”¨åŸºåº§æ¨¡å‹
        
        use_lora = st.checkbox(
            "ä½¿ç”¨LoRAæ¨¡å‹ï¼ˆç®€æ´é£æ ¼ï¼‰",
            value=st.session_state.use_lora,
            help="âœ… LoRAï¼šç®€æ´ã€ç›´æ¥ï¼ˆçº¦20å­—ï¼‰\nâŒ åŸºåº§ï¼šè‡ªç„¶ã€è¯¦ç»†ï¼ˆçº¦60å­—ï¼‰",
            disabled=(models['qwen_lora_model'] is None)
        )
        st.session_state.use_lora = use_lora
        
        if st.session_state.use_lora:
            st.info("ğŸ”¹ å½“å‰ï¼šLoRAæ¨¡å‹ï¼ˆç®€æ´é£æ ¼ï¼‰")
        else:
            st.info("ğŸ”¹ å½“å‰ï¼šåŸºåº§æ¨¡å‹ï¼ˆè‡ªç„¶å¯¹è¯é£æ ¼ï¼‰")
        st.markdown("---")
    
    # ç”Ÿæˆç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
    if st.session_state.current_question is None:
        with st.spinner("Qwenæ­£åœ¨ç”Ÿæˆå¼€åœºé—®é¢˜..."):
            skills = st.session_state.resume_data.get('skills', ['æŠ€æœ¯'])
            job_position = st.session_state.get('job_position', 'Pythonåç«¯å·¥ç¨‹å¸ˆ')
            st.session_state.current_question = generate_initial_question(models, skills, job_position, use_lora=st.session_state.use_lora)
    
    # é¡¶éƒ¨ä¿¡æ¯
    if st.session_state.resume_data:
        st.caption(f"ğŸ‘¤ {st.session_state.resume_data.get('name', 'å€™é€‰äºº')} | åº”è˜ï¼š{st.session_state.job_position} | å·²å®Œæˆï¼š{st.session_state.total_rounds}è½®")
    
    # ä¸»å¸ƒå±€ï¼šå·¦ä¾§(æ•°å­—äºº+å¯¹è¯) å³ä¾§(è¯„åˆ†)
    col_main, col_score = st.columns([2.5, 1])
    
    with col_main:
        # ========== æ•°å­—äººé¢è¯•å®˜ ==========
        if st.session_state.digital_human_mode == 'linly':
            # ä½¿ç”¨ Linly-Talker ç”Ÿæˆè§†é¢‘
            st.markdown('<div style="text-align: center; margin-bottom: 1rem;">', unsafe_allow_html=True)
            
            # ç”Ÿæˆè§†é¢‘ï¼ˆå¸¦ç¼“å­˜keyé¿å…é‡å¤ç”Ÿæˆï¼‰
            video_cache_key = f"video_{hash(st.session_state.current_question)}"
            if video_cache_key not in st.session_state:
                with st.spinner("ğŸ¬ æ•°å­—äººæ­£åœ¨ç”Ÿæˆè§†é¢‘..."):
                    try:
                        video_path = st.session_state.linly_client.generate_video(
                            text=st.session_state.current_question,
                            avatar_image=st.session_state.avatar_image_path
                        )
                        st.session_state[video_cache_key] = video_path
                    except Exception as e:
                        st.error(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")
                        st.info("å·²è‡ªåŠ¨åˆ‡æ¢åˆ°è½»é‡çº§æ¨¡å¼")
                        st.session_state.digital_human_mode = 'lightweight'
                        video_path = None
            else:
                video_path = st.session_state[video_cache_key]
            
            if video_path and Path(video_path).exists():
                st.markdown(f"""
                <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                            border-radius: 20px; padding: 1.5rem; margin-bottom: 1rem;">
                    <div style="color: white; font-size: 1.2rem; font-weight: 600; margin-bottom: 1rem; text-align: center;">
                        Alice Â· {st.session_state.current_topic}
                    </div>
                </div>
                """, unsafe_allow_html=True)
                st.video(video_path, autoplay=True)
            else:
                # é™çº§åˆ°è½»é‡çº§æ¨¡å¼
                avatar_html = st.session_state.digital_human.get_avatar_html(
                    question=st.session_state.current_question,
                    topic=st.session_state.current_topic,
                    include_audio=True
                )
            st.markdown('</div>', unsafe_allow_html=True)
        else:
            # è™šæ‹Ÿå½¢è±¡æ¨¡å¼ï¼šå¸¦å£°çº¹æ•ˆæœçš„åŠ¨ç”»å½¢è±¡
            # æ˜¾ç¤ºè™šæ‹Ÿå½¢è±¡
            st.markdown(f"""
            <div class="avatar-container">
                <div class="avatar-wrapper">
                    <div class="virtual-avatar">
                        ğŸ¤–
                        <div class="status-indicator"></div>
                    </div>
                    <div class="sound-wave">
                        <div class="sound-bar"></div>
                        <div class="sound-bar"></div>
                        <div class="sound-bar"></div>
                        <div class="sound-bar"></div>
                        <div class="sound-bar"></div>
                        <div class="sound-bar"></div>
                        <div class="sound-bar"></div>
                        <div class="sound-bar"></div>
                    </div>
                </div>
                <div style="margin-top: 2rem; color: white;">
                    <h3 style="margin: 0;">Alice</h3>
                    <p style="margin: 0.5rem 0; opacity: 0.9;">è¯é¢˜ï¼š{st.session_state.current_topic}</p>
                </div>
            </div>
            
            <div class="avatar-speech">
                <div style="font-size: 0.9rem; color: #999; margin-bottom: 0.5rem;">ğŸ’¬ é¢è¯•å®˜æé—®ï¼š</div>
                <div style="font-size: 1.1rem; line-height: 1.6;">{st.session_state.current_question}</div>
            </div>
            """, unsafe_allow_html=True)
            
            # ç”Ÿæˆå¹¶è‡ªåŠ¨æ’­æ”¾è¯­éŸ³
            try:
                audio_file = st.session_state.digital_human.text_to_speech(st.session_state.current_question)
                if audio_file and Path(audio_file).exists():
                    import base64
                    with open(audio_file, 'rb') as f:
                        audio_bytes = f.read()
                    audio_base64 = base64.b64encode(audio_bytes).decode()
                    
                    # ä½¿ç”¨é—®é¢˜å†…å®¹çš„hashä½œä¸ºå”¯ä¸€IDï¼ˆç¡®ä¿ä¸åŒé—®é¢˜æœ‰ä¸åŒIDï¼‰
                    import hashlib
                    question_hash = hashlib.md5(st.session_state.current_question.encode()).hexdigest()[:8]
                    audio_id = f"tts_{question_hash}"
                    
                    # åªåœ¨session_stateä¸­æ²¡æœ‰è®°å½•è¿™ä¸ªéŸ³é¢‘IDæ—¶æ‰æ’­æ”¾ï¼ˆé¿å…é‡å¤ï¼‰
                    if 'last_audio_id' not in st.session_state or st.session_state.last_audio_id != audio_id:
                        st.session_state.last_audio_id = audio_id
                        
                        # ä½¿ç”¨HTML5éŸ³é¢‘è‡ªåŠ¨æ’­æ”¾
                        st.components.v1.html(f"""
                        <audio id="{audio_id}" autoplay style="display:none;">
                            <source src="data:audio/mp3;base64,{audio_base64}" type="audio/mp3">
                        </audio>
                        """, height=0)
            except Exception as e:
                print(f"[WARNING] Audio generation failed: {e}")
        
        # ========== å¯¹è¯å†å² ==========
        st.markdown("### ğŸ’¬ å¯¹è¯è®°å½•")
        
        # èŠå¤©å¼å†å²è®°å½•ï¼ˆåªæ˜¾ç¤ºæœ€è¿‘5è½®ï¼‰
        with st.container():
            st.markdown('<div style="max-height: 350px; overflow-y: auto; padding: 1rem; background: #f8f9fa; border-radius: 10px; margin-bottom: 1rem;">', unsafe_allow_html=True)
            
            if not st.session_state.qa_history:
                st.markdown('<div style="text-align: center; color: #999; padding: 2rem;">æš‚æ— å¯¹è¯è®°å½•</div>', unsafe_allow_html=True)
            else:
                for i, qa in enumerate(st.session_state.qa_history[-5:]):
                    quality_emoji = {'å·®': 'ğŸ”´', 'ä¸€èˆ¬': 'ğŸŸ¡', 'è‰¯å¥½': 'ğŸ”µ', 'ä¼˜ç§€': 'ğŸŸ¢'}.get(qa.get('quality', 'ä¸€èˆ¬'), 'âšª')
                    
                    # AIæ¶ˆæ¯
                    st.markdown(f'''
                    <div class="chat-message ai">
                        <div class="message-bubble">
                            <div style="font-size: 0.85rem; opacity: 0.9;">ğŸ¤– é¢è¯•å®˜</div>
                            <div style="margin-top: 0.3rem;">{qa["question"]}</div>
                        </div>
                    </div>
                    ''', unsafe_allow_html=True)
                    
                    # ç”¨æˆ·æ¶ˆæ¯
                    st.markdown(f'''
                    <div class="chat-message user">
                        <div class="message-bubble">
                            <div style="font-size: 0.85rem; opacity: 0.7;">ğŸ‘¤ å€™é€‰äºº</div>
                            <div style="margin-top: 0.3rem;">{qa["answer"]}</div>
                            <div style="font-size: 0.75rem; margin-top: 0.5rem; opacity: 0.8;">{quality_emoji} {qa.get('quality', 'ä¸€èˆ¬')}</div>
                        </div>
                    </div>
                    ''', unsafe_allow_html=True)
            
            st.markdown('</div>', unsafe_allow_html=True)
        
        # ========== å›ç­”è¾“å…¥ ==========
        st.markdown("### âœï¸ æ‚¨çš„å›ç­”")
        answer = st.text_area(
            "è¯·è¾“å…¥æ‚¨çš„å›ç­”",
            height=120,
            key=f"answer_{st.session_state.total_rounds}",
            placeholder="åœ¨æ­¤è¾“å…¥æ‚¨çš„å›ç­”..."
        )
        
        col_s, col_e = st.columns([3, 1])
        
        with col_s:
                if st.button("âœ… æäº¤å›ç­”", type="primary", use_container_width=True):
                    if answer.strip():
                        # è¯„ä¼°
                        with st.spinner("æ­£åœ¨è¯„ä¼°å›ç­”..."):
                            eval_result = evaluate_answer(models, st.session_state.current_question, answer, st.session_state.qa_history)
                        
                        # æ˜¾ç¤ºè¯„ä¼°è¯¦æƒ…
                        st.sidebar.markdown("### ğŸ“Š æœ¬è½®è¯„ä¼°")
                        st.sidebar.write(f"è´¨é‡: {eval_result['current_label']}")
                        st.sidebar.write(f"å½“å‰åˆ†: {eval_result['current_score']}")
                        st.sidebar.write(f"æ•´ä½“åˆ†: {eval_result['overall_score']:.1f}")
                        st.sidebar.write(f"ç½®ä¿¡åº¦: {eval_result['confidence']:.1%}")
                        
                        # è®°å½•
                        st.session_state.qa_history.append({
                            'question': st.session_state.current_question,
                            'answer': answer,
                            'quality': eval_result['current_label'],
                            'eval': eval_result
                        })
                        
                        st.session_state.total_rounds += 1
                        
                        # BERTå†³ç­–
                        with st.spinner("BERTæ­£åœ¨å†³ç­–..."):
                            decision = decide_next_action(
                                models,
                                st.session_state.current_question,
                                answer,
                                st.session_state.follow_up_depth,
                                st.session_state.current_topic
                            )
                        
                        # æ˜¾ç¤ºå†³ç­–è¯¦æƒ…
                        st.sidebar.markdown("### ğŸ§  BERTå†³ç­–")
                        st.sidebar.write(f"**åŠ¨ä½œ**: {decision['action']}")
                        st.sidebar.write(f"**ç½®ä¿¡åº¦**: {decision['confidence']:.1%}")
                        st.sidebar.write(f"**ç†ç”±**: {decision['reason']}")
                        st.sidebar.caption(f"çŠ¹è±«åº¦: {decision['hesitation_score']:.2f} | FOLLOW_UPæ¦‚ç‡: {decision['probs'][0]:.1%}")
                        
                        # ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜
                        if decision['action'] == 'FOLLOW_UP':
                            st.session_state.follow_up_depth += 1
                            # ä½¿ç”¨Qwenç”Ÿæˆè¿½é—®
                            st.sidebar.markdown("### ğŸ¤– Qwenç”Ÿæˆè¿½é—®")
                            with st.spinner("Qwenæ­£åœ¨ç”Ÿæˆè¿½é—®..."):
                                context = {
                                    'last_answer': answer,
                                    'question': st.session_state.current_question,
                                    'topic': st.session_state.current_topic,
                                    'score': eval_result['current_score']
                                }
                                new_q = generate_qwen_question(models, context, 'follow_up', use_lora=st.session_state.use_lora)
                                st.session_state.current_question = new_q
                                st.sidebar.write(f"**ç±»å‹**: è¿½é—®")
                                st.sidebar.write(f"**é—®é¢˜**: {new_q}")
                                if '[é”™è¯¯]' not in new_q:
                                    with st.sidebar.expander("æŸ¥çœ‹æç¤ºè¯"):
                                        st.code(f"é—®é¢˜: {context['question']}\nå›ç­”: {context['last_answer'][:50]}...", language="text")
                        else:
                            st.session_state.follow_up_depth = 0
                            # ä½¿ç”¨Qwenç”Ÿæˆæ–°è¯é¢˜é—®é¢˜
                            st.sidebar.markdown("### ğŸ¤– Qwenç”Ÿæˆæ–°é¢˜")
                            with st.spinner("Qwenæ­£åœ¨ç”Ÿæˆæ–°é—®é¢˜..."):
                                context = {
                                    'last_answer': answer,
                                    'question': st.session_state.current_question,
                                    'topic': st.session_state.current_topic,
                                    'score': eval_result['current_score'],
                                    'skills': st.session_state.resume_data.get('skills', []),
                                    'history': st.session_state.qa_history
                                }
                                new_q = generate_qwen_question(models, context, 'new_topic', use_lora=st.session_state.use_lora)
                                st.session_state.current_question = new_q
                                
                                # æ›´æ–°å½“å‰è¯é¢˜
                                skills = context.get('skills', [])
                                for skill in skills:
                                    if skill in new_q:
                                        st.session_state.current_topic = skill
                                        break
                                
                                st.sidebar.write(f"**ç±»å‹**: æ–°è¯é¢˜")
                                st.sidebar.write(f"**é—®é¢˜**: {new_q}")
                                if '[é”™è¯¯]' not in new_q:
                                    with st.sidebar.expander("æŸ¥çœ‹æç¤ºè¯"):
                                        st.code(f"æŠ€èƒ½: {', '.join(context.get('skills', []))}\nå·²é—®è½®æ•°: {len(context.get('history', []))}", language="text")
                        
                        st.rerun()
                    else:
                        st.error("è¯·è¾“å…¥å›ç­”")
            
        with col_e:
            if st.button("ğŸ ç»“æŸé¢è¯•", use_container_width=True):
                if st.session_state.qa_history:
                    st.session_state.stage = 'summary'
                    st.rerun()
    
    with col_score:
        # ========== è¯„åˆ†é¢æ¿ ==========
        st.markdown("### ğŸ“Š å®æ—¶è¯„åˆ†")
        
        if st.session_state.qa_history:
            latest = st.session_state.qa_history[-1]['eval']
            
            st.markdown(f"""
            <div class="score-card">
                <div style="color: #999; font-size: 0.9rem;">å½“å‰å›ç­”</div>
                <div style="font-size: 2rem; font-weight: bold; color: #667eea;">{latest['current_label']}</div>
                <div style="color: #999; font-size: 0.85rem;">{latest['current_score']}åˆ†</div>
            </div>
            
            <div class="score-card">
                <div style="color: #999; font-size: 0.9rem;">æ•´ä½“è¡¨ç°</div>
                <div style="font-size: 2rem; font-weight: bold; color: #667eea;">{latest['overall_score']:.1f}</div>
                <div style="color: #999; font-size: 0.85rem;">æ»¡åˆ†100</div>
            </div>
            
            <div class="score-card">
                <div style="color: #999; font-size: 0.9rem;">å·²å®Œæˆè½®æ•°</div>
                <div style="font-size: 2rem; font-weight: bold; color: #667eea;">{st.session_state.total_rounds}</div>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.info("å›ç­”é—®é¢˜åå°†æ˜¾ç¤ºè¯„åˆ†")
        
        st.markdown("---")
        
        # AIæ¨¡å‹çŠ¶æ€
        with st.expander("ğŸ” AIå†³ç­–è¯¦æƒ…", expanded=False):
            st.caption("æŸ¥çœ‹BERTã€RoBERTaè¯¦ç»†åˆ†æ")
            if st.session_state.qa_history:
                last_qa = st.session_state.qa_history[-1]
                st.json(last_qa.get('eval', {}))

# ==================== é˜¶æ®µ3: æ€»ç»“ ====================
elif st.session_state.stage == 'summary':
    st.markdown("---")
    st.subheader("ğŸ“ˆ é¢è¯•æ€»ç»“æŠ¥å‘Š")
    
    if st.session_state.qa_history:
        final_eval = st.session_state.qa_history[-1]['eval']
        final_score = final_eval['overall_score']
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("æ€»ä½“è¯„åˆ†", f"{final_score:.1f}/100")
        with col2:
            st.metric("æ€»è½®æ•°", st.session_state.total_rounds)
        with col3:
            avg_current = sum(qa['eval']['current_score'] for qa in st.session_state.qa_history) / len(st.session_state.qa_history)
            st.metric("å¹³å‡åˆ†", f"{avg_current:.0f}")
        with col4:
            excellent = sum(1 for qa in st.session_state.qa_history if qa['quality'] == 'ä¼˜ç§€')
            st.metric("ä¼˜ç§€", f"{excellent}ä¸ª")
        
        st.markdown("---")
        if final_score >= 85:
            st.success(f"### ğŸŒŸ å¼ºçƒˆæ¨è\nå€™é€‰äººè¡¨ç°ä¼˜ç§€ï¼ŒæŠ€æœ¯åŠŸåº•æ‰å®ã€‚")
        elif final_score >= 70:
            st.success(f"### ğŸ‘ æ¨è\nå€™é€‰äººå…·å¤‡ç›¸åº”æŠ€èƒ½ï¼Œè¡¨ç°è‰¯å¥½ã€‚")
        elif final_score >= 50:
            st.warning(f"### ğŸ¤” å¾…å®š\nå€™é€‰äººåŸºç¡€ä¸€èˆ¬ï¼Œéœ€è¿›ä¸€æ­¥è€ƒå¯Ÿã€‚")
        else:
            st.error(f"### âŒ ä¸æ¨è\nå€™é€‰äººæŠ€æœ¯èƒ½åŠ›ä¸è¶³ã€‚")
        
        st.markdown("---")
        st.markdown("### ğŸ“ å¯¹è¯è¯¦æƒ…")
        
        for i, qa in enumerate(st.session_state.qa_history, 1):
            with st.expander(f"ç¬¬{i}è½® - {qa['quality']} ({qa['eval']['current_score']}åˆ†)"):
                st.markdown(f"**Q:** {qa['question']}")
                st.markdown(f"**A:** {qa['answer']}")
                st.markdown(f"**è¯„åˆ†:** {qa['eval']['current_label']} ({qa['eval']['current_score']}åˆ†) | æ•´ä½“: {qa['eval']['overall_score']:.1f}/100")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ”„ é‡æ–°å¼€å§‹", type="primary", use_container_width=True):
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.rerun()
    
    with col2:
        if st.button("ğŸ“¥ å¯¼å‡ºæŠ¥å‘Š", use_container_width=True):
            report = {
                'time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'candidate': st.session_state.resume_data.get('name', 'æœªçŸ¥'),
                'final_score': final_score,
                'total_rounds': st.session_state.total_rounds,
                'qa_history': st.session_state.qa_history
            }
            
            st.download_button(
                "ğŸ’¾ ä¸‹è½½æŠ¥å‘Š",
                data=json.dumps(report, ensure_ascii=False, indent=2),
                file_name=f"interview_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json",
                use_container_width=True
            )