"""
æµ‹è¯•Pipeline - å¤„ç†testvæ–‡ä»¶å¤¹ä¸­çš„è§†é¢‘
"""
import cv2
import numpy as np
from deepface import DeepFace
import mediapipe as mp
from transformers import pipeline
import pandas as pd
import os
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

class TestPipelineAnnotator:
    def __init__(self):
        # åŠ è½½æ¨¡å‹
        print("ğŸ”§ åŠ è½½æ¨¡å‹ä¸­...")
        try:
            print("  â”œâ”€ åŠ è½½Whisper ASR...")
            self.asr = pipeline(
                "automatic-speech-recognition", 
                model="openai/whisper-small",
                device=-1  # CPUæ¨¡å¼
            )
            print("  âœ… WhisperåŠ è½½å®Œæˆ")
        except Exception as e:
            print(f"  âš ï¸ WhisperåŠ è½½å¤±è´¥: {e}")
            self.asr = None
        
        print("  â”œâ”€ åŠ è½½MediaPipe Pose...")
        self.mp_pose = mp.solutions.pose.Pose(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        print("  âœ… MediaPipe PoseåŠ è½½å®Œæˆ")
        
        print("  â”œâ”€ åŠ è½½MediaPipe Face Mesh...")
        self.mp_face = mp.solutions.face_mesh.FaceMesh(
            max_num_faces=1,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        print("  âœ… MediaPipe Face MeshåŠ è½½å®Œæˆ")
        
        # å¡«å……è¯åº“
        self.filler_words = ['å—¯', 'å•Š', 'å‘ƒ', 'é‚£ä¸ª', 'è¿™ä¸ª', 'å°±æ˜¯', 'ç„¶å', 'å˜›', 'å§']
        print("\nâœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼\n")
        
    def extract_audio_from_video(self, video_path):
        """ä»è§†é¢‘ä¸­æå–éŸ³é¢‘"""
        try:
            import subprocess
            import tempfile
            
            # åˆ›å»ºä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
            temp_audio = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
            temp_audio.close()
            
            # ä½¿ç”¨ffmpegæå–éŸ³é¢‘ï¼ˆå¦‚æœæœ‰ï¼‰
            # ç®€åŒ–ç‰ˆï¼šç›´æ¥ç”¨OpenCVè¯»å–ï¼Œä¸æå–éŸ³é¢‘
            print(f"    âš ï¸ è·³è¿‡éŸ³é¢‘æå–ï¼ˆéœ€è¦ffmpegï¼‰ï¼Œå°†ä½¿ç”¨è§†é¢‘ç›´æ¥åˆ†æ")
            return None
        except Exception as e:
            print(f"    âš ï¸ éŸ³é¢‘æå–å¤±è´¥: {e}")
            return None
    
    def process_video(self, video_path, question, sample_id):
        """å¤„ç†ä¸€ä¸ªè§†é¢‘ï¼Œç”Ÿæˆ10ç§’çª—å£çš„æ ‡æ³¨"""
        print(f"\n{'='*60}")
        print(f"ğŸ“¹ å¤„ç†è§†é¢‘: {os.path.basename(video_path)}")
        print(f"â“ æ¨¡æ‹Ÿé—®é¢˜: {question}")
        print(f"{'='*60}\n")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
            return []
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps
        
        print(f"  ğŸ“Š è§†é¢‘ä¿¡æ¯:")
        print(f"     â”œâ”€ æ€»å¸§æ•°: {total_frames}")
        print(f"     â”œâ”€ å¸§ç‡: {fps:.2f} fps")
        print(f"     â””â”€ æ—¶é•¿: {duration:.2f} ç§’")
        
        # æ¯10ç§’ä¸€ä¸ªçª—å£
        window_size = 10  # ç§’
        annotations = []
        
        num_windows = int(duration / window_size) + (1 if duration % window_size > 0 else 0)
        print(f"\n  ğŸªŸ å°†åˆ†æ {num_windows} ä¸ªæ—¶é—´çª—å£ï¼ˆæ¯ä¸ª{window_size}ç§’ï¼‰\n")
        
        for window_idx in range(num_windows):
            start_time = window_idx * window_size
            end_time = min(start_time + window_size, int(duration))
            
            print(f"  â±ï¸  çª—å£ {window_idx+1}/{num_windows}: {start_time}s - {end_time}s")
            
            # æå–è¿™ä¸ªçª—å£çš„ç‰¹å¾
            features = self.extract_window_features(
                cap, fps, start_time, end_time
            )
            
            # è‡ªåŠ¨è¯„åˆ†
            scores = self.auto_score(features, question)
            
            # æ£€æµ‹å¼‚å¸¸
            alert_type, alert_text = self.detect_alert(features)
            
            annotations.append({
                'sample_id': sample_id,
                'video_path': video_path,
                'question': question,
                'start_time': start_time,
                'end_time': end_time,
                'focus_score': scores['focus'],
                'psychological_score': scores['psychological'],
                'language_score': scores['language'],
                'professional_score': scores['professional'],
                'alert_type': alert_type,
                'alert_text': alert_text,
                'transcription': features.get('transcription', ''),
                'notes': features['summary']
            })
            
            print(f"     â”œâ”€ ä¸“æ³¨åº¦: {scores['focus']:.0f}")
            print(f"     â”œâ”€ å¿ƒç†ç´ è´¨: {scores['psychological']:.0f}")
            print(f"     â”œâ”€ è¯­è¨€è¡¨è¾¾: {scores['language']:.0f}")
            print(f"     â”œâ”€ ä¸“ä¸šèƒ½åŠ›: {scores['professional']:.0f}")
            if alert_type > 0:
                print(f"     â””â”€ âš ï¸  æé†’: {alert_text}")
            else:
                print(f"     â””â”€ âœ… æ— å¼‚å¸¸")
        
        cap.release()
        print(f"\n  âœ… è§†é¢‘å¤„ç†å®Œæˆï¼ç”Ÿæˆ {len(annotations)} ä¸ªæ ‡æ³¨\n")
        return annotations
    
    def extract_window_features(self, cap, fps, start_time, end_time):
        """æå–10ç§’çª—å£çš„ç‰¹å¾"""
        start_frame = int(start_time * fps)
        end_frame = int(end_time * fps)
        
        emotions = []
        gazes = []
        poses = []
        
        # é‡‡æ ·å¸§ï¼ˆæ¯2ç§’é‡‡æ ·1å¸§ï¼ŒèŠ‚çœæ—¶é—´ï¼‰
        sample_interval = max(1, int(fps * 2))
        sample_frames = list(range(start_frame, end_frame, sample_interval))
        
        for frame_idx in sample_frames:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if not ret:
                break
            
            # æƒ…ç»ªåˆ†æ
            try:
                emotion_result = DeepFace.analyze(
                    frame, 
                    actions=['emotion'], 
                    enforce_detection=False,
                    silent=True
                )
                emotions.append(emotion_result[0]['emotion'])
            except Exception as e:
                # print(f"       æƒ…ç»ªåˆ†æå¤±è´¥: {e}")
                pass
            
            # çœ¼åŠ¨æ£€æµ‹
            try:
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                face_result = self.mp_face.process(frame_rgb)
                if face_result.multi_face_landmarks:
                    landmarks = face_result.multi_face_landmarks[0].landmark
                    # MediaPipe iris landmarks: 468-473
                    left_iris = landmarks[468]
                    gaze_x = left_iris.x
                    gaze_deviation = abs(gaze_x - 0.5)
                    gazes.append({'x': gaze_x, 'deviation': gaze_deviation})
            except Exception as e:
                # print(f"       çœ¼åŠ¨æ£€æµ‹å¤±è´¥: {e}")
                pass
            
            # å§¿åŠ¿æ£€æµ‹
            try:
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                pose_result = self.mp_pose.process(frame_rgb)
                if pose_result.pose_landmarks:
                    # è‚©è†€å…³é”®ç‚¹: 11=å·¦è‚©, 12=å³è‚©
                    shoulder_l = pose_result.pose_landmarks.landmark[11]
                    shoulder_r = pose_result.pose_landmarks.landmark[12]
                    shoulder_y = (shoulder_l.y + shoulder_r.y) / 2
                    poses.append({'shoulder_y': shoulder_y})
            except Exception as e:
                # print(f"       å§¿åŠ¿æ£€æµ‹å¤±è´¥: {e}")
                pass
        
        # è¯­éŸ³è½¬å½•ï¼ˆæš‚æ—¶è·³è¿‡ï¼Œå› ä¸ºéœ€è¦ä»è§†é¢‘æå–éŸ³é¢‘ï¼‰
        transcription = ""
        
        # ç»Ÿè®¡ç‰¹å¾
        return {
            'emotions': emotions,
            'gazes': gazes,
            'poses': poses,
            'transcription': transcription,
            'summary': self.summarize_features(emotions, gazes, poses, transcription)
        }
    
    def summarize_features(self, emotions, gazes, poses, transcription):
        """ç”Ÿæˆç‰¹å¾æ‘˜è¦"""
        summary = []
        
        # æƒ…ç»ªæ‘˜è¦
        if emotions:
            avg_emotions = {k: np.mean([e[k] for e in emotions]) for k in emotions[0].keys()}
            dominant = max(avg_emotions, key=avg_emotions.get)
            summary.append(f"æƒ…ç»ª:{dominant}({avg_emotions[dominant]:.1f}%)")
        else:
            summary.append("æƒ…ç»ª:æœªæ£€æµ‹")
        
        # çœ¼åŠ¨æ‘˜è¦
        if gazes:
            avg_deviation = np.mean([g['deviation'] for g in gazes])
            if avg_deviation > 0.08:
                summary.append("çœ¼ç¥åç¦»")
            else:
                summary.append("çœ¼ç¥ä¸“æ³¨")
        else:
            summary.append("çœ¼ç¥:æœªæ£€æµ‹")
        
        # å§¿åŠ¿æ‘˜è¦
        if poses:
            summary.append(f"å§¿åŠ¿æ­£å¸¸")
        else:
            summary.append("å§¿åŠ¿:æœªæ£€æµ‹")
        
        return "; ".join(summary)
    
    def auto_score(self, features, question):
        """è‡ªåŠ¨è¯„åˆ†"""
        # ä¸“æ³¨åº¦ï¼ˆåŸºäºçœ¼åŠ¨å’Œå§¿åŠ¿ï¼‰
        focus = 80
        if features['gazes']:
            avg_deviation = np.mean([g['deviation'] for g in features['gazes']])
            if avg_deviation > 0.1:
                focus -= 20
            elif avg_deviation < 0.05:
                focus += 10
        else:
            focus = 70  # æœªæ£€æµ‹åˆ°ï¼Œç»™ä¸­ç­‰åˆ†
        
        # å¿ƒç†ç´ è´¨ï¼ˆåŸºäºæƒ…ç»ªï¼‰
        psychological = 75
        if features['emotions']:
            avg_emotions = {k: np.mean([e[k] for e in features['emotions']]) 
                           for k in features['emotions'][0].keys()}
            
            # ç§¯ææƒ…ç»ª
            positive = avg_emotions.get('happy', 0) + avg_emotions.get('neutral', 0)
            # æ¶ˆææƒ…ç»ª
            negative = avg_emotions.get('fear', 0) + avg_emotions.get('sad', 0)
            
            if positive > 70:
                psychological += 15
            elif negative > 50:
                psychological -= 20
        else:
            psychological = 70
        
        # è¯­è¨€è¡¨è¾¾ï¼ˆæš‚æ—¶åŸºäºé»˜è®¤å€¼ï¼Œå› ä¸ºæ²¡æœ‰éŸ³é¢‘ï¼‰
        language = 70
        
        # ä¸“ä¸šèƒ½åŠ›ï¼ˆéœ€è¦QAå¯¹é½ï¼Œæš‚æ—¶é»˜è®¤ï¼‰
        professional = 65
        
        return {
            'focus': max(0, min(100, focus)),
            'psychological': max(0, min(100, psychological)),
            'language': max(0, min(100, language)),
            'professional': professional
        }
    
    def detect_alert(self, features):
        """æ£€æµ‹å¼‚å¸¸"""
        # çœ¼ç¥é—®é¢˜
        if features['gazes']:
            avg_deviation = np.mean([g['deviation'] for g in features['gazes']])
            if avg_deviation > 0.1:
                return 1, "è¯·ä¿æŒçœ¼ç¥ä¸“æ³¨"
        
        # æƒ…ç»ªé—®é¢˜
        if features['emotions']:
            avg_emotions = {k: np.mean([e[k] for e in features['emotions']]) 
                           for k in features['emotions'][0].keys()}
            
            fear_level = avg_emotions.get('fear', 0)
            sad_level = avg_emotions.get('sad', 0)
            
            if fear_level > 40:
                return 2, "æ·±å‘¼å¸ï¼Œæ”¾è½»æ¾"
            elif sad_level > 50:
                return 2, "ä¿æŒç§¯æå¿ƒæ€"
        
        return 0, ""


def main():
    """æµ‹è¯•Pipeline"""
    print("\n" + "="*60)
    print("     ğŸš€ é¢è¯•è¯„ä¼°Pipelineæµ‹è¯•")
    print("="*60 + "\n")
    
    # åˆå§‹åŒ–æ ‡æ³¨å™¨
    annotator = TestPipelineAnnotator()
    
    # testvæ–‡ä»¶å¤¹ä¸­çš„è§†é¢‘
    video_dir = "./testv"
    video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]
    video_files.sort()
    
    print(f"ğŸ“‚ å‘ç° {len(video_files)} ä¸ªæµ‹è¯•è§†é¢‘:\n")
    for i, vf in enumerate(video_files, 1):
        print(f"   {i}. {vf}")
    print()
    
    # æ¨¡æ‹Ÿé—®é¢˜ï¼ˆå®é™…ä½¿ç”¨æ—¶åº”è¯¥æä¾›çœŸå®é—®é¢˜ï¼‰
    questions = {
        't1-1.mp4': "è¯·åšä¸€ä¸ªç®€çŸ­çš„è‡ªæˆ‘ä»‹ç»",
        't2-1.mp4': "ä»‹ç»ä¸€ä¸‹ä½ æœ€è¿‘çš„é¡¹ç›®ç»éªŒ",
        't3-1.mp4': "è¯´è¯´ä½ å¯¹Pythonçš„ç†è§£"
    }
    
    all_annotations = []
    
    for i, video_file in enumerate(video_files, 1):
        video_path = os.path.join(video_dir, video_file)
        sample_id = f"test_{i:03d}"
        question = questions.get(video_file, "é€šç”¨é¢è¯•é—®é¢˜")
        
        # å¤„ç†è§†é¢‘
        annotations = annotator.process_video(
            video_path=video_path,
            question=question,
            sample_id=sample_id
        )
        
        all_annotations.extend(annotations)
    
    # ä¿å­˜ä¸ºCSV
    if all_annotations:
        df = pd.DataFrame(all_annotations)
        output_path = "test_annotations.csv"
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        print("\n" + "="*60)
        print("âœ… Pipelineæµ‹è¯•å®Œæˆï¼")
        print("="*60)
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   â”œâ”€ å¤„ç†è§†é¢‘æ•°: {len(video_files)}")
        print(f"   â”œâ”€ ç”Ÿæˆæ ‡æ³¨æ•°: {len(all_annotations)}")
        print(f"   â””â”€ ä¿å­˜æ–‡ä»¶: {output_path}")
        
        print(f"\nğŸ“ˆ è¯„åˆ†æ‘˜è¦:")
        print(f"   â”œâ”€ ä¸“æ³¨åº¦: {df['focus_score'].mean():.1f} Â± {df['focus_score'].std():.1f}")
        print(f"   â”œâ”€ å¿ƒç†ç´ è´¨: {df['psychological_score'].mean():.1f} Â± {df['psychological_score'].std():.1f}")
        print(f"   â”œâ”€ è¯­è¨€è¡¨è¾¾: {df['language_score'].mean():.1f} Â± {df['language_score'].std():.1f}")
        print(f"   â””â”€ ä¸“ä¸šèƒ½åŠ›: {df['professional_score'].mean():.1f} Â± {df['professional_score'].std():.1f}")
        
        alert_count = (df['alert_type'] > 0).sum()
        print(f"\nâš ï¸  æ£€æµ‹åˆ°å¼‚å¸¸: {alert_count}/{len(all_annotations)} ä¸ªçª—å£")
        
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"   1. æ‰“å¼€ {output_path} æŸ¥çœ‹è¯¦ç»†æ ‡æ³¨")
        print(f"   2. äººå·¥è°ƒæ•´è¯„åˆ†ï¼ˆå°¤å…¶æ˜¯ professional_scoreï¼‰")
        print(f"   3. å‡†å¤‡æ›´å¤šè§†é¢‘æ•°æ®")
        print(f"   4. å¼€å§‹è®­ç»ƒTransformeræ¨¡å‹\n")
    else:
        print("\nâŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æ ‡æ³¨ï¼Œè¯·æ£€æŸ¥è§†é¢‘æ–‡ä»¶")


if __name__ == "__main__":
    main()

