"""
AIé¢è¯•è¯„åˆ†ç³»ç»Ÿ - çœŸæ­£çš„å®æ—¶æ¼”ç¤ºï¼ˆä¿®å¤ç‰ˆï¼‰
ä¿®å¤: å•ä¸€æ‘„åƒå¤´æ•è·ï¼Œå¸§åˆ†å‘ç»™å„ä¸ªæå–çº¿ç¨‹
ç¯å¢ƒ: interview_realtime
"""

import cv2
import numpy as np
import torch
import time
import os
from collections import deque
from threading import Thread
import queue
from PIL import Image, ImageDraw, ImageFont

# å¯¼å…¥ç‰¹å¾æå–åº“
from deepface import DeepFace
import mediapipe as mp
import pyaudio
import librosa

from model.transformer_model import InterviewTransformer, REMINDER_MAP

# ==================== é…ç½® ====================
WINDOW_SIZE = 5
MODEL_PATH = './checkpoints/best_model.pth'
CAMERA_ID = 0
AUDIO_RATE = 16000
AUDIO_CHUNK = 1024

# ==================== å…¨å±€å˜é‡ ====================
emotion_buffer = deque(maxlen=WINDOW_SIZE)
audio_buffer = deque(maxlen=WINDOW_SIZE)
pose_buffer = deque(maxlen=WINDOW_SIZE)
gaze_buffer = deque(maxlen=WINDOW_SIZE)

current_scores = None
current_reminder = "Initializing..."

# ç‰¹å¾é˜Ÿåˆ—ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
emotion_queue = queue.Queue(maxsize=5)
audio_queue = queue.Queue(maxsize=5)
pose_gaze_queue = queue.Queue(maxsize=5)

# å¸§åˆ†å‘é˜Ÿåˆ—
emotion_frame_queue = queue.Queue(maxsize=2)
pose_frame_queue = queue.Queue(maxsize=2)

is_running = True

# ==================== æ¨¡å‹åŠ è½½ ====================
def load_model(device='cpu'):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    model = InterviewTransformer(
        d_model=128,
        nhead=4,
        num_encoder_layers=2,
        dropout=0.3,
        num_reminders=30
    ).to(device)
    
    if os.path.exists(MODEL_PATH):
        checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        print(f"[OK] Model loaded: {MODEL_PATH}")
        return model
    else:
        print(f"[ERROR] Model not found: {MODEL_PATH}")
        return None

# ==================== çœŸå®ç‰¹å¾æå–ï¼ˆçº¿ç¨‹ï¼‰ ====================
class EmotionExtractor(Thread):
    """æƒ…ç»ªç‰¹å¾æå–çº¿ç¨‹ - ä»é˜Ÿåˆ—æ¥æ”¶å¸§"""
    def __init__(self, frame_queue):
        super().__init__()
        self.frame_queue = frame_queue
        self.daemon = True
        
    def run(self):
        global is_running
        print("[Emotion] Thread started")
        frame_count = 0
        
        # é»˜è®¤ä¸­æ€§æƒ…ç»ª
        default_emotion = np.array([0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.9], dtype=np.float32)
        
        while is_running:
            try:
                # ä»é˜Ÿåˆ—è·å–å¸§ï¼ˆè¶…æ—¶1ç§’ï¼‰
                frame = self.frame_queue.get(timeout=1.0)
            except queue.Empty:
                continue
            
            # æ¯å¸§éƒ½æå–ï¼ˆæé«˜æˆåŠŸç‡ï¼‰
            emotion_features = default_emotion.copy()
            
            try:
                result = DeepFace.analyze(
                    frame,
                    actions=['emotion'],
                    enforce_detection=False,
                    detector_backend='opencv',
                    silent=True
                )
                
                if isinstance(result, list):
                    result = result[0]
                
                emotion_scores = result.get('emotion', {})
                emotion_features = np.array([
                    emotion_scores.get('angry', 0.0) / 100.0,
                    emotion_scores.get('disgust', 0.0) / 100.0,
                    emotion_scores.get('fear', 0.0) / 100.0,
                    emotion_scores.get('happy', 0.0) / 100.0,
                    emotion_scores.get('sad', 0.0) / 100.0,
                    emotion_scores.get('surprise', 0.0) / 100.0,
                    emotion_scores.get('neutral', 0.0) / 100.0
                ], dtype=np.float32)
                
            except Exception as e:
                pass  # ä½¿ç”¨é»˜è®¤å€¼
            
            # æ€»æ˜¯æ”¾å…¥ç‰¹å¾ï¼ˆå³ä½¿æ˜¯é»˜è®¤å€¼ï¼‰
            if not emotion_queue.full():
                emotion_queue.put(emotion_features)
            
            frame_count += 1
            time.sleep(0.1)  # é¿å…CPUå ç”¨è¿‡é«˜
        
        print("[Emotion] Thread stopped")


class AudioExtractor(Thread):
    """éŸ³é¢‘ç‰¹å¾æå–çº¿ç¨‹ï¼ˆç‹¬ç«‹å½•éŸ³ï¼‰"""
    def __init__(self):
        super().__init__()
        self.daemon = True
        
    def run(self):
        global is_running
        
        try:
            p = pyaudio.PyAudio()
            stream = p.open(
                format=pyaudio.paInt16,
                channels=1,
                rate=AUDIO_RATE,
                input=True,
                frames_per_buffer=AUDIO_CHUNK
            )
            
            print("[Audio] Thread started")
            
            while is_running:
                try:
                    audio_data = stream.read(AUDIO_CHUNK, exception_on_overflow=False)
                    audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
                    
                    if len(audio_np) > 512:
                        mel_spec = librosa.feature.melspectrogram(
                            y=audio_np,
                            sr=AUDIO_RATE,
                            n_mels=80,
                            hop_length=512
                        )
                        mel_db = librosa.power_to_db(mel_spec, ref=np.max)
                        audio_features = np.mean(mel_db, axis=1).astype(np.float32)
                        
                        if not audio_queue.full():
                            audio_queue.put(audio_features)
                    
                except Exception as e:
                    pass
                
                time.sleep(0.1)
            
            stream.stop_stream()
            stream.close()
            p.terminate()
            print("[Audio] Thread stopped")
            
        except Exception as e:
            print(f"[Audio] Failed to start: {e}")


class PoseGazeExtractor(Thread):
    """å§¿åŠ¿å’Œçœ¼åŠ¨ç‰¹å¾æå–çº¿ç¨‹ - ä»é˜Ÿåˆ—æ¥æ”¶å¸§"""
    def __init__(self, frame_queue):
        super().__init__()
        self.frame_queue = frame_queue
        self.daemon = True
        
    def run(self):
        global is_running
        
        mp_pose = mp.solutions.pose
        mp_face_mesh = mp.solutions.face_mesh
        
        pose_detector = mp_pose.Pose(
            static_image_mode=False,
            model_complexity=1,
            min_detection_confidence=0.5
        )
        
        face_mesh = mp_face_mesh.FaceMesh(
            static_image_mode=False,
            max_num_faces=1,
            min_detection_confidence=0.5
        )
        
        print("[Pose/Gaze] Thread started")
        frame_count = 0
        
        # é»˜è®¤ç‰¹å¾
        default_pose = np.zeros(99, dtype=np.float32)
        default_gaze = np.array([0.5, 0.5, 0.0, 0.5, 0.5], dtype=np.float32)
        
        while is_running:
            try:
                # ä»é˜Ÿåˆ—è·å–å¸§ï¼ˆè¶…æ—¶1ç§’ï¼‰
                frame = self.frame_queue.get(timeout=1.0)
            except queue.Empty:
                continue
            
            # æ¯å¸§éƒ½æå–ï¼ˆæé«˜æˆåŠŸç‡ï¼‰
            pose_features = default_pose.copy()
            gaze_features = default_gaze.copy()
            
            try:
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # å§¿åŠ¿
                pose_results = pose_detector.process(rgb_frame)
                
                if pose_results.pose_landmarks:
                    landmarks = pose_results.pose_landmarks.landmark
                    for i, lm in enumerate(landmarks[:33]):
                        pose_features[i*3] = lm.x
                        pose_features[i*3+1] = lm.y
                        pose_features[i*3+2] = lm.z
                
                # çœ¼åŠ¨
                face_results = face_mesh.process(rgb_frame)
                
                if face_results.multi_face_landmarks:
                    face_landmarks = face_results.multi_face_landmarks[0].landmark
                    left_eye = face_landmarks[33]
                    right_eye = face_landmarks[263]
                    gaze_features = np.array([
                        left_eye.x, left_eye.y,
                        0.0,
                        right_eye.x, right_eye.y
                    ], dtype=np.float32)
                
            except Exception as e:
                pass  # ä½¿ç”¨é»˜è®¤å€¼
            
            # æ€»æ˜¯æ”¾å…¥ç‰¹å¾ï¼ˆå³ä½¿æ˜¯é»˜è®¤å€¼ï¼‰
            if not pose_gaze_queue.full():
                pose_gaze_queue.put((pose_features, gaze_features))
            
            frame_count += 1
            time.sleep(0.1)
        
        print("[Pose/Gaze] Thread stopped")


# ==================== æé†’æ–‡æœ¬ä¸­æ–‡æ˜ å°„ ====================
REMINDER_TEXT_CN = {
    0: "è¯´è¯æ›´æ¸…æ™°", 1: "å‡å°‘å£å¤´ç¦…", 2: "æ§åˆ¶è¯­é€Ÿ",
    3: "ç»„ç»‡å¥½é€»è¾‘", 4: "æ”¹å–„è¡¨è¾¾æ–¹å¼", 5: "ä¿æŒå†·é™",
    6: "æ›´åŠ è‡ªä¿¡", 7: "æ§åˆ¶ç„¦è™‘", 8: "ä¿æŒä¸“æ³¨",
    9: "é¿å…ç´§å¼ ", 10: "åç›´èº«ä½“", 11: "å‡å°‘ä¸å¿…è¦åŠ¨ä½œ",
    12: "é€‚å½“ä½¿ç”¨æ‰‹åŠ¿", 13: "ä¿æŒè‰¯å¥½å§¿æ€", 14: "æ§åˆ¶è‚¢ä½“åŠ¨ä½œ",
    15: "ä¿æŒçœ¼ç¥äº¤æµ", 16: "é¿å…åˆ†å¿ƒ", 17: "ä¿æŒæŠ•å…¥",
    18: "ä¸“æ³¨é¢è¯•å®˜", 19: "é¿å…ç§»å¼€è§†çº¿", 20: "è¡¨ç°å‡ºè‰²ï¼",
    21: "è¡¨è¾¾å¾ˆå¥½", 22: "å¿ƒæ€æ²‰ç¨³", 23: "è‚¢ä½“è¯­è¨€è‰¯å¥½",
    24: "æ³¨æ„åŠ›é›†ä¸­", 25: "èŒä¸šå½¢è±¡å¥½", 26: "æ²Ÿé€šèƒ½åŠ›å¼º",
    27: "è‡ªä¿¡è¡¨è¾¾", 28: "æ°”åœºå¾ˆå¥½", 29: "é¢è¯•è¡¨ç°ä¼˜ç§€ï¼"
}

def get_reminder_text(idx):
    """è·å–æé†’æ–‡æœ¬ï¼ˆä¸­æ–‡ï¼‰"""
    return REMINDER_TEXT_CN.get(idx, "åˆ†æä¸­...")


# ==================== æ¨ç†å’Œæ˜¾ç¤º ====================
def run_inference(model, device='cpu'):
    """è¿è¡Œæ¨¡å‹æ¨ç†"""
    global current_scores, current_reminder
    
    # æ£€æŸ¥æ‰€æœ‰bufferæ˜¯å¦éƒ½æ»¡è¶³è¦æ±‚
    if (len(emotion_buffer) < WINDOW_SIZE or 
        len(audio_buffer) < WINDOW_SIZE or 
        len(pose_buffer) < WINDOW_SIZE or 
        len(gaze_buffer) < WINDOW_SIZE):
        print(f"[Inference] Waiting for buffers: E:{len(emotion_buffer)} A:{len(audio_buffer)} P:{len(pose_buffer)} G:{len(gaze_buffer)}/{WINDOW_SIZE}")
        return
    
    try:
        # å‡†å¤‡è¾“å…¥
        emotion_arr = np.array(list(emotion_buffer))
        audio_arr = np.array(list(audio_buffer))
        pose_arr = np.array(list(pose_buffer))
        gaze_arr = np.array(list(gaze_buffer))
        
        print(f"[Inference] Input shapes: E:{emotion_arr.shape} A:{audio_arr.shape} P:{pose_arr.shape} G:{gaze_arr.shape}")
        
        emotion_seq = torch.tensor(emotion_arr, dtype=torch.float32).unsqueeze(0).to(device)
        audio_seq = torch.tensor(audio_arr, dtype=torch.float32).unsqueeze(0).to(device)
        pose_seq = torch.tensor(pose_arr, dtype=torch.float32).unsqueeze(0).to(device)
        gaze_seq = torch.tensor(gaze_arr, dtype=torch.float32).unsqueeze(0).to(device)
        
        print(f"[Inference] Tensor shapes: E:{emotion_seq.shape} A:{audio_seq.shape} P:{pose_seq.shape} G:{gaze_seq.shape}")
        
        # æ¨ç†
        with torch.no_grad():
            scores_pred, reminder_pred, _ = model(emotion_seq, audio_seq, pose_seq, gaze_seq)
        
        scores_np = scores_pred.cpu().numpy()[0]
        reminder_idx = torch.argmax(reminder_pred, dim=1).item()
        
        current_scores = scores_np
        current_reminder = get_reminder_text(reminder_idx)
        
        print(f"[Inference SUCCESS] Scores: {scores_np}, Reminder: {current_reminder}")
        
    except Exception as e:
        import traceback
        print(f"[Inference Error] {e}")
        print(traceback.format_exc())


def cv2_add_chinese_text(img, text, position, font_size=20, color=(255, 255, 255)):
    """åœ¨OpenCVå›¾åƒä¸Šæ·»åŠ ä¸­æ–‡æ–‡å­—ï¼ˆä½¿ç”¨PILï¼‰"""
    # è½¬æ¢ä¸ºPILå›¾åƒ
    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    
    # å°è¯•ä½¿ç”¨ç³»ç»Ÿå­—ä½“
    try:
        # Windowsç³»ç»Ÿå­—ä½“è·¯å¾„
        font_path = "C:/Windows/Fonts/msyh.ttc"  # å¾®è½¯é›…é»‘
        if not os.path.exists(font_path):
            font_path = "C:/Windows/Fonts/simhei.ttf"  # é»‘ä½“
        if not os.path.exists(font_path):
            font_path = None
        
        if font_path:
            font = ImageFont.truetype(font_path, font_size)
        else:
            font = ImageFont.load_default()
    except:
        font = ImageFont.load_default()
    
    # ç»˜åˆ¶æ–‡å­—
    draw.text(position, text, font=font, fill=color)
    
    # è½¬æ¢å›OpenCVæ ¼å¼
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)


def draw_ui(frame, scores, reminder_text, fps):
    """ç»˜åˆ¶ä¼˜åŒ–çš„UIç•Œé¢ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰"""
    h, w = frame.shape[:2]
    
    # åˆ›å»ºå³ä¾§è¯„åˆ†é¢æ¿ï¼ˆæ›´å®½ï¼Œæ·±è‰²èƒŒæ™¯ï¼‰
    panel = np.ones((h, 500, 3), dtype=np.uint8) * 30
    
    # é¡¶éƒ¨æ¸å˜è£…é¥°æ¡ï¼ˆç¼©å°ï¼‰
    for i in range(5):
        color_val = 60 + i * 10
        cv2.rectangle(panel, (0, i*2), (500, (i+1)*2), (color_val, color_val, color_val), -1)
    
    # æ ‡é¢˜ï¼ˆä½¿ç”¨PILç»˜åˆ¶ä¸­æ–‡ï¼Œç¼©å°ï¼‰
    panel = cv2_add_chinese_text(panel, "AI æ™ºèƒ½é¢è¯•è¯„åˆ†", (30, 18), 24, (255, 255, 255))
    
    # è£…é¥°çº¿
    cv2.line(panel, (30, 52), (470, 52), (80, 80, 80), 2)
    
    # åˆ†æ•°æ˜¾ç¤ºï¼ˆè¶…ç´§å‡‘å¸ƒå±€ï¼‰
    if scores is not None:
        labels_cn = ["è¯­è¨€è¡¨è¾¾", "å¿ƒç†ç´ è´¨", "è‚¢ä½“è¯­è¨€", "ä¸“æ³¨åº¦", "ç»¼åˆå¾—åˆ†"]
        colors_bgr = [(100, 255, 100), (100, 180, 255), (255, 100, 255), (255, 200, 100), (100, 255, 255)]
        
        for i, (label, score, color) in enumerate(zip(labels_cn, scores, colors_bgr)):
            y_start = 65 + i * 62  # è¿›ä¸€æ­¥å‡å°é—´è·ï¼šä»72åˆ°62
            
            # èƒŒæ™¯å¡ç‰‡ï¼ˆè¿›ä¸€æ­¥å‡å°é«˜åº¦ï¼‰
            cv2.rectangle(panel, (20, y_start), (480, y_start + 58), (45, 45, 45), -1)
            cv2.rectangle(panel, (20, y_start), (480, y_start + 58), (70, 70, 70), 2)
            
            # æ ‡ç­¾ï¼ˆä¸­æ–‡ï¼Œç¼©å°å­—ä½“ï¼‰
            panel = cv2_add_chinese_text(panel, label, (35, y_start + 10), 18, (220, 220, 220))
            
            # åˆ†æ•°ï¼ˆç¼©å°ï¼‰
            score_text = f"{score:.1f}"
            cv2.putText(panel, score_text, (380, y_start + 36), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, 2)
            
            # è¿›åº¦æ¡èƒŒæ™¯
            cv2.rectangle(panel, (35, y_start + 44), (370, y_start + 55), (60, 60, 60), -1)
            
            # è¿›åº¦æ¡
            bar_width = int((score / 100.0) * 335)
            cv2.rectangle(panel, (35, y_start + 44), (35 + bar_width, y_start + 55), color, -1)
            
            # è¿›åº¦æ¡é«˜å…‰æ•ˆæœ
            if bar_width > 10:
                highlight_color = tuple(min(255, c + 40) for c in color)
                cv2.rectangle(panel, (35, y_start + 44), (35 + bar_width, y_start + 49), highlight_color, -1)
    
    # æ™ºèƒ½æé†’åŒºåŸŸï¼ˆè¿›ä¸€æ­¥ç¼©å°ï¼‰
    y_reminder = h - 70
    cv2.rectangle(panel, (20, y_reminder), (480, h - 30), (50, 50, 70), -1)
    cv2.rectangle(panel, (20, y_reminder), (480, h - 30), (100, 100, 150), 2)
    
    # æé†’å†…å®¹ï¼ˆåˆå¹¶æ ‡é¢˜å’Œå†…å®¹ï¼‰
    reminder_display = f"ğŸ’¡ {reminder_text}"
    panel = cv2_add_chinese_text(panel, reminder_display, (30, y_reminder + 10), 16, (255, 255, 150))
    
    # FPSå’ŒçŠ¶æ€ï¼ˆåˆå¹¶åˆ°ä¸€è¡Œï¼‰
    cv2.rectangle(panel, (20, h - 28), (480, h - 5), (40, 40, 40), -1)
    cv2.putText(panel, f"FPS: {fps:.1f}", (30, h - 10), 
               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 255, 100), 1)
    
    # BufferçŠ¶æ€ï¼ˆå³ä¾§ï¼‰
    status_text = f"E:{len(emotion_buffer)} A:{len(audio_buffer)} P:{len(pose_buffer)} G:{len(gaze_buffer)}"
    cv2.putText(panel, status_text, (250, h - 10), 
               cv2.FONT_HERSHEY_SIMPLEX, 0.45, (150, 150, 150), 1)
    
    # æ‹¼æ¥ç”»é¢
    combined = np.hstack([frame, panel])
    return combined


# ==================== ä¸»ç¨‹åº ====================
def main():
    global is_running
    
    print("=" * 80)
    print("  AI Interview Scoring - REAL-TIME LIVE DEMO (Fixed)")
    print("=" * 80)
    print()
    
    # è®¾å¤‡
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Device: {device}")
    
    # åŠ è½½æ¨¡å‹
    model = load_model(device)
    if model is None:
        print("[ERROR] Cannot start without model!")
        return
    
    # æ‰“å¼€æ‘„åƒå¤´ï¼ˆä»…ä¸»çº¿ç¨‹ï¼‰
    print("\nOpening camera (main thread only)...")
    cap = cv2.VideoCapture(CAMERA_ID, cv2.CAP_DSHOW)
    
    if not cap.isOpened():
        print(f"[ERROR] Cannot open camera {CAMERA_ID}")
        return
    
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    cap.set(cv2.CAP_PROP_FPS, 30)
    
    # é¢„çƒ­æ‘„åƒå¤´
    print("Warming up camera...")
    for _ in range(10):
        cap.read()
    
    print("[OK] Camera ready")
    
    # å¯åŠ¨ç‰¹å¾æå–çº¿ç¨‹
    print("\nStarting feature extraction threads...")
    emotion_thread = EmotionExtractor(emotion_frame_queue)
    audio_thread = AudioExtractor()
    pose_thread = PoseGazeExtractor(pose_frame_queue)
    
    emotion_thread.start()
    audio_thread.start()
    pose_thread.start()
    
    # ä¸»å¾ªç¯
    print("\nMain loop starting...")
    print("Press 'q' or ESC to quit\n")
    
    fps_counter = deque(maxlen=30)
    last_time = time.time()
    last_status_print = time.time()
    frame_count = 0
    
    try:
        while is_running:
            ret, frame = cap.read()
            if not ret or frame is None:
                print("[WARNING] Failed to grab frame")
                time.sleep(0.1)
                continue
            
            # åˆ†å‘å¸§åˆ°æå–çº¿ç¨‹
            if frame_count % 5 == 0:  # æ¯5å¸§åˆ†å‘ä¸€æ¬¡
                try:
                    emotion_frame_queue.put_nowait(frame.copy())
                    pose_frame_queue.put_nowait(frame.copy())
                except queue.Full:
                    pass
            
            # æ”¶é›†ç‰¹å¾
            try:
                while not emotion_queue.empty():
                    emotion_buffer.append(emotion_queue.get_nowait())
            except queue.Empty:
                pass
            
            try:
                while not audio_queue.empty():
                    audio_buffer.append(audio_queue.get_nowait())
            except queue.Empty:
                pass
            
            try:
                while not pose_gaze_queue.empty():
                    pose_feat, gaze_feat = pose_gaze_queue.get_nowait()
                    pose_buffer.append(pose_feat)
                    gaze_buffer.append(gaze_feat)
            except queue.Empty:
                pass
            
            # å®šæœŸæ‰“å°çŠ¶æ€ï¼ˆæ¯3ç§’ï¼‰
            if time.time() - last_status_print > 3.0:
                print(f"[Status] Buffers - E:{len(emotion_buffer)} A:{len(audio_buffer)} P:{len(pose_buffer)} G:{len(gaze_buffer)}/{WINDOW_SIZE}")
                last_status_print = time.time()
            
            # æ¨ç†ï¼ˆæ¯10å¸§ï¼‰
            if frame_count % 10 == 0 and len(emotion_buffer) >= WINDOW_SIZE:
                run_inference(model, device)
            
            # è®¡ç®—FPS
            current_time = time.time()
            fps = 1.0 / (current_time - last_time) if (current_time - last_time) > 0 else 0
            fps_counter.append(fps)
            avg_fps = np.mean(fps_counter)
            last_time = current_time
            
            # ç»˜åˆ¶UI
            display_frame = draw_ui(frame, current_scores, current_reminder, avg_fps)
            
            # æ˜¾ç¤º
            cv2.imshow('AI Interview Scoring', display_frame)
            
            # æŒ‰é”®
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q') or key == 27:  # 'q' or ESC
                break
            
            frame_count += 1
    
    except KeyboardInterrupt:
        print("\n[INFO] Interrupted by user")
    
    finally:
        print("\nShutting down...")
        is_running = False
        
        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        time.sleep(2)
        
        # é‡Šæ”¾èµ„æº
        cap.release()
        cv2.destroyAllWindows()
        
        print("[OK] Cleanup complete")


if __name__ == '__main__':
    main()

